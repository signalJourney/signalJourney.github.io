{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Redirecting... <p>If you are not redirected automatically, follow this link to the Introduction.</p>"},{"location":"bids/","title":"BIDS Integration","text":"<p>This document outlines strategies and best practices for integrating signalJourney provenance files within datasets structured according to the Brain Imaging Data Structure (BIDS) standard.</p>"},{"location":"bids/#goal","title":"Goal","text":"<p>The primary goal is to store detailed pipeline provenance alongside the BIDS data it describes or generates, without violating BIDS validation rules.</p>"},{"location":"bids/#placement-of-_signaljourneyjson-files","title":"Placement of <code>*_signalJourney.json</code> Files","text":"<p>BIDS currently does not formally specify <code>*_signalJourney.json</code> files. Therefore, they should always be placed within the <code>/derivatives</code> subdirectory of a BIDS dataset.</p> <p>Recommended Location:</p> <p>Place signalJourney files within the specific pipeline derivative directory they document:</p> <pre><code>&lt;bids_root&gt;/\n  derivatives/\n    &lt;pipeline_name&gt;/\n      sub-&lt;label&gt;/\n        [ses-&lt;label&gt;/]\n          &lt;datatype&gt;/\n            &lt;source_entities&gt;_desc-&lt;description&gt;_&lt;suffix&gt;.json &lt;-- Associated data file\n            &lt;source_entities&gt;_desc-&lt;description&gt;_signalJourney.json &lt;-- Provenance for the data file\n      ...\n      dataset_description.json\n      *_signalJourney.json &lt;-- Optional: For pipeline-level info not tied to a specific output file\n</code></pre> <ul> <li><code>&lt;pipeline_name&gt;</code>: A descriptive name for your processing pipeline (e.g., <code>eeg_preprocessing</code>, <code>fmri_stats</code>).</li> <li>The signalJourney filename should mirror the BIDS filename of the data file it primarily documents, replacing the data suffix (e.g., <code>_eeg.fif</code>) with <code>_signalJourney.json</code>.</li> <li>Pipeline-level signalJourney files (not documenting a specific output file but perhaps the overall pipeline execution) can be placed at the root of the <code>&lt;pipeline_name&gt;</code> directory.</li> </ul> <p>Why <code>/derivatives</code>?</p> <ul> <li>Raw data directories in BIDS have strict validation rules. signalJourney files are not part of the raw data standard.</li> <li><code>/derivatives</code> is the designated location for outputs of processing pipelines.</li> <li>This keeps provenance clearly associated with the derived data.</li> </ul>"},{"location":"bids/#referencing-files-within-signaljourney","title":"Referencing Files within signalJourney","text":"<p>When specifying file paths within <code>inputSources</code> or <code>outputTargets</code> in a signalJourney file placed within a BIDS <code>/derivatives</code> directory:</p> <ul> <li>Use relative paths: Paths should ideally be relative to the BIDS root directory.<ul> <li>Input Raw Data: <code>../rawdata/sub-01/ses-test/eeg/sub-01_ses-test_task-rest_eeg.vhdr</code></li> <li>Input Derivative Data: <code>./derivatives/preprocessing_pipeline/sub-01/eeg/sub-01_desc-preproc_eeg.fif</code> (Note the leading <code>./</code>)</li> <li>Output Derivative Data: <code>./derivatives/ica_pipeline/sub-01/eeg/sub-01_desc-cleaned_eeg.fif</code></li> </ul> </li> <li>This ensures paths remain valid regardless of where the BIDS dataset is moved.</li> <li>Avoid absolute paths unless necessary for external resources.</li> </ul>"},{"location":"bids/#using-bidsignore","title":"Using <code>.bidsignore</code>","text":"<p>To prevent standard BIDS validators (which don't recognize <code>*_signalJourney.json</code>) from generating errors or warnings about these files, add an entry to the <code>.bidsignore</code> file located at the root of your BIDS dataset:</p> <pre><code># Ignore signalJourney provenance files\n*_signalJourney.json\n</code></pre> <p>This tells BIDS-compliant tools to ignore these files during validation scans.</p>"},{"location":"bids/#root-level-vs-derivative-level-pipelines","title":"Root-Level vs. Derivative-Level Pipelines","text":"<p>signalJourney can document pipelines that operate on entire datasets or individual subject/session files.</p> <ul> <li>Subject/Session Level: If a pipeline is run independently for each subject/session, the corresponding <code>*_signalJourney.json</code> file should typically reside alongside the output data within the subject's derivative directory (as shown in the recommended location example).</li> <li>Dataset Level: If a single pipeline instance processes multiple subjects (e.g., group analysis, template creation), a single <code>*_signalJourney.json</code> file might be placed at the root of the specific derivative pipeline directory (e.g., <code>/derivatives/&lt;group_analysis_pipeline&gt;/group_analysis_signalJourney.json</code>). This file would then reference multiple subject input files using relative paths from the BIDS root.</li> </ul>"},{"location":"bids/#linking-signaljourney-files","title":"Linking signalJourney Files","text":"<p>If one pipeline's output (documented in <code>pipelineA_signalJourney.json</code>) is the input to another pipeline (documented in <code>pipelineB_signalJourney.json</code>), use the <code>pipelineSource</code> field within the <code>inputSources</code> of <code>pipelineB_signalJourney.json</code>:</p> <pre><code>// Inside pipelineB_signalJourney.json\n\"inputSources\": [\n  {\n    \"sourceType\": \"file\",\n    \"location\": \"./derivatives/pipelineA/sub-01/eeg/sub-01_desc-outputA_eeg.fif\",\n    \"pipelineSource\": {\n      \"pipelineName\": \"Pipeline A Name\", // Matches pipelineInfo.name in pipelineA_signalJourney.json\n      \"pipelineVersion\": \"1.2.0\",       // Matches pipelineInfo.version in pipelineA_signalJourney.json\n      \"signalJourneyFile\": \"./derivatives/pipelineA/sub-01_desc-outputA_signalJourney.json\" // Optional path to the source file\n    }\n  }\n]\n</code></pre>"},{"location":"bids/#summary-of-recommendations","title":"Summary of Recommendations","text":"<ol> <li>Place <code>*_signalJourney.json</code> files only within <code>/derivatives/&lt;pipeline_name&gt;/</code>.</li> <li>Use filenames that mirror the associated data file, replacing the data suffix with <code>_signalJourney.json</code>.</li> <li>Use relative paths from the BIDS root within <code>inputSources</code> and <code>outputTargets</code>.</li> <li>Add <code>*_signalJourney.json</code> to your top-level <code>.bidsignore</code> file.</li> <li>Store provenance alongside the data it describes (subject/session level where appropriate).</li> <li>Use <code>pipelineSource</code> to link dependent pipelines. </li> </ol>"},{"location":"contributing/","title":"Contributing","text":"<p>Please refer to the main CONTRIBUTING.md file in the repository root.</p> <p>This section will contain documentation-specific contribution guidelines if needed. </p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>Common questions about the signalJourney specification and tools.</p>"},{"location":"faq/#general","title":"General","text":"<p>Q: What is the main purpose of signalJourney?</p> <p>A: To provide a standardized, machine-readable format for documenting the provenance of biosignal processing pipelines. This improves reproducibility, facilitates data sharing, and enables automated analysis of processing steps across studies.</p> <p>Q: How is this different from BIDS?</p> <p>A: BIDS primarily standardizes the organization and metadata of raw and derived neuroimaging data. signalJourney focuses specifically on documenting the processing steps (the pipeline itself) used to generate derived data. They are complementary: signalJourney files can be placed within BIDS derivatives to document how specific BIDS-compliant derivatives were created. See the BIDS Integration guide.</p> <p>Q: Do I need to create signalJourney files manually?</p> <p>A: Not necessarily. While you can create them manually (see Tutorial), the goal is for analysis software (like EEGLAB plugins, MNE-Python scripts) to generate these files automatically as part of the processing workflow. Tools for reading/writing/validating are provided (Python, MATLAB).</p>"},{"location":"faq/#schema-validation","title":"Schema &amp; Validation","text":"<p>Q: Where can I find the official JSON schema?</p> <p>A: The schema is located in the main repository at <code>schema/signalJourney.schema.json</code>.</p> <p>Q: What's the difference between <code>sj_version</code> and <code>schema_version</code>?</p> <p>A: <code>sj_version</code> refers to the version of the overall specification standard the file adheres to. <code>schema_version</code> refers to the specific version of the <code>signalJourney.schema.json</code> file itself. They might differ slightly if schema fixes are made that don't change the core specification. See Versioning.</p> <p>Q: My file looks correct, but fails validation. Why?</p> <p>A: Common reasons include: *   Typos in field names (JSON is case-sensitive). *   Incorrect data types (e.g., using a number where a string is expected). *   Missing required fields. *   Incorrectly formatted values (e.g., dates not in ISO 8601 format, version strings not matching SemVer). *   Invalid structure (e.g., <code>processingSteps</code> not being an array of objects). Use the <code>signaljourney-validate</code> tool (CLI or Python) for detailed error messages and suggestions.</p>"},{"location":"faq/#content-best-practices","title":"Content &amp; Best Practices","text":"<p>Q: How much detail should I include in parameters?</p> <p>A: Include all parameters that were actually used in the function call, even defaults, if possible. The goal is reproducibility. If a parameter value was determined algorithmically, describe the algorithm or reference the relevant input.</p> <p>Q: How should I represent a step that involves manual intervention?</p> <p>A: You can describe the manual step in the <code>description</code> field. For software, you might list the tool used for visualization/interaction. For parameters, describe the criteria used for the manual decision (e.g., <code>\"parameters\": {\"manual_rejection_criteria\": \"Visual inspection for amplitudes &gt; 100 uV\"}</code>). Consider adding a quality metric indicating manual review occurred.</p> <p>Q: Where should I put custom metadata specific to my lab or tool?</p> <p>A: Use the top-level <code>extensions</code> object, organized by namespace. See the Namespaces documentation. Avoid adding custom fields outside of <code>extensions</code>.</p>"},{"location":"faq/#tools","title":"Tools","text":"<p>Q: Do the MATLAB tools perform full schema validation?</p> <p>A: No. <code>validateSignalJourney.m</code> performs only basic structural and type checks on the MATLAB struct. For full schema validation, use the Python library or CLI.</p> <p>(This FAQ will be expanded based on user feedback and common issues.) </p>"},{"location":"introduction/","title":"Introduction to Signal Journey","text":"<p>Signal Journey is a specification for creating machine-readable descriptions of biosignal processing pipelines. Its primary goal is to enhance the reproducibility, transparency, and interoperability of complex data analysis workflows commonly found in fields like neuroscience, electrophysiology, and biomedical engineering.</p>"},{"location":"introduction/#the-problem","title":"The Problem","text":"<p>Modern biosignal analysis involves numerous steps, diverse software tools (MATLAB, Python, EEGLAB, MNE-Python, etc.), and complex parameter settings. Documenting these pipelines accurately is often challenging, relying on:</p> <ul> <li>Handwritten notes in lab notebooks.</li> <li>Comments within analysis scripts.</li> <li>Separate manuscript methods sections.</li> </ul> <p>These methods are prone to errors, omissions, and inconsistencies, making it difficult for researchers to:</p> <ul> <li>Understand precisely how data was processed.</li> <li>Reproduce the results reliably.</li> <li>Compare results across different studies or labs.</li> <li>Re-apply or adapt pipelines to new datasets.</li> </ul>"},{"location":"introduction/#the-solution-signal-journey-files","title":"The Solution: Signal Journey Files","text":"<p>Signal Journey addresses these challenges by defining a standardized JSON format (<code>*_signalJourney.json</code>) to capture the essential details of a processing pipeline. A Signal Journey file describes:</p> <ul> <li>Pipeline Information: Name, version, description.</li> <li>Input Data: Origin and type of the raw data.</li> <li>Processing Steps: A detailed sequence of analysis steps, including:</li> <li>Software used (name, version).</li> <li>Function or command executed.</li> <li>Parameters and their values.</li> <li>Inputs and outputs of each step.</li> <li>Dependencies between steps.</li> <li>Output Data: Description of the final processed data products.</li> <li>Schema Versioning: Ensures compatibility and understanding over time.</li> </ul>"},{"location":"introduction/#key-benefits","title":"Key Benefits","text":"<ul> <li>Standardization: Provides a common language for describing pipelines.</li> <li>Machine Readability: Allows automated validation, parsing, and potentially visualization or re-execution of pipelines.</li> <li>Reproducibility: Captures the critical details needed to replicate an analysis.</li> <li>Transparency: Makes the entire workflow explicit and understandable.</li> <li>Interoperability: Facilitates sharing and comparison of methods across different tools and platforms.</li> </ul>"},{"location":"introduction/#next-steps","title":"Next Steps","text":"<ul> <li>Dive into the detailed Specification.</li> <li>Follow the Tutorials to create or validate your first Signal Journey file.</li> <li>Explore the Examples of common pipelines.</li> <li>Learn how to use the Validation Tools. </li> </ul>"},{"location":"pipeline_research/","title":"Research: Common Pipeline Parameters &amp; Metrics","text":"<p>This document outlines common parameters, functions, and quality metrics used in standard EEG/MEG processing pipelines across popular toolboxes (EEGLAB, MNE-Python, FieldTrip). This research informs the creation of example <code>signalJourney.json</code> files (Task 4).</p> <p>(Note: This is a starting point and requires further detailed research for specific function calls and parameter values.)</p>"},{"location":"pipeline_research/#1-basic-preprocessing","title":"1. Basic Preprocessing","text":"<p>Goal: Initial cleaning (filtering, referencing, basic artifact handling).</p> <p>Common Steps:</p> <ul> <li>Loading Data: Reading raw data formats (e.g., EDF, FIF, SET).</li> <li>Filtering:<ul> <li>High-pass (e.g., 0.5 Hz, 1 Hz)</li> <li>Low-pass (e.g., 40 Hz, 45 Hz, 100 Hz)</li> <li>Notch (e.g., 50 Hz or 60 Hz line noise)</li> </ul> </li> <li>Referencing: Applying a new reference (e.g., Average, Mastoids, Linked-Mastoids).</li> <li>(Optional) Downsampling: Reducing sampling rate.</li> <li>(Optional) Bad Channel Detection/Interpolation: Identifying and handling faulty channels.</li> <li>(Optional) Basic Artifact Rejection: Simple thresholding or visual inspection.</li> </ul> <p>Toolbox Examples (Conceptual):</p> <ul> <li>EEGLAB: <code>pop_loadset()</code>, <code>pop_eegfiltnew()</code>, <code>pop_reref()</code>, <code>pop_resample()</code>, <code>pop_rejchan()</code>, <code>pop_rejcont()</code></li> <li>MNE-Python: <code>mne.io.read_raw_*()</code>, <code>raw.filter()</code>, <code>raw.set_eeg_reference()</code>, <code>raw.resample()</code>, <code>raw.interpolate_bads()</code>, <code>mne.preprocessing.find_bad_channels_maxwell()</code></li> <li>FieldTrip: <code>ft_preprocessing()</code> (with various cfg options for filtering, referencing, resampling), <code>ft_rejectartifact()</code></li> </ul> <p>Common Parameters:</p> <ul> <li>Filter type (FIR, IIR), order, cutoff frequencies.</li> <li>Reference channel(s).</li> <li>New sampling rate.</li> <li>Bad channel detection thresholds/methods.</li> <li>Artifact rejection thresholds.</li> </ul> <p>Quality Metrics:</p> <ul> <li>Number/percentage of bad channels interpolated.</li> <li>Number/percentage of segments rejected (if applicable).</li> <li>Power Spectral Density (PSD) before/after filtering.</li> </ul> <p>Toolbox Examples (Specific):</p> <ul> <li>EEGLAB:<ul> <li><code>EEG = pop_loadset('filename', 'your_data.set');</code></li> <li><code>EEG = pop_eegfiltnew(EEG, 'locutoff', 1, 'hicutoff', 40);</code> % Band-pass</li> <li><code>EEG = pop_eegfiltnew(EEG, 'locutoff', 58, 'hicutoff', 62, 'revfilt', 1);</code> % Notch filter</li> <li><code>EEG = pop_reref( EEG, [65 66] );</code> % Re-reference to channels 65, 66 (e.g., Mastoids)</li> <li><code>EEG = pop_resample( EEG, 250);</code> % Downsample to 250 Hz</li> <li><code>EEG = pop_rejchan(EEG, 'elec',[1:64] ,'threshold',5,'norm','on','measure','kurt');</code> % Detect bad channels by kurtosis</li> <li><code>EEG = pop_interp(EEG, EEG.reject.indelec, 'spherical');</code> % Interpolate bad channels</li> </ul> </li> <li>MNE-Python:<ul> <li><code>raw = mne.io.read_raw_fif('your_data_raw.fif', preload=True)</code></li> <li><code>raw.filter(l_freq=1.0, h_freq=40.0, fir_design='firwin')</code></li> <li><code>raw.notch_filter(freqs=60, fir_design='firwin')</code></li> <li><code>raw.set_eeg_reference(ref_channels=['M1', 'M2'])</code> % Or 'average'</li> <li><code>raw.resample(sfreq=250, npad='auto')</code></li> <li><code>raw.info['bads'] = ['EEG 053']</code> % Manually mark bad channel</li> <li><code>raw.interpolate_bads(reset_bads=True)</code></li> </ul> </li> <li>FieldTrip:<ul> <li><code>cfg = []; cfg.dataset = 'your_data.ds'; data_raw = ft_preprocessing(cfg);</code></li> <li><code>cfg = []; cfg.hpfilter = 'yes'; cfg.hpfreq = 1; cfg.lpfilter = 'yes'; cfg.lpfreq = 40; data_filt = ft_preprocessing(cfg, data_raw);</code></li> <li><code>cfg = []; cfg.bsfilter = 'yes'; cfg.bsfreq = [58 62]; data_notch = ft_preprocessing(cfg, data_filt);</code></li> <li><code>cfg = []; cfg.reref = 'yes'; cfg.refchannel = {'REF1', 'REF2'}; data_reref = ft_preprocessing(cfg, data_notch);</code> % REF1/2 are e.g., mastoid channels</li> <li><code>cfg = []; cfg.resamplefs = 250; data_resampled = ft_resampledata(cfg, data_reref);</code></li> <li><code>cfg = []; cfg.method = 'summary'; cfg.metric = 'zvalue'; data_clean = ft_rejectvisual(cfg, data_resampled);</code> % Visual inspection/summary for bad channels/trials</li> </ul> </li> </ul> <p>Common Parameters:</p> <ul> <li>Filter: Cutoff frequencies (<code>locutoff</code>/<code>hicutoff</code>, <code>l_freq</code>/<code>h_freq</code>, <code>hpfreq</code>/<code>lpfreq</code>), filter type (<code>fir_design</code>, filter order), notch frequencies (<code>bsfreq</code>).</li> <li>Reference: Reference channel names/indices (<code>refchannel</code>, <code>ref_channels</code>), or 'average'.</li> <li>Sampling: New sampling rate (<code>sfreq</code>, <code>resamplefs</code>).</li> <li>Bad Channels: Detection method/thresholds (<code>measure</code>, <code>threshold</code>), interpolation method (<code>spherical</code>, <code>reset_bads</code>).</li> </ul>"},{"location":"pipeline_research/#2-ica-decomposition-cleaning","title":"2. ICA Decomposition &amp; Cleaning","text":"<p>Goal: Identify and remove artifactual components (blinks, muscle, heartbeats).</p> <p>Common Steps:</p> <ul> <li>Preprocessing: Typically includes high-pass filtering (e.g., 1 Hz) and potentially bad channel removal.</li> <li>Run ICA: Applying an ICA algorithm (e.g., runica, infomax, fastica, picard).</li> <li>Component Classification: Identifying artifactual components (manually or using automated classifiers like ICLabel, SASICA).</li> <li>Component Removal: Subtracting artifactual components from the data.</li> </ul> <p>Toolbox Examples (Conceptual):</p> <ul> <li>EEGLAB: <code>pop_runica()</code>, <code>pop_iclabel()</code>, <code>pop_subcomp()</code></li> <li>MNE-Python: <code>mne.preprocessing.ICA()</code>, <code>ica.fit()</code>, <code>ica.find_bads_eog()</code>, <code>ica.find_bads_ecg()</code>, <code>ica.apply()</code></li> <li>FieldTrip: <code>ft_componentanalysis()</code> (with cfg.method = 'runica'), component selection often manual or via external scripts, <code>ft_rejectcomponent()</code></li> </ul> <p>Common Parameters:</p> <ul> <li>ICA algorithm choice.</li> <li>Number of components (or rank estimation).</li> <li>Component classification thresholds/method.</li> <li>Indices of components to remove.</li> </ul> <p>Quality Metrics:</p> <ul> <li>Number/percentage of components removed.</li> <li>Variance explained by removed components.</li> <li>Comparison of data before/after component removal (e.g., PSD, ERPs).</li> <li>ICLabel classification probabilities (if used).</li> </ul> <p>Toolbox Examples (Specific):</p> <ul> <li>EEGLAB:<ul> <li><code>EEG = pop_runica(EEG, 'icatype', 'runica', 'extended',1,'interrupt','on');</code></li> <li><code>EEG = iclabel(EEG);</code> % Run ICLabel classifier</li> <li><code>EEG = pop_icflag(EEG, [NaN NaN;0.8 1;0.8 1;NaN NaN;NaN NaN;NaN NaN;NaN NaN]);</code> % Flag components based on ICLabel thresholds (example: Muscle &amp; Eye &gt; 80%)</li> <li><code>EEG = pop_subcomp( EEG, find(EEG.reject.gcompreject), 0);</code> % Remove flagged components</li> </ul> </li> <li>MNE-Python:<ul> <li><code>ica = mne.preprocessing.ICA(n_components=15, method='fastica', random_state=97, max_iter='auto')</code></li> <li><code>ica.fit(raw_filt, picks='eeg')</code> % Fit on filtered data</li> <li><code>eog_indices, eog_scores = ica.find_bads_eog(raw, ch_name='EOG061')</code> % Find EOG components</li> <li><code>ica.exclude = eog_indices</code> % Mark components for exclusion</li> <li><code>ica.apply(raw)</code> % Apply ICA to remove components</li> </ul> </li> <li>FieldTrip:<ul> <li><code>cfg = []; cfg.method = 'runica'; comp = ft_componentanalysis(cfg, data_clean);</code></li> <li>% Manual component selection usually follows (visual inspection of topography, time course)</li> <li><code>cfg = []; cfg.component = [3 5 12]; data_postica = ft_rejectcomponent(cfg, comp, data_clean);</code> % Remove components 3, 5, 12 (example)</li> </ul> </li> </ul> <p>Common Parameters:</p> <ul> <li>ICA algorithm (<code>icatype</code>, <code>method</code>).</li> <li>Number of components (<code>n_components</code>) or data rank.</li> <li>Component classification method (ICLabel thresholds, <code>find_bads_eog</code>, manual selection).</li> <li>Indices of components to remove (<code>component</code>, <code>ica.exclude</code>).</li> </ul>"},{"location":"pipeline_research/#3-time-frequency-analysis","title":"3. Time-Frequency Analysis","text":"<p>Goal: Analyze spectral power/phase over time.</p> <p>Common Steps:</p> <ul> <li>Preprocessing: As needed (filtering, artifact removal).</li> <li>Time-Frequency Decomposition: Applying methods like Morlet wavelets, multitaper FFT, Hilbert transform.</li> <li>(Optional) Baseline Correction: Subtracting/dividing by baseline period activity.</li> <li>(Optional) Statistical Analysis: Comparing conditions.</li> </ul> <p>Toolbox Examples (Conceptual):</p> <ul> <li>EEGLAB: <code>pop_newtimef()</code></li> <li>MNE-Python: <code>mne.time_frequency.tfr_morlet()</code>, <code>mne.time_frequency.tfr_multitaper()</code>, <code>mne.time_frequency.tfr_stockwell()</code></li> <li>FieldTrip: <code>ft_freqanalysis()</code> (with cfg.method = 'mtmconvol', 'wavelet', 'tfr')</li> </ul> <p>Common Parameters:</p> <ul> <li>Frequency range and resolution.</li> <li>Time window(s).</li> <li>Method parameters (e.g., number of wavelet cycles, multitaper smoothing).</li> <li>Baseline period.</li> <li>Statistical correction method.</li> </ul> <p>Quality Metrics:</p> <ul> <li>Signal-to-Noise Ratio (SNR) of evoked power.</li> <li>Statistical significance values (p-values).</li> </ul> <p>Toolbox Examples (Specific):</p> <ul> <li>EEGLAB:<ul> <li><code>[ersp,itc,powbase,times,freqs] = pop_newtimef( EEG, 1, 1, [-1000  2000], [3         0.5] , 'topovec', 1, 'elocs', EEG.chanlocs, 'chaninfo', EEG.chaninfo, 'baseline',[-500 0], 'freqs', [2 40], 'plotphase', 'off', 'plotersp', 'off', 'plotitc', 'off');</code> % Wavelet analysis on channel 1, baseline [-500 0]ms</li> </ul> </li> <li>MNE-Python:<ul> <li><code>freqs = np.arange(2., 40., 1.)</code> % Frequencies of interest</li> <li><code>n_cycles = freqs / 2.</code> % Number of cycles in Morlet wavelets</li> <li><code>power = mne.time_frequency.tfr_morlet(epochs['ConditionA'], freqs=freqs, n_cycles=n_cycles, use_fft=True, return_itc=False, decim=3, n_jobs=1)</code> % Calculate power for epochs</li> <li><code>power.apply_baseline(baseline=(-0.5, 0), mode='logratio')</code> % Baseline correction</li> </ul> </li> <li>FieldTrip:<ul> <li><code>cfg = []; cfg.method = 'mtmconvol'; cfg.taper = 'hanning'; cfg.foi = 2:1:40;</code> % Frequencies 2 to 40 Hz</li> <li><code>cfg.t_ftimwin = ones(length(cfg.foi),1).*0.5;</code>   % Time window 500ms</li> <li><code>cfg.toi = -1:0.05:2;</code> % Time points -1 to 2 sec, 50ms steps</li> <li><code>freq = ft_freqanalysis(cfg, data_epoched);</code></li> <li><code>cfg = []; cfg.baseline = [-0.5 0]; cfg.baselinetype = 'relative'; freq_bl = ft_freqbaseline(cfg, freq);</code></li> </ul> </li> </ul> <p>Common Parameters:</p> <ul> <li>Frequencies (<code>freqs</code>, <code>foi</code>).</li> <li>Time window/points of interest (<code>times</code>, <code>toi</code>).</li> <li>Method (<code>mtmconvol</code>, <code>wavelet</code>, <code>tfr_morlet</code>).</li> <li>Method-specific parameters: cycles (<code>n_cycles</code>), taper (<code>taper</code>), time-window (<code>t_ftimwin</code>).</li> <li>Baseline period (<code>baseline</code>, <code>cfg.baseline</code>), baseline type (<code>mode</code>, <code>baselinetype</code>).</li> </ul>"},{"location":"pipeline_research/#4-source-localization","title":"4. Source Localization","text":"<p>Goal: Estimate the location of neural activity within the brain.</p> <p>Common Steps:</p> <ul> <li>Preprocessing: Thorough cleaning often required.</li> <li>Forward Modeling: Creating a head model (BEM, FEM, spherical) and leadfield matrix based on sensor locations and head geometry.</li> <li>Covariance Estimation: Calculating data covariance matrix.</li> <li>Inverse Solution: Applying an inverse method (e.g., MNE, dSPM, sLORETA, LCMV Beamformer).</li> <li>(Optional) Morphing to Template: Transforming source estimates to a standard brain space (e.g., MNI, fsaverage).</li> </ul> <p>Toolbox Examples (Conceptual):</p> <ul> <li>EEGLAB (via plugins like FieldTrip or custom scripts): <code>pop_dipfit_settings()</code>, <code>pop_dipfit_selectcomps()</code> (for component dipoles)</li> <li>MNE-Python: <code>mne.make_bem_model()</code>, <code>mne.make_bem_solution()</code>, <code>mne.setup_source_space()</code>, <code>mne.make_forward_solution()</code>, <code>mne.compute_covariance()</code>, <code>mne.minimum_norm.make_inverse_operator()</code>, <code>mne.minimum_norm.apply_inverse()</code>, <code>mne.beamformer.make_lcmv()</code>, <code>mne.beamformer.apply_lcmv()</code></li> <li>FieldTrip: <code>ft_prepare_headmodel()</code>, <code>ft_prepare_leadfield()</code>, <code>ft_timelockanalysis()</code> (for covariance), <code>ft_sourceanalysis()</code> (with various cfg.method options)</li> </ul> <p>Common Parameters:</p> <ul> <li>Head model type and parameters (conductivity).</li> <li>Source space definition (surface/volume, resolution).</li> <li>Inverse method choice and regularization parameters (e.g., SNR, lambda2).</li> <li>Covariance estimation parameters (time window, baseline).</li> </ul> <p>Quality Metrics:</p> <ul> <li>Goodness of Fit (GoF) for dipole fitting.</li> <li>Explained variance.</li> <li>Resolution metrics (e.g., Point Spread Function).</li> <li>Cross-validation results.</li> </ul> <p>Toolbox Examples (Specific):</p> <ul> <li>EEGLAB (Dipfit Plugin):<ul> <li><code>EEG = pop_dipfit_settings( EEG, 'hdmfile','standard_BEM.mat','coordformat','MNI', ...);</code> % Set head model</li> <li><code>EEG = pop_multifit( EEG, [1:15] , 'threshold', 100);</code> % Fit dipoles to ICA components 1-15</li> <li><code>EEG = pop_dipfit_select( EEG, 'percent', 15 );</code> % Select dipoles with residual variance &lt; 15%</li> </ul> </li> <li>MNE-Python:<ul> <li><code>subjects_dir = mne.datasets.fetch_fsaverage()</code></li> <li><code>src = mne.setup_source_space(subject='fsaverage', spacing='oct6', subjects_dir=subjects_dir, add_dist=False)</code> % Setup source space</li> <li><code>conductivity = (0.3,)</code> # for single layer BEM</li> <li><code>model = mne.make_bem_model(subject='fsaverage', ico=4, conductivity=conductivity, subjects_dir=subjects_dir)</code></li> <li><code>bem_sol = mne.make_bem_solution(model)</code></li> <li><code>fwd = mne.make_forward_solution(epochs.info, trans='fsaverage', src=src, bem=bem_sol, meg=False, eeg=True, mindist=5.0)</code></li> <li><code>cov = mne.compute_covariance(epochs, tmax=0., method=['shrunk', 'empirical'])</code> % Compute covariance from baseline</li> <li><code>inverse_operator = make_inverse_operator(epochs.info, fwd, cov, loose=0.2, depth=0.8)</code></li> <li><code>method = \"dSPM\"</code></li> <li><code>snr = 3.0</code></li> <li><code>lambda2 = 1.0 / snr ** 2</code></li> <li><code>stc = apply_inverse(evoked, inverse_operator, lambda2, method=method, pick_ori=None)</code> % Apply dSPM</li> </ul> </li> <li>FieldTrip:<ul> <li><code>cfg = []; cfg.method = 'standard_bem'; headmodel = ft_prepare_headmodel(cfg, segmented_mri);</code> % Create BEM head model</li> <li><code>cfg = []; cfg.grid.resolution = 10; cfg.headmodel = headmodel; leadfield = ft_prepare_leadfield(cfg, data_avg);</code> % Create leadfield grid</li> <li><code>cfg = []; cfg.method = 'lcmv'; cfg.headmodel = headmodel; cfg.grid = leadfield; cfg.lcmv.lambda = '5%'; source = ft_sourceanalysis(cfg, data_avg);</code> % LCMV beamformer</li> </ul> </li> </ul> <p>Common Parameters:</p> <ul> <li>Head model file/type (<code>hdmfile</code>, <code>method='standard_bem'</code>, <code>make_bem_model</code>).</li> <li>Source space definition (<code>spacing</code>, <code>grid.resolution</code>).</li> <li>Inverse method (<code>method='dSPM'</code>, <code>method='lcmv'</code>).</li> <li>Regularization (<code>lambda2</code>, <code>lcmv.lambda</code>).</li> <li>Covariance matrix calculation parameters (<code>tmax</code>, time window).</li> </ul>"},{"location":"pipeline_research/#5-connectivity-analysis","title":"5. Connectivity Analysis","text":"<p>Goal: Measure statistical dependencies between signals from different sensors or sources.</p> <p>Common Steps:</p> <ul> <li>Preprocessing: Careful cleaning is crucial.</li> <li>Connectivity Measure Calculation: Applying measures like Coherence, Phase Locking Value (PLV), Phase Lag Index (PLI/wPLI), Granger Causality.</li> <li>(Optional) Source Reconstruction: Applying connectivity analysis in source space.</li> <li>Statistical Assessment: Comparing conditions, correcting for multiple comparisons.</li> </ul> <p>Toolbox Examples (Conceptual):</p> <ul> <li>EEGLAB (via plugins like SIFT, BCILAB or custom scripts): Various toolboxes available.</li> <li>MNE-Python: <code>mne_connectivity.spectral_connectivity_epochs()</code>, <code>mne_connectivity.phase_amplitude_coupling()</code></li> <li>FieldTrip: <code>ft_connectivityanalysis()</code> (with various cfg.method options)</li> </ul> <p>Common Parameters:</p> <ul> <li>Connectivity measure choice.</li> <li>Frequency bands of interest.</li> <li>Time windows.</li> <li>Statistical thresholding/correction methods.</li> <li>Model order (for Granger causality).</li> </ul> <p>Quality Metrics:</p> <ul> <li>Statistical significance values.</li> <li>Comparison to surrogate data.</li> <li>Consistency across time/frequency.</li> </ul> <p>Toolbox Examples (Specific):</p> <ul> <li>EEGLAB (SIFT Plugin/Custom Scripts):<ul> <li>% (Requires specific toolboxes like SIFT) Example concepts:</li> <li><code>EEG = pop_est_sourcemodel(EEG, 'model','AR', 'order', 5);</code> % Estimate VAR model</li> <li><code>EEG = pop_est_connectivity(EEG, 'connmethods', {'dDTF'});</code> % Calculate directed Transfer Function</li> </ul> </li> <li>MNE-Python (mne-connectivity):<ul> <li><code>con = mne_connectivity.spectral_connectivity_epochs(epochs['ConditionA'], method='coh', mode='multitaper', sfreq=epochs.info['sfreq'], fmin=8., fmax=13., faverage=True, mt_adaptive=True, n_jobs=1)</code> % Calculate coherence in alpha band</li> <li><code>con_pli = mne_connectivity.spectral_connectivity_epochs(epochs['ConditionB'], method='pli', sfreq=epochs.info['sfreq'], fmin=4., fmax=8.)</code> % Calculate PLI in theta band</li> </ul> </li> <li>FieldTrip:<ul> <li><code>cfg = []; cfg.method = 'coh'; cfg.complex = 'complex'; freq_coh = ft_connectivityanalysis(cfg, freq);</code> % Calculate coherence</li> <li><code>cfg = []; cfg.method = 'plv'; freq_plv = ft_connectivityanalysis(cfg, freq);</code> % Calculate Phase Locking Value</li> </ul> </li> </ul> <p>Common Parameters:</p> <ul> <li>Connectivity measure (<code>connmethods</code>, <code>method</code> in <code>spectral_connectivity_epochs</code>, <code>cfg.method</code>).</li> <li>Frequency range (<code>fmin</code>, <code>fmax</code>, <code>foi</code>).</li> <li>Mode/tapering (<code>mode</code>, <code>mt_adaptive</code>, <code>cfg.taper</code>).</li> <li>Model order (for VAR/Granger).</li> </ul> <p>Quality Metrics:</p> <ul> <li>Statistical significance values (p-values, often from permutation tests).</li> <li>Comparison to surrogate data (e.g., phase-randomized).</li> <li>Consistency across time/frequency/subjects.</li> </ul> <p>(Note: Parameter values are illustrative and should be chosen based on specific experimental design and data characteristics.) </p>"},{"location":"api/","title":"API Reference Index","text":"<p>API documentation for the signalJourney libraries.</p>"},{"location":"api/#api-reference","title":"API Reference","text":"<p>This section provides the API documentation for the <code>signaljourney_validator</code> package.</p>"},{"location":"api/#signaljourney_validator","title":"<code>signaljourney_validator</code>","text":""},{"location":"api/#signaljourney_validator.ValidationErrorDetail","title":"<code>ValidationErrorDetail</code>  <code>dataclass</code>","text":"<p>Represents a detailed validation error.</p>"},{"location":"api/#signaljourney_validator.ValidationErrorDetail.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Generate suggestions based on error type after initialization.</p>"},{"location":"api/#signaljourney_validator.SignalJourneyValidationError","title":"<code>SignalJourneyValidationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for validation errors.</p>"},{"location":"api/#signaljourney_validator.Validator","title":"<code>Validator</code>","text":"<p>Validates a signalJourney JSON file or dictionary against the schema. Optionally performs BIDS context validation. Supports version-based schema validation.</p>"},{"location":"api/#signaljourney_validator.Validator.__init__","title":"<code>__init__(schema=None, schema_version=None, schema_dir=None)</code>","text":"<p>Initializes the Validator.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Optional[Union[Path, str, JsonDict]]</code> <p>Path to the schema file, the schema dictionary, or None     to use version-based schema loading. External file $refs will be     automatically inlined during initialization.</p> <code>None</code> <code>schema_version</code> <code>Optional[str]</code> <p>Specific schema version to use (e.g., \"0.1.0\").            If None, will auto-detect from data during validation.</p> <code>None</code> <code>schema_dir</code> <code>Optional[Path]</code> <p>Custom schema directory path. If None, uses default.</p> <code>None</code>"},{"location":"api/#signaljourney_validator.Validator.validate","title":"<code>validate(data, raise_exceptions=True, bids_context=None, auto_detect_version=True)</code>","text":"<p>Validates the given data against the appropriate signalJourney schema.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Path, str, JsonDict]</code> <p>Path to the JSON file, the JSON string, a dictionary   representing the JSON data.</p> required <code>raise_exceptions</code> <code>bool</code> <p>If True (default), raises SignalJourneyValidationError               on the first failure. If False, returns a list of               all validation errors found.</p> <code>True</code> <code>bids_context</code> <code>Optional[Path]</code> <p>Optional Path to the BIDS dataset root directory.           If provided, enables BIDS context validation checks           (e.g., file existence relative to the root).</p> <code>None</code> <code>auto_detect_version</code> <code>bool</code> <p>If True (default), automatically detects the schema                 version from the data and uses the appropriate schema.                 If False, uses the validator's configured schema.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[ValidationErrorDetail]</code> <p>A list of ValidationErrorDetail objects if raise_exceptions is False</p> <code>List[ValidationErrorDetail]</code> <p>and validation fails. Returns an empty list if validation succeeds.</p> <p>Raises:</p> Type Description <code>SignalJourneyValidationError</code> <p>If validation fails and                       raise_exceptions is True.</p> <code>FileNotFoundError</code> <p>If data file/path does not exist.</p> <code>TypeError</code> <p>If data is not a Path, string, or dictionary.</p> <code>ValueError</code> <p>If detected schema version is not supported.</p>"},{"location":"api/#signaljourney_validator.Validator.get_supported_versions","title":"<code>get_supported_versions()</code>","text":"<p>Get list of supported schema versions.</p>"},{"location":"api/#signaljourney_validator.Validator.get_current_version","title":"<code>get_current_version()</code>","text":"<p>Get the currently configured schema version.</p>"},{"location":"api/#signaljourney_validator.Validator.get_latest_version","title":"<code>get_latest_version()</code>","text":"<p>Get the latest available schema version.</p>"},{"location":"api/#validator-module","title":"Validator Module","text":""},{"location":"api/#signaljourney_validator.validator","title":"<code>signaljourney_validator.validator</code>","text":""},{"location":"api/#signaljourney_validator.validator.Validator","title":"<code>Validator</code>","text":"<p>Validates a signalJourney JSON file or dictionary against the schema. Optionally performs BIDS context validation. Supports version-based schema validation.</p> Source code in <code>src/signaljourney_validator/validator.py</code> <pre><code>class Validator:\n    \"\"\"\n    Validates a signalJourney JSON file or dictionary against the schema.\n    Optionally performs BIDS context validation.\n    Supports version-based schema validation.\n    \"\"\"\n\n    _schema: JsonDict\n    _validator: Draft202012Validator\n    _registry: SchemaVersionRegistry\n    _schema_version: Optional[str]\n\n    def __init__(\n        self,\n        schema: Optional[Union[Path, str, JsonDict]] = None,\n        schema_version: Optional[str] = None,\n        schema_dir: Optional[Path] = None,\n    ):\n        \"\"\"\n        Initializes the Validator.\n\n        Args:\n            schema: Path to the schema file, the schema dictionary, or None\n                    to use version-based schema loading. External file $refs will be\n                    automatically inlined during initialization.\n            schema_version: Specific schema version to use (e.g., \"0.1.0\").\n                           If None, will auto-detect from data during validation.\n            schema_dir: Custom schema directory path. If None, uses default.\n        \"\"\"\n        self._registry = SchemaVersionRegistry(schema_dir)\n        self._schema_version = schema_version\n\n        if schema is not None:\n            # Use provided schema (legacy behavior)\n            schema_path = self._get_schema_path(schema)\n            initial_schema = self._load_schema_dict(schema, schema_path)\n            base_resolve_path = (\n                schema_path.parent if schema_path else DEFAULT_SCHEMA_PATH.parent\n            )\n        elif schema_version is not None:\n            # Use specific version from registry\n            if not self._registry.is_version_supported(schema_version):\n                raise ValueError(\n                    f\"Schema version '{schema_version}' is not supported. \"\n                    f\"Available versions: {self._registry.get_supported_versions()}\"\n                )\n            schema_path = self._registry.get_schema_path(schema_version)\n            initial_schema = self._registry.load_schema(schema_version)\n            base_resolve_path = schema_path.parent\n        else:\n            # Use latest version as default\n            try:\n                latest_version = self._registry.get_latest_version()\n                self._schema_version = latest_version\n                schema_path = self._registry.get_schema_path(latest_version)\n                initial_schema = self._registry.load_schema(latest_version)\n                base_resolve_path = schema_path.parent\n            except Exception:\n                # Fallback to legacy schema path if registry fails\n                schema_path = DEFAULT_SCHEMA_PATH\n                initial_schema = self._load_schema_dict(None, schema_path)\n                base_resolve_path = schema_path.parent\n\n        # Inline external $refs\n        print(\"\\n--- Inlining schema refs within Validator ---\")\n        loaded_cache = {}\n        self._schema = inline_refs(initial_schema, base_resolve_path, loaded_cache)\n        print(\"--- Inlining complete --- \\n\")\n\n        # Initialize the validator with the resolved schema.\n        try:\n            # Check if schema is valid before creating validator\n            Draft202012Validator.check_schema(self._schema)\n            self._validator = Draft202012Validator(\n                schema=self._schema,\n            )\n        except jsonschema.SchemaError as e:\n            raise SignalJourneyValidationError(f\"Invalid schema provided: {e}\") from e\n\n    def _get_schema_path(\n        self, schema_input: Optional[Union[Path, str, JsonDict]]\n    ) -&gt; Optional[Path]:\n        \"\"\"Determines the Path object if schema is given as Path or str.\"\"\"\n        if schema_input is None:\n            return DEFAULT_SCHEMA_PATH\n        if isinstance(schema_input, Path):\n            return schema_input\n        if isinstance(schema_input, str):\n            return Path(schema_input)\n        return None\n\n    def _load_schema_dict(\n        self,\n        schema_input: Optional[Union[Path, str, JsonDict]],\n        schema_path: Optional[Path],\n    ) -&gt; JsonDict:\n        \"\"\"Loads the schema into a dictionary.\"\"\"\n        if isinstance(schema_input, dict):\n            return schema_input.copy()  # Return a copy\n\n        # Determine path to load from\n        load_path = schema_path if schema_path else DEFAULT_SCHEMA_PATH\n\n        if not load_path or not load_path.exists():\n            raise FileNotFoundError(f\"Schema file not found: {load_path}\")\n        try:\n            with open(load_path, \"r\", encoding=\"utf-8\") as f:\n                return json.load(f)\n        except Exception as e:\n            raise IOError(f\"Error reading schema file {load_path}: {e}\") from e\n\n    def _create_validator_for_version(self, version: str) -&gt; Draft202012Validator:\n        \"\"\"\n        Create a validator for a specific schema version.\n\n        Args:\n            version: Schema version string\n\n        Returns:\n            Configured validator for the specified version\n        \"\"\"\n        if not self._registry.is_version_supported(version):\n            raise ValueError(\n                f\"Schema version '{version}' is not supported. \"\n                f\"Available versions: {self._registry.get_supported_versions()}\"\n            )\n\n        schema_path = self._registry.get_schema_path(version)\n        schema_dict = self._registry.load_schema(version)\n\n        # Inline external $refs\n        loaded_cache = {}\n        resolved_schema = inline_refs(schema_dict, schema_path.parent, loaded_cache)\n\n        # Create and return validator\n        Draft202012Validator.check_schema(resolved_schema)\n        return Draft202012Validator(schema=resolved_schema)\n\n    def validate(\n        self,\n        data: Union[Path, str, JsonDict],\n        raise_exceptions: bool = True,\n        bids_context: Optional[Path] = None,\n        auto_detect_version: bool = True,\n    ) -&gt; List[ValidationErrorDetail]:\n        \"\"\"\n        Validates the given data against the appropriate signalJourney schema.\n\n        Args:\n            data: Path to the JSON file, the JSON string, a dictionary\n                  representing the JSON data.\n            raise_exceptions: If True (default), raises SignalJourneyValidationError\n                              on the first failure. If False, returns a list of\n                              all validation errors found.\n            bids_context: Optional Path to the BIDS dataset root directory.\n                          If provided, enables BIDS context validation checks\n                          (e.g., file existence relative to the root).\n            auto_detect_version: If True (default), automatically detects the schema\n                                version from the data and uses the appropriate schema.\n                                If False, uses the validator's configured schema.\n\n        Returns:\n            A list of ValidationErrorDetail objects if raise_exceptions is False\n            and validation fails. Returns an empty list if validation succeeds.\n\n        Raises:\n            SignalJourneyValidationError: If validation fails and\n                                      raise_exceptions is True.\n            FileNotFoundError: If data file/path does not exist.\n            TypeError: If data is not a Path, string, or dictionary.\n            ValueError: If detected schema version is not supported.\n        \"\"\"\n        instance: JsonDict\n        file_path_context: Optional[Path] = None  # For BIDS checks\n\n        # --- Load Instance ---\n        if isinstance(data, (Path, str)):\n            file_path = Path(data)\n            file_path_context = file_path  # Store for BIDS checks\n            if not file_path.exists():\n                raise FileNotFoundError(f\"Data file not found: {file_path}\")\n            try:\n                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                    instance = json.load(f)\n            except json.JSONDecodeError as e:\n                raise SignalJourneyValidationError(\n                    f\"Error decoding data JSON from {file_path}: {e}\"\n                ) from e\n            except Exception as e:\n                raise SignalJourneyValidationError(\n                    f\"Error loading data file from {file_path}: {e}\"\n                ) from e\n        elif isinstance(data, dict):\n            instance = data\n        else:\n            raise TypeError(\"Data must be a Path, string, or dictionary.\")\n\n        # --- Version Detection and Validator Selection ---\n        validator_to_use = self._validator\n        detected_version = None\n\n        if auto_detect_version:\n            try:\n                detected_version = self._registry.detect_schema_version(instance)\n                if detected_version:\n                    if not self._registry.is_version_supported(detected_version):\n                        error_msg = (\n                            f\"Detected schema version '{detected_version}' is not \"\n                            f\"supported. Available versions: \"\n                            f\"{self._registry.get_supported_versions()}\"\n                        )\n                        if raise_exceptions:\n                            raise SignalJourneyValidationError(error_msg)\n                        else:\n                            # Return a validation error\n                            return [\n                                ValidationErrorDetail(\n                                    message=error_msg,\n                                    path=[\"schema_version\"],\n                                    schema_path=[],\n                                    validator=\"version_support\",\n                                    validator_value=self._registry.get_supported_versions(),\n                                    instance_value=detected_version,\n                                )\n                            ]\n\n                    # Use version-specific validator if different from current\n                    if detected_version != self._schema_version:\n                        validator_to_use = self._create_validator_for_version(\n                            detected_version\n                        )\n\n                elif self._schema_version is None:\n                    # No version detected and no default configured\n                    error_msg = (\n                        \"No schema_version field found in data and no default version \"\n                        \"configured. Please specify a schema_version in your \"\n                        \"signalJourney file.\"\n                    )\n                    if raise_exceptions:\n                        raise SignalJourneyValidationError(error_msg)\n                    else:\n                        return [\n                            ValidationErrorDetail(\n                                message=error_msg,\n                                path=[\"schema_version\"],\n                                schema_path=[],\n                                validator=\"required\",\n                                validator_value=True,\n                                instance_value=None,\n                            )\n                        ]\n\n            except (FileNotFoundError, IOError) as e:\n                # Re-raise file system errors\n                raise e\n            except Exception as e:\n                # Handle version detection errors\n                error_msg = f\"Error during schema version detection: {e}\"\n                if raise_exceptions:\n                    raise SignalJourneyValidationError(error_msg) from e\n                else:\n                    return [\n                        ValidationErrorDetail(\n                            message=error_msg,\n                            path=[\"schema_version\"],\n                            schema_path=[],\n                            validator=\"version_detection\",\n                            validator_value=None,\n                            instance_value=instance.get(\"schema_version\"),\n                        )\n                    ]\n\n        schema_errors: List[ValidationErrorDetail] = []\n        bids_errors: List[ValidationErrorDetail] = []\n\n        # --- Schema Validation ---\n        # Use the selected validator's iter_errors method\n        try:\n            errors = sorted(\n                validator_to_use.iter_errors(instance), key=lambda e: e.path\n            )\n            if errors:\n                for error in errors:\n                    # Convert jsonschema error to our custom format\n                    detail = ValidationErrorDetail(\n                        message=error.message,\n                        path=list(error.path),\n                        schema_path=list(error.schema_path),\n                        validator=error.validator,\n                        validator_value=error.validator_value,\n                        instance_value=error.instance,\n                        # suggestion is added by ValidationErrorDetail constructor\n                    )\n                    schema_errors.append(detail)\n        except jsonschema.RefResolutionError as e:\n            # This shouldn't happen if schema is fully resolved, but handle defensively\n            print(f\"DEBUG: Unexpected RefResolutionError: {e}\")\n            failed_ref = getattr(e, \"ref\", \"[unknown ref]\")\n            raise SignalJourneyValidationError(\n                f\"Schema validation failed: Unexpectedly could not resolve \"\n                f\"reference '{failed_ref}'\"\n            ) from e\n        except jsonschema.SchemaError as e:\n            # Catch schema errors separately from resolution errors\n            raise SignalJourneyValidationError(f\"Invalid schema: {e}\") from e\n        except Exception as e:\n            # Capture the actual exception type for better debugging\n            print(\n                f\"DEBUG: Unexpected validation error type: \"\n                f\"{type(e).__name__}, Error: {e}\"\n            )\n            # Reraise or wrap depending on desired behavior\n            raise SignalJourneyValidationError(\n                f\"An unexpected error occurred during schema validation: {e}\"\n            ) from e\n\n        # --- BIDS Context Validation (Optional) ---\n        if bids_context:\n            bids_errors = self._validate_bids_context(\n                instance, file_path_context, bids_context\n            )\n\n        all_errors = schema_errors + bids_errors\n\n        # --- Result Handling ---\n        if all_errors:\n            if raise_exceptions:\n                raise SignalJourneyValidationError(\n                    \"Validation failed.\", errors=all_errors\n                )\n            else:\n                return all_errors\n        else:\n            return []  # Success\n\n    def _validate_bids_context(\n        self, instance: JsonDict, file_path: Optional[Path], bids_root: Path\n    ) -&gt; List[ValidationErrorDetail]:\n        \"\"\"Placeholder for BIDS context validation logic.\"\"\"\n        errors: List[ValidationErrorDetail] = []\n        print(\n            f\"[INFO] BIDS context validation requested for {file_path} \"\n            f\"within {bids_root} (Not implemented)\"\n        )\n\n        # TODO: Implement BIDS checks using file_path and bids_root\n        # Examples:\n        # - Check if file_path is correctly placed within bids_root derivatives\n        # - Check naming convention against BIDS standards (might need pybids)\n        # - Check if files referenced in inputSources/outputTargets exist\n        #   relative to bids_root\n        # - Differentiate rules for root-level vs derivative-level journey files\n\n        # Example error:\n        # if some_bids_check_fails:\n        #     errors.append(ValidationErrorDetail(\n        #         message=\"BIDS Check Failed: Reason...\",\n        #         path=['relevant', 'path']\n        #     ))\n\n        return errors\n\n    def get_supported_versions(self) -&gt; List[str]:\n        \"\"\"Get list of supported schema versions.\"\"\"\n        return self._registry.get_supported_versions()\n\n    def get_current_version(self) -&gt; Optional[str]:\n        \"\"\"Get the currently configured schema version.\"\"\"\n        return self._schema_version\n\n    def get_latest_version(self) -&gt; Optional[str]:\n        \"\"\"Get the latest available schema version.\"\"\"\n        return self._registry.get_latest_version()\n</code></pre>"},{"location":"api/#signaljourney_validator.validator.Validator.__init__","title":"<code>__init__(schema=None, schema_version=None, schema_dir=None)</code>","text":"<p>Initializes the Validator.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Optional[Union[Path, str, JsonDict]]</code> <p>Path to the schema file, the schema dictionary, or None     to use version-based schema loading. External file $refs will be     automatically inlined during initialization.</p> <code>None</code> <code>schema_version</code> <code>Optional[str]</code> <p>Specific schema version to use (e.g., \"0.1.0\").            If None, will auto-detect from data during validation.</p> <code>None</code> <code>schema_dir</code> <code>Optional[Path]</code> <p>Custom schema directory path. If None, uses default.</p> <code>None</code> Source code in <code>src/signaljourney_validator/validator.py</code> <pre><code>def __init__(\n    self,\n    schema: Optional[Union[Path, str, JsonDict]] = None,\n    schema_version: Optional[str] = None,\n    schema_dir: Optional[Path] = None,\n):\n    \"\"\"\n    Initializes the Validator.\n\n    Args:\n        schema: Path to the schema file, the schema dictionary, or None\n                to use version-based schema loading. External file $refs will be\n                automatically inlined during initialization.\n        schema_version: Specific schema version to use (e.g., \"0.1.0\").\n                       If None, will auto-detect from data during validation.\n        schema_dir: Custom schema directory path. If None, uses default.\n    \"\"\"\n    self._registry = SchemaVersionRegistry(schema_dir)\n    self._schema_version = schema_version\n\n    if schema is not None:\n        # Use provided schema (legacy behavior)\n        schema_path = self._get_schema_path(schema)\n        initial_schema = self._load_schema_dict(schema, schema_path)\n        base_resolve_path = (\n            schema_path.parent if schema_path else DEFAULT_SCHEMA_PATH.parent\n        )\n    elif schema_version is not None:\n        # Use specific version from registry\n        if not self._registry.is_version_supported(schema_version):\n            raise ValueError(\n                f\"Schema version '{schema_version}' is not supported. \"\n                f\"Available versions: {self._registry.get_supported_versions()}\"\n            )\n        schema_path = self._registry.get_schema_path(schema_version)\n        initial_schema = self._registry.load_schema(schema_version)\n        base_resolve_path = schema_path.parent\n    else:\n        # Use latest version as default\n        try:\n            latest_version = self._registry.get_latest_version()\n            self._schema_version = latest_version\n            schema_path = self._registry.get_schema_path(latest_version)\n            initial_schema = self._registry.load_schema(latest_version)\n            base_resolve_path = schema_path.parent\n        except Exception:\n            # Fallback to legacy schema path if registry fails\n            schema_path = DEFAULT_SCHEMA_PATH\n            initial_schema = self._load_schema_dict(None, schema_path)\n            base_resolve_path = schema_path.parent\n\n    # Inline external $refs\n    print(\"\\n--- Inlining schema refs within Validator ---\")\n    loaded_cache = {}\n    self._schema = inline_refs(initial_schema, base_resolve_path, loaded_cache)\n    print(\"--- Inlining complete --- \\n\")\n\n    # Initialize the validator with the resolved schema.\n    try:\n        # Check if schema is valid before creating validator\n        Draft202012Validator.check_schema(self._schema)\n        self._validator = Draft202012Validator(\n            schema=self._schema,\n        )\n    except jsonschema.SchemaError as e:\n        raise SignalJourneyValidationError(f\"Invalid schema provided: {e}\") from e\n</code></pre>"},{"location":"api/#signaljourney_validator.validator.Validator.validate","title":"<code>validate(data, raise_exceptions=True, bids_context=None, auto_detect_version=True)</code>","text":"<p>Validates the given data against the appropriate signalJourney schema.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Path, str, JsonDict]</code> <p>Path to the JSON file, the JSON string, a dictionary   representing the JSON data.</p> required <code>raise_exceptions</code> <code>bool</code> <p>If True (default), raises SignalJourneyValidationError               on the first failure. If False, returns a list of               all validation errors found.</p> <code>True</code> <code>bids_context</code> <code>Optional[Path]</code> <p>Optional Path to the BIDS dataset root directory.           If provided, enables BIDS context validation checks           (e.g., file existence relative to the root).</p> <code>None</code> <code>auto_detect_version</code> <code>bool</code> <p>If True (default), automatically detects the schema                 version from the data and uses the appropriate schema.                 If False, uses the validator's configured schema.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[ValidationErrorDetail]</code> <p>A list of ValidationErrorDetail objects if raise_exceptions is False</p> <code>List[ValidationErrorDetail]</code> <p>and validation fails. Returns an empty list if validation succeeds.</p> <p>Raises:</p> Type Description <code>SignalJourneyValidationError</code> <p>If validation fails and                       raise_exceptions is True.</p> <code>FileNotFoundError</code> <p>If data file/path does not exist.</p> <code>TypeError</code> <p>If data is not a Path, string, or dictionary.</p> <code>ValueError</code> <p>If detected schema version is not supported.</p> Source code in <code>src/signaljourney_validator/validator.py</code> <pre><code>def validate(\n    self,\n    data: Union[Path, str, JsonDict],\n    raise_exceptions: bool = True,\n    bids_context: Optional[Path] = None,\n    auto_detect_version: bool = True,\n) -&gt; List[ValidationErrorDetail]:\n    \"\"\"\n    Validates the given data against the appropriate signalJourney schema.\n\n    Args:\n        data: Path to the JSON file, the JSON string, a dictionary\n              representing the JSON data.\n        raise_exceptions: If True (default), raises SignalJourneyValidationError\n                          on the first failure. If False, returns a list of\n                          all validation errors found.\n        bids_context: Optional Path to the BIDS dataset root directory.\n                      If provided, enables BIDS context validation checks\n                      (e.g., file existence relative to the root).\n        auto_detect_version: If True (default), automatically detects the schema\n                            version from the data and uses the appropriate schema.\n                            If False, uses the validator's configured schema.\n\n    Returns:\n        A list of ValidationErrorDetail objects if raise_exceptions is False\n        and validation fails. Returns an empty list if validation succeeds.\n\n    Raises:\n        SignalJourneyValidationError: If validation fails and\n                                  raise_exceptions is True.\n        FileNotFoundError: If data file/path does not exist.\n        TypeError: If data is not a Path, string, or dictionary.\n        ValueError: If detected schema version is not supported.\n    \"\"\"\n    instance: JsonDict\n    file_path_context: Optional[Path] = None  # For BIDS checks\n\n    # --- Load Instance ---\n    if isinstance(data, (Path, str)):\n        file_path = Path(data)\n        file_path_context = file_path  # Store for BIDS checks\n        if not file_path.exists():\n            raise FileNotFoundError(f\"Data file not found: {file_path}\")\n        try:\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                instance = json.load(f)\n        except json.JSONDecodeError as e:\n            raise SignalJourneyValidationError(\n                f\"Error decoding data JSON from {file_path}: {e}\"\n            ) from e\n        except Exception as e:\n            raise SignalJourneyValidationError(\n                f\"Error loading data file from {file_path}: {e}\"\n            ) from e\n    elif isinstance(data, dict):\n        instance = data\n    else:\n        raise TypeError(\"Data must be a Path, string, or dictionary.\")\n\n    # --- Version Detection and Validator Selection ---\n    validator_to_use = self._validator\n    detected_version = None\n\n    if auto_detect_version:\n        try:\n            detected_version = self._registry.detect_schema_version(instance)\n            if detected_version:\n                if not self._registry.is_version_supported(detected_version):\n                    error_msg = (\n                        f\"Detected schema version '{detected_version}' is not \"\n                        f\"supported. Available versions: \"\n                        f\"{self._registry.get_supported_versions()}\"\n                    )\n                    if raise_exceptions:\n                        raise SignalJourneyValidationError(error_msg)\n                    else:\n                        # Return a validation error\n                        return [\n                            ValidationErrorDetail(\n                                message=error_msg,\n                                path=[\"schema_version\"],\n                                schema_path=[],\n                                validator=\"version_support\",\n                                validator_value=self._registry.get_supported_versions(),\n                                instance_value=detected_version,\n                            )\n                        ]\n\n                # Use version-specific validator if different from current\n                if detected_version != self._schema_version:\n                    validator_to_use = self._create_validator_for_version(\n                        detected_version\n                    )\n\n            elif self._schema_version is None:\n                # No version detected and no default configured\n                error_msg = (\n                    \"No schema_version field found in data and no default version \"\n                    \"configured. Please specify a schema_version in your \"\n                    \"signalJourney file.\"\n                )\n                if raise_exceptions:\n                    raise SignalJourneyValidationError(error_msg)\n                else:\n                    return [\n                        ValidationErrorDetail(\n                            message=error_msg,\n                            path=[\"schema_version\"],\n                            schema_path=[],\n                            validator=\"required\",\n                            validator_value=True,\n                            instance_value=None,\n                        )\n                    ]\n\n        except (FileNotFoundError, IOError) as e:\n            # Re-raise file system errors\n            raise e\n        except Exception as e:\n            # Handle version detection errors\n            error_msg = f\"Error during schema version detection: {e}\"\n            if raise_exceptions:\n                raise SignalJourneyValidationError(error_msg) from e\n            else:\n                return [\n                    ValidationErrorDetail(\n                        message=error_msg,\n                        path=[\"schema_version\"],\n                        schema_path=[],\n                        validator=\"version_detection\",\n                        validator_value=None,\n                        instance_value=instance.get(\"schema_version\"),\n                    )\n                ]\n\n    schema_errors: List[ValidationErrorDetail] = []\n    bids_errors: List[ValidationErrorDetail] = []\n\n    # --- Schema Validation ---\n    # Use the selected validator's iter_errors method\n    try:\n        errors = sorted(\n            validator_to_use.iter_errors(instance), key=lambda e: e.path\n        )\n        if errors:\n            for error in errors:\n                # Convert jsonschema error to our custom format\n                detail = ValidationErrorDetail(\n                    message=error.message,\n                    path=list(error.path),\n                    schema_path=list(error.schema_path),\n                    validator=error.validator,\n                    validator_value=error.validator_value,\n                    instance_value=error.instance,\n                    # suggestion is added by ValidationErrorDetail constructor\n                )\n                schema_errors.append(detail)\n    except jsonschema.RefResolutionError as e:\n        # This shouldn't happen if schema is fully resolved, but handle defensively\n        print(f\"DEBUG: Unexpected RefResolutionError: {e}\")\n        failed_ref = getattr(e, \"ref\", \"[unknown ref]\")\n        raise SignalJourneyValidationError(\n            f\"Schema validation failed: Unexpectedly could not resolve \"\n            f\"reference '{failed_ref}'\"\n        ) from e\n    except jsonschema.SchemaError as e:\n        # Catch schema errors separately from resolution errors\n        raise SignalJourneyValidationError(f\"Invalid schema: {e}\") from e\n    except Exception as e:\n        # Capture the actual exception type for better debugging\n        print(\n            f\"DEBUG: Unexpected validation error type: \"\n            f\"{type(e).__name__}, Error: {e}\"\n        )\n        # Reraise or wrap depending on desired behavior\n        raise SignalJourneyValidationError(\n            f\"An unexpected error occurred during schema validation: {e}\"\n        ) from e\n\n    # --- BIDS Context Validation (Optional) ---\n    if bids_context:\n        bids_errors = self._validate_bids_context(\n            instance, file_path_context, bids_context\n        )\n\n    all_errors = schema_errors + bids_errors\n\n    # --- Result Handling ---\n    if all_errors:\n        if raise_exceptions:\n            raise SignalJourneyValidationError(\n                \"Validation failed.\", errors=all_errors\n            )\n        else:\n            return all_errors\n    else:\n        return []  # Success\n</code></pre>"},{"location":"api/#signaljourney_validator.validator.Validator.get_supported_versions","title":"<code>get_supported_versions()</code>","text":"<p>Get list of supported schema versions.</p> Source code in <code>src/signaljourney_validator/validator.py</code> <pre><code>def get_supported_versions(self) -&gt; List[str]:\n    \"\"\"Get list of supported schema versions.\"\"\"\n    return self._registry.get_supported_versions()\n</code></pre>"},{"location":"api/#signaljourney_validator.validator.Validator.get_current_version","title":"<code>get_current_version()</code>","text":"<p>Get the currently configured schema version.</p> Source code in <code>src/signaljourney_validator/validator.py</code> <pre><code>def get_current_version(self) -&gt; Optional[str]:\n    \"\"\"Get the currently configured schema version.\"\"\"\n    return self._schema_version\n</code></pre>"},{"location":"api/#signaljourney_validator.validator.Validator.get_latest_version","title":"<code>get_latest_version()</code>","text":"<p>Get the latest available schema version.</p> Source code in <code>src/signaljourney_validator/validator.py</code> <pre><code>def get_latest_version(self) -&gt; Optional[str]:\n    \"\"\"Get the latest available schema version.\"\"\"\n    return self._registry.get_latest_version()\n</code></pre>"},{"location":"api/#signaljourney_validator.validator.inline_refs","title":"<code>inline_refs(schema, base_path, loaded_schemas_cache)</code>","text":"<p>Recursively replace $ref keys with the content of the referenced file. Uses a cache (loaded_schemas_cache) to avoid infinite loops with circular refs and redundant file loading. Cache keys should be absolute POSIX paths of the schema files.</p> Source code in <code>src/signaljourney_validator/validator.py</code> <pre><code>def inline_refs(\n    schema: Union[Dict, list], base_path: Path, loaded_schemas_cache: Dict[str, Dict]\n):\n    \"\"\"Recursively replace $ref keys with the content of the referenced file.\n    Uses a cache (loaded_schemas_cache) to avoid infinite loops with circular refs\n    and redundant file loading.\n    Cache keys should be absolute POSIX paths of the schema files.\n    \"\"\"\n    if isinstance(schema, dict):\n        if (\n            \"$ref\" in schema\n            and isinstance(schema[\"$ref\"], str)\n            and not schema[\"$ref\"].startswith(\"#\")\n        ):\n            ref_path_str = schema[\"$ref\"]\n            # Resolve relative ref path against the current base path\n            ref_path = (base_path / ref_path_str).resolve()\n\n            # Cache key based on resolved absolute path\n            cache_key = ref_path.as_posix()\n\n            # Check cache first\n            if cache_key in loaded_schemas_cache:\n                # Return a copy to prevent modification issues during recursion\n                return loaded_schemas_cache[cache_key].copy()\n\n            # If not cached, load the file\n            if ref_path.exists() and ref_path.is_file():\n                try:\n                    # print(\n                    #     f\"[Inline] Loading $ref: {ref_path_str} \"\n                    #     f\"(from {base_path}) -&gt; {ref_path}\"\n                    # )\n                    with open(ref_path, \"r\", encoding=\"utf-8\") as f:\n                        ref_content = json.load(f)\n                    # Store in cache BEFORE recursion to handle circular refs\n                    loaded_schemas_cache[cache_key] = ref_content\n                    # Recursively resolve refs *within* the loaded content\n                    # Use the directory of the *referenced* file as the new base path\n                    resolved_content = inline_refs(\n                        ref_content, ref_path.parent, loaded_schemas_cache\n                    )\n                    # Update cache with the fully resolved content\n                    loaded_schemas_cache[cache_key] = resolved_content\n                    # Return a copy of the resolved content\n                    return resolved_content.copy()\n                except Exception as e:\n                    print(\n                        f\"Warning: Failed to load or parse $ref: {ref_path_str} \"\n                        f\"from {ref_path}. Error: {e}\"\n                    )\n                    return schema  # Keep original $ref on error\n            else:\n                print(\n                    f\"Warning: $ref path does not exist or is not a file: \"\n                    f\"{ref_path_str} -&gt; {ref_path}\"\n                )\n                return schema  # Keep original $ref if file not found\n        else:\n            # Recursively process other keys in the dictionary\n            new_schema = {}\n            for key, value in schema.items():\n                new_schema[key] = inline_refs(value, base_path, loaded_schemas_cache)\n            return new_schema\n    elif isinstance(schema, list):\n        # Recursively process items in the list\n        return [inline_refs(item, base_path, loaded_schemas_cache) for item in schema]\n    else:\n        # Return non-dict/list items as is\n        return schema\n</code></pre>"},{"location":"api/#cli-module","title":"CLI Module","text":""},{"location":"api/#signaljourney_validator.cli","title":"<code>signaljourney_validator.cli</code>","text":""},{"location":"api/#signaljourney_validator.cli.cli","title":"<code>cli()</code>","text":"<p>Signal Journey Validator CLI.</p> <p>Provides tools to validate signalJourney JSON files against the official specification schema. Supports validating single files or entire directories.</p> Source code in <code>src/signaljourney_validator/cli.py</code> <pre><code>@click.group(context_settings=dict(help_option_names=[\"-h\", \"--help\"]))\n@click.version_option(\n    package_name=\"signaljourney-validator\", prog_name=\"signaljourney-validate\"\n)\ndef cli():\n    \"\"\"\n    Signal Journey Validator CLI.\n\n    Provides tools to validate signalJourney JSON files against the official\n    specification schema.\n    Supports validating single files or entire directories.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#signaljourney_validator.cli.validate","title":"<code>validate(path, schema, schema_version, recursive, output_format, verbose, bids, bids_root)</code>","text":"<p>Validate one or more signalJourney JSON files.</p> <p>Checks conformance against the official signalJourney schema (or a custom schema if provided via --schema). Supports version-based validation using --schema-version.</p> <p>Examples:</p> <p>Validate a single file (auto-detects schema version):</p> <pre><code>signaljourney-validate path/to/sub-01_task-rest_signalJourney.json\n</code></pre> <p>Validate with specific schema version:</p> <pre><code>signaljourney-validate --schema-version 0.1.0 path/to/file.json\n</code></pre> <p>Validate all files in a directory (non-recursively):</p> <pre><code>signaljourney-validate path/to/derivatives/pipelineX/\n</code></pre> <p>Validate all files recursively, outputting JSON:</p> <pre><code>signaljourney-validate -r -o json path/to/bids_dataset/\n</code></pre> <p>Validate with BIDS context checks:</p> <pre><code>signaljourney-validate --bids --bids-root path/to/bids_dataset \\\n    path/to/bids_dataset/derivatives/...\n</code></pre> Source code in <code>src/signaljourney_validator/cli.py</code> <pre><code>@cli.command()\n@click.argument(\n    \"path\",\n    type=click.Path(exists=True, readable=True, resolve_path=True, path_type=Path),\n)\n@click.option(\n    \"--schema\",\n    \"-s\",\n    type=click.Path(\n        exists=True, dir_okay=False, readable=True, resolve_path=True, path_type=Path\n    ),\n    help=\"Path to a custom JSON schema file to validate against.\",\n)\n@click.option(\n    \"--schema-version\",\n    type=str,\n    help=\"Specific schema version to use (e.g., '0.1.0'). If not specified, \"\n    \"auto-detects from each file's schema_version field.\",\n)\n@click.option(\n    \"--recursive\",\n    \"-r\",\n    is_flag=True,\n    default=False,\n    help=\"Recursively search for *_signalJourney.json files in subdirectories.\",\n)\n@click.option(\n    \"--output-format\",\n    \"-o\",\n    type=click.Choice([\"text\", \"json\"], case_sensitive=False),\n    default=\"text\",\n    help='Output format: \"text\" (human-readable, default) or \"json\" '\n    \"(machine-readable).\",\n)\n@click.option(\n    \"--verbose\",\n    \"-v\",\n    is_flag=True,\n    default=False,\n    help='Enable verbose output for the \"text\" format (shows more error details).',\n)\n@click.option(\n    \"--bids\",\n    is_flag=True,\n    default=False,\n    help=\"Enable BIDS context validation checks (experimental).\",\n)\n@click.option(\n    \"--bids-root\",\n    type=click.Path(exists=True, file_okay=False, path_type=Path, resolve_path=True),\n    help=\"Path to the BIDS dataset root directory (required if --bids is used).\",\n)\ndef validate(\n    path: Path,\n    schema: Path,\n    schema_version: str,\n    recursive: bool,\n    output_format: str,\n    verbose: bool,\n    bids: bool,\n    bids_root: Path,\n):\n    \"\"\"\n    Validate one or more signalJourney JSON files.\n\n    Checks conformance against the official signalJourney schema (or a custom schema\n    if provided via --schema). Supports version-based validation using --schema-version.\n\n    Examples:\n\n    Validate a single file (auto-detects schema version):\n\n        signaljourney-validate path/to/sub-01_task-rest_signalJourney.json\n\n    Validate with specific schema version:\n\n        signaljourney-validate --schema-version 0.1.0 path/to/file.json\n\n    Validate all files in a directory (non-recursively):\n\n        signaljourney-validate path/to/derivatives/pipelineX/\n\n    Validate all files recursively, outputting JSON:\n\n        signaljourney-validate -r -o json path/to/bids_dataset/\n\n    Validate with BIDS context checks:\n\n        signaljourney-validate --bids --bids-root path/to/bids_dataset \\\\\n            path/to/bids_dataset/derivatives/...\n    \"\"\"\n    if bids and not bids_root:\n        click.echo(\n            \"Error: --bids-root is required when using the --bids flag.\", err=True\n        )\n        sys.exit(1)\n\n    if schema and schema_version:\n        click.echo(\n            \"Error: Cannot specify both --schema and --schema-version. \"\n            \"Use --schema for custom schema files or --schema-version for \"\n            \"version-based validation.\",\n            err=True,\n        )\n        sys.exit(1)\n\n    files_to_validate: List[Path] = []\n    if path.is_file():\n        # If a single file is provided, attempt to validate it if it's JSON,\n        # regardless of the _signalJourney suffix.\n        # Keep the suffix check for directory scanning.\n        if path.name.lower().endswith(\".json\"):\n            files_to_validate.append(path)\n        elif output_format == \"text\":  # Only print skip message for non-JSON files\n            click.echo(f\"Skipping non-JSON file: {path}\", err=True)\n    elif path.is_dir():\n        if output_format == \"text\":\n            scan_mode = \" recursively\" if recursive else \"\"\n            bids_mode = \" (BIDS mode)\" if bids else \"\"\n            click.echo(f\"Scanning directory: {path}{scan_mode}{bids_mode}\")\n        if recursive:\n            # Keep the suffix check for recursive scanning\n            for root, _, filenames in os.walk(path):\n                for filename in filenames:\n                    if filename.endswith(\"_signalJourney.json\"):\n                        files_to_validate.append(Path(root) / filename)\n        else:\n            # For non-recursive directory scan, find any .json file\n            for item in path.iterdir():\n                if item.is_file() and item.name.lower().endswith(\".json\"):\n                    files_to_validate.append(item)\n    else:\n        # This error should occur regardless of format\n        click.echo(\n            f\"Error: Input path is neither a file nor a directory: {path}\", err=True\n        )\n        sys.exit(1)\n\n    # Prepare results structure\n    results: Dict[str, Any] = {\"overall_success\": True, \"files\": []}\n    validator_instance: Optional[Validator] = None\n\n    if not files_to_validate:\n        if output_format == \"text\" and path.is_dir():\n            # Only print if input was a directory and no files were found.\n            scan_mode = \" recursively\" if recursive else \"\"\n            click.echo(\n                f\"No *_signalJourney.json files found to validate in {path}{scan_mode}\",\n                err=True,\n            )\n        elif output_format == \"json\":\n            # Output valid JSON even if no files processed\n            print(json.dumps({\"files\": [], \"overall_success\": True}, indent=2))\n        # Exit code 0 if input dir was empty, 1 otherwise (e.g., non-JSON file input)\n        sys.exit(0 if path.is_dir() else 1)\n\n    # --- Schema Loading and Resolver Setup (Load ONCE) ---\n    # try:\n    #     schema_to_use = schema if schema else DEFAULT_SCHEMA_PATH\n    #     if not schema_to_use.exists():\n    #         raise FileNotFoundError(f\"Schema file not found: {schema_to_use}\")\n    #     with open(schema_to_use, \"r\", encoding=\"utf-8\") as f:\n    #         main_schema_dict = json.load(f)\n    #\n    #     # Setup resolver similar to conftest.py - REMOVED\n    #     ...\n    #\n    #     resolver = jsonschema.RefResolver(...)\n    # except Exception as e:\n    #     click.echo(f\"Error loading schema or building resolver: {e}\", err=True)\n    #     sys.exit(1)\n    # --- End Schema Loading ---\n\n    overall_success = True\n    for filepath in files_to_validate:\n        file_result: Dict[str, Any] = {\n            \"filepath\": str(filepath),\n            \"status\": \"unknown\",\n            \"errors\": [],\n        }\n        if output_format == \"text\":\n            click.echo(f\"Validating: {filepath} ... \", nl=False)\n\n        try:\n            if validator_instance is None:\n                # Create validator ONCE using the schema path (or None for default)\n                # Validator internal __init__ now handles registry setup.\n                try:\n                    # Create validator with schema version support\n                    validator_instance = Validator(\n                        schema=schema, schema_version=schema_version\n                    )\n                except Exception as e:\n                    # Handle potential errors during Validator initialization\n                    # (e.g., schema loading)\n                    click.echo(f\"CRITICAL ERROR initializing validator: {e}\", err=True)\n                    # For JSON output, log the critical error at file level\n                    if output_format == \"json\":\n                        file_result[\"status\"] = \"critical_error\"\n                        file_result[\"errors\"] = [\n                            {\"message\": f\"Validator init failed: {e}\"}\n                        ]\n                        results[\"files\"].append(file_result)\n                        results[\"overall_success\"] = False\n                        overall_success = False  # Ensure overall failure\n                    # Exit or continue? Maybe continue to report errors for other files?\n                    # For now, let's make it a fatal error for the specific file.\n                    if output_format == \"text\":\n                        click.echo(\" CRITICAL ERROR\")\n                        click.echo(f\"  - Initialization Failed: {e}\")\n                    # Skip to the next file if validator init fails\n                    continue\n\n            # Pass bids_root to validator if bids flag is set\n            current_bids_context = bids_root if bids else None\n            validation_errors = validator_instance.validate(\n                filepath, raise_exceptions=False, bids_context=current_bids_context\n            )\n\n            if validation_errors:\n                overall_success = False\n                file_result[\"status\"] = \"failed\"\n                if output_format == \"text\":\n                    click.secho(\"FAILED\", fg=\"red\")\n                for error in validation_errors:\n                    # Store structured error\n                    error_dict = {\n                        \"message\": error.message,\n                        \"path\": list(error.path) if error.path else [],\n                        \"schema_path\": list(error.schema_path)\n                        if error.schema_path\n                        else [],\n                        \"validator\": error.validator,\n                        \"validator_value\": repr(error.validator_value),  # Use repr\n                        \"instance_value\": repr(error.instance_value),  # Use repr\n                        \"suggestion\": error.suggestion,\n                    }\n                    file_result[\"errors\"].append(error_dict)\n\n                    # Print detailed error in text mode\n                    if output_format == \"text\":\n                        error_path_list = list(error.path) if error.path else []\n                        error_path_str = (\n                            \"/\".join(map(str, error_path_list))\n                            if error_path_list\n                            else \"root\"\n                        )\n                        error_msg = f\"  - Error at '{error_path_str}': {error.message}\"\n                        if verbose:\n                            # Add more details in verbose mode\n                            error_msg += f\" (validator: '{error.validator}')\"\n                            # Add more details if needed\n                        if error.suggestion:\n                            error_msg += f\" -- Suggestion: {error.suggestion}\"\n                        click.echo(error_msg)\n            else:\n                file_result[\"status\"] = \"passed\"\n                if output_format == \"text\":\n                    click.secho(\"PASSED\", fg=\"green\")\n\n        except SignalJourneyValidationError as e:\n            overall_success = False\n            file_result[\"status\"] = \"error\"\n            file_result[\"error_message\"] = str(e)\n            if output_format == \"text\":\n                click.secho(\"ERROR\", fg=\"yellow\")\n                click.echo(f\"  - Validation Error: {e}\", err=True)\n            # Include details from SignalJourneyValidationError if available\n            detailed_errors = []\n            if (\n                isinstance(e, SignalJourneyValidationError)\n                and hasattr(e, \"errors\")\n                and e.errors\n            ):\n                detailed_errors = [\n                    {\n                        \"message\": detail.get(\"message\", \"N/A\"),\n                        \"path\": detail.get(\"path\", []),\n                        # Add other relevant fields from ValidationErrorDetail if needed\n                    }\n                    for detail in e.errors\n                ]\n                file_result[\"errors\"] = detailed_errors  # Overwrite with details\n                if output_format == \"text\":\n                    click.echo(\"    Detailed Errors:\", err=True)\n                    for detail in detailed_errors:\n                        path_str = detail.get(\"path\", \"N/A\")\n                        msg_str = detail.get(\"message\", \"N/A\")\n                        click.echo(f\"    - Path: {path_str}, Msg: {msg_str}\", err=True)\n\n        except Exception as e:\n            overall_success = False\n            file_result[\"status\"] = \"error\"\n            file_result[\"error_message\"] = f\"An unexpected error occurred: {e}\"\n            if output_format == \"text\":\n                click.secho(\"CRITICAL ERROR\", fg=\"red\", bold=True)\n                click.echo(f\"  - Unexpected Error: {e}\", err=True)\n\n        results[\"files\"].append(file_result)\n\n    results[\"overall_success\"] = overall_success\n\n    if output_format == \"json\":\n        print(json.dumps(results, indent=2))\n    elif output_format == \"text\" and verbose:\n        # Add a summary line in verbose text mode\n        status_msg = (\n            click.style(\"PASSED\", fg=\"green\")\n            if overall_success\n            else click.style(\"FAILED\", fg=\"red\")\n        )\n        click.echo(f\"\\nOverall validation result: {status_msg}\")\n\n    # Exit with appropriate code\n    sys.exit(0 if overall_success else 1)\n</code></pre>"},{"location":"api/#errors-module","title":"Errors Module","text":""},{"location":"api/#signaljourney_validator.errors","title":"<code>signaljourney_validator.errors</code>","text":""},{"location":"api/#signaljourney_validator.errors.ValidationErrorDetail","title":"<code>ValidationErrorDetail</code>  <code>dataclass</code>","text":"<p>Represents a detailed validation error.</p> Source code in <code>src/signaljourney_validator/errors.py</code> <pre><code>@dataclass\nclass ValidationErrorDetail:\n    \"\"\"Represents a detailed validation error.\"\"\"\n\n    message: str\n    path: Sequence[JsonPath] = field(default_factory=list)\n    schema_path: Sequence[JsonPath] = field(default_factory=list)\n    validator: str = \"\"\n    validator_value: Any = None\n    instance_value: Any = None\n    # For nested errors\n    context: List[\"ValidationErrorDetail\"] = field(default_factory=list)\n    suggestion: Optional[str] = None  # Added field for suggestions\n\n    def __post_init__(self):\n        \"\"\"Generate suggestions based on error type after initialization.\"\"\"\n        self._generate_suggestion()\n\n    def __str__(self) -&gt; str:\n        path_str = \"/\".join(map(str, self.path)) if self.path else \"root\"\n        msg = f\"Error at '{path_str}': {self.message}\"\n        if self.suggestion:\n            msg += f\" -- Suggestion: {self.suggestion}\"\n        return msg\n\n    def _generate_suggestion(self):\n        \"\"\"Internal method to populate the suggestion field based on validator type.\"\"\"\n        if self.validator == \"required\":\n            # 'validator_value' usually holds the list of required properties\n            missing_props = self.validator_value\n            if isinstance(missing_props, list):\n                props_str = \"', '\".join(missing_props)\n                self.suggestion = (\n                    f\"Ensure required property or properties ('{props_str}') \"\n                    f\"are present.\"\n                )\n            else:\n                self.suggestion = (\n                    \"Ensure required property is present (check schema for details).\"\n                )\n\n        elif self.validator == \"type\":\n            expected_types = self.validator_value\n            actual_type = type(self.instance_value).__name__\n            if isinstance(expected_types, list):\n                types_str = \"', '\".join(expected_types)\n                self.suggestion = (\n                    f\"Change value type from '{actual_type}' to one of: '{types_str}'.\"\n                )\n            elif isinstance(expected_types, str):\n                self.suggestion = (\n                    f\"Change value type from '{actual_type}' to '{expected_types}'.\"\n                )\n            else:\n                self.suggestion = (\n                    f\"Check schema for expected type(s) instead of '{actual_type}'.\"\n                )\n\n        elif self.validator == \"pattern\":\n            pattern = self.validator_value\n            self.suggestion = (\n                f\"Ensure value matches the required regex pattern: '{pattern}'.\"\n            )\n\n        elif self.validator == \"enum\":\n            allowed_values = self.validator_value\n            if isinstance(allowed_values, list):\n                suggestion_text = (\n                    f\"Value must be one of: {', '.join(map(repr, allowed_values))}.\"\n                )\n                # Optional: Add fuzzy matching\n                if (\n                    HAS_FUZZY\n                    and isinstance(self.instance_value, str)\n                    and self.instance_value\n                ):\n                    try:\n                        # Filter for string choices\n                        string_allowed_values = [\n                            str(v) for v in allowed_values if isinstance(v, str)\n                        ]\n                        if string_allowed_values:\n                            best_match, score = fuzzy_process.extractOne(\n                                self.instance_value, string_allowed_values\n                            )\n                            if score &gt; 80:  # Threshold\n                                suggestion_text += f\" Did you mean '{best_match}'?\"\n                    except Exception:\n                        pass  # Ignore fuzzy matching errors\n                self.suggestion = suggestion_text\n            else:\n                self.suggestion = (\n                    \"Ensure value is one of the allowed options (check schema).\"\n                )\n\n        # Add suggestions for length/item constraints\n        elif self.validator == \"minLength\":\n            min_len = self.validator_value\n            actual_len = (\n                len(self.instance_value)\n                if isinstance(self.instance_value, (str, list))\n                else \"N/A\"\n            )\n            self.suggestion = (\n                f\"Ensure value has at least {min_len} characters/items \"\n                f\"(currently {actual_len}).\"\n            )\n\n        elif self.validator == \"maxLength\":\n            max_len = self.validator_value\n            actual_len = (\n                len(self.instance_value)\n                if isinstance(self.instance_value, (str, list))\n                else \"N/A\"\n            )\n            self.suggestion = (\n                f\"Ensure value has at most {max_len} characters/items \"\n                f\"(currently {actual_len}).\"\n            )\n\n        elif self.validator == \"minItems\":\n            min_num = self.validator_value\n            actual_num = (\n                len(self.instance_value)\n                if isinstance(self.instance_value, list)\n                else \"N/A\"\n            )\n            self.suggestion = (\n                f\"Ensure array has at least {min_num} items (currently {actual_num}).\"\n            )\n\n        elif self.validator == \"maxItems\":\n            max_num = self.validator_value\n            actual_num = (\n                len(self.instance_value)\n                if isinstance(self.instance_value, list)\n                else \"N/A\"\n            )\n            self.suggestion = (\n                f\"Ensure array has at most {max_num} items (currently {actual_num}).\"\n            )\n\n        elif self.validator == \"minimum\":\n            min_val = self.validator_value\n            self.suggestion = f\"Ensure value is at least {min_val}.\"\n\n        elif self.validator == \"maximum\":\n            max_val = self.validator_value\n            self.suggestion = f\"Ensure value is at most {max_val}.\"\n\n        elif self.validator == \"exclusiveMinimum\":\n            ex_min_val = self.validator_value\n            self.suggestion = f\"Ensure value is strictly greater than {ex_min_val}.\"\n\n        elif self.validator == \"exclusiveMaximum\":\n            ex_max_val = self.validator_value\n            self.suggestion = f\"Ensure value is strictly less than {ex_max_val}.\"\n</code></pre>"},{"location":"api/#signaljourney_validator.errors.ValidationErrorDetail.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Generate suggestions based on error type after initialization.</p> Source code in <code>src/signaljourney_validator/errors.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Generate suggestions based on error type after initialization.\"\"\"\n    self._generate_suggestion()\n</code></pre>"},{"location":"api/#signaljourney_validator.errors.SignalJourneyValidationError","title":"<code>SignalJourneyValidationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for validation errors.</p> Source code in <code>src/signaljourney_validator/errors.py</code> <pre><code>class SignalJourneyValidationError(Exception):\n    \"\"\"Custom exception for validation errors.\"\"\"\n\n    def __init__(\n        self, message: str, errors: Optional[List[ValidationErrorDetail]] = None\n    ):\n        super().__init__(message)\n        self.errors = errors or []\n</code></pre>"},{"location":"examples/","title":"signalJourney Examples","text":"<p>Welcome to the signalJourney examples! These examples demonstrate how to document signal processing pipelines using the signalJourney specification. Each example includes a detailed breakdown of the JSON structure and showcases different features of the format.</p>"},{"location":"examples/#example-categories","title":"Example Categories","text":""},{"location":"examples/#schema-examples","title":"\ud83d\udcda Schema Examples","text":"<p>These examples are found in the <code>schema/examples/</code> directory and demonstrate core signalJourney features with implementations in both MNE-Python and EEGLAB:</p> Pipeline Type MNE-Python Implementation EEGLAB Implementation Key Features Basic Preprocessing MNE-Python EEGLAB File I/O, step dependencies, quality metrics ICA Decomposition MNE-Python EEGLAB Multi-output steps, artifact removal, variables Time-Frequency Analysis MNE-Python EEGLAB Wavelet decomposition, baseline correction Source Localization MNE-Python EEGLAB Forward modeling, inverse solutions Connectivity Analysis MNE-Python EEGLAB Cross-channel analysis, network metrics"},{"location":"examples/#real-world-examples","title":"\ud83d\udd2c Real-World Examples","text":"<p>Production pipelines demonstrating advanced signalJourney features:</p> Pipeline Software Description Key Features NEMAR Pipeline EEGLAB/MATLAB Complete EEG preprocessing for OpenNeuro datasets 12-step workflow, inline data, extension schemas"},{"location":"examples/#implementation-comparison","title":"Implementation Comparison","text":""},{"location":"examples/#software-specific-features","title":"Software-Specific Features","text":"<p>MNE-Python Examples: - Object-oriented API (<code>epochs.filter()</code>, <code>raw.set_eeg_reference()</code>) - HDF5 and FIF file formats - Advanced source modeling (BEM, forward solutions) - Integration with scipy/numpy ecosystem</p> <p>EEGLAB Examples: - Pop-up function interface (<code>pop_loadset</code>, <code>pop_eegfiltnew</code>) - .set/.fdt file formats - ICLabel automated component classification - MATLAB ecosystem integration</p>"},{"location":"examples/#learning-path-recommendations","title":"Learning Path Recommendations","text":""},{"location":"examples/#for-beginners","title":"\ud83c\udfaf For Beginners","text":"<ol> <li>Start with Basic Preprocessing (MNE-Python or EEGLAB)</li> <li>Understand signalJourney structure and common patterns</li> <li>Learn about quality metrics and parameter documentation</li> </ol>"},{"location":"examples/#for-intermediate-users","title":"\ud83d\ude80 For Intermediate Users","text":"<ol> <li>Explore ICA Decomposition examples for multi-output workflows</li> <li>Study Time-Frequency Analysis for advanced parameter handling</li> <li>Examine Real-World Examples for production pipeline patterns</li> </ol>"},{"location":"examples/#for-advanced-users","title":"\ud83d\udd2c For Advanced Users","text":"<ol> <li>Analyze Source Localization for complex dependency chains</li> <li>Study Connectivity Analysis for cross-channel processing</li> <li>Use NEMAR Pipeline as template for your own workflows</li> </ol>"},{"location":"examples/#key-signaljourney-features-demonstrated","title":"Key signalJourney Features Demonstrated","text":"Feature Example Categories Description Step Dependencies All examples <code>dependsOn</code> field linking processing steps Quality Metrics All examples Step-level and summary quality assessments Multi-Output Steps ICA, Time-Frequency Steps producing files, variables, and inline data Pipeline Provenance All examples Links between related processing pipelines Extension Schemas NEMAR Pipeline Domain-specific metadata and parameters Parameter Documentation All examples Complete parameter sets for reproducibility Software Integration All examples Function calls, versions, and dependencies"},{"location":"examples/#interactive-features","title":"Interactive Features","text":"<p>Each example page includes: - \ud83d\udcca Mermaid flowcharts showing pipeline structure - \ud83d\udcbb JSON code examples with syntax highlighting - \ud83d\udccb Parameter explanations for each processing step - \ud83d\udd0d Quality control integration examples - \ud83d\udd17 Cross-references to related examples and schemas</p>"},{"location":"examples/#mermaid-diagram-legend","title":"Mermaid Diagram Legend","text":"<p>The flowcharts use a consistent color scheme to distinguish different types of elements:</p> <pre><code>flowchart TD\n    A[Processing StepFunction call] --&gt; B[Another StepFunction call]\n\n    %% Input and output files  \n    C[\"\ud83d\udcc1 input_file.fifInput data\"] --&gt; A\n    B --&gt; D[\"\ud83d\udcbe output_file.fifProcessed data\"]\n\n    %% Inline data (small parameters/values saved in JSON)\n    A --&gt; E[\"\ud83d\udcca Inline DataSmall parameters/values\"]\n\n    %% Quality metrics\n    B --&gt; F[\"\ud83d\udcc8 Quality MetricsProcessing statistics\"]\n\n    %% Styling\n    classDef processStep fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef inputFile fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    classDef outputFile fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px\n    classDef inlineData fill:#f3e5f5,stroke:#4a148c,stroke-width:2px\n    classDef qualityMetric fill:#f9f9f9,stroke:#666,stroke-width:1px\n\n    class A,B processStep\n    class C inputFile\n    class D outputFile\n    class E inlineData\n    class F qualityMetric\n</code></pre> <p>Legend: - \ud83d\udd35 Light Blue: Processing steps (function calls) - \ud83d\udfe0 Orange: Input files  - \ud83d\udfe2 Green: Output files (saved results) - \ud83d\udfe3 Purple: Inline data (small parameters/values stored in JSON) - \u26aa Gray: Quality metrics (processing statistics)</p>"},{"location":"examples/#getting-started","title":"Getting Started","text":"<ol> <li>Choose your software: Pick MNE-Python or EEGLAB based on your workflow</li> <li>Start simple: Begin with Basic Preprocessing to understand the format</li> <li>Build complexity: Progress through examples as your needs grow</li> <li>Adapt patterns: Use examples as templates for your own pipelines</li> </ol> <p>Each example is designed to be both educational and practical, providing patterns you can adapt for documenting your own signal processing workflows.</p>"},{"location":"examples/#json-schema-reference","title":"JSON Schema Reference","text":"<p>All examples validate against the signalJourney JSON schema. For detailed field specifications, see: - Schema Overview - Field Definitions - Validation Guide</p>"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>Have a pipeline you'd like to document as an example? See our contributing guide for how to add new examples to the collection.</p>"},{"location":"examples/basic_preprocessing_eeglab/","title":"Example: Basic EEG Preprocessing Pipeline (EEGLAB)","text":"<p>This page explains the <code>basic_preprocessing_pipeline_eeglab.signalJourney.json</code> example file, which documents a standard EEG preprocessing workflow using EEGLAB.</p>"},{"location":"examples/basic_preprocessing_eeglab/#pipeline-overview","title":"Pipeline Overview","text":"<p>The EEGLAB basic preprocessing pipeline demonstrates fundamental EEG preprocessing steps using EEGLAB functions:</p> <ul> <li>Load raw data from EEGLAB .set format</li> <li>Apply high-pass filtering (1 Hz) using <code>pop_eegfiltnew</code></li> <li>Apply low-pass filtering (40 Hz) using <code>pop_eegfiltnew</code></li> <li>Apply notch filtering (60 Hz) using <code>pop_eegfiltnew</code></li> <li>Set average reference using <code>pop_reref</code></li> <li>Interpolate bad channels using <code>pop_interp</code></li> </ul>"},{"location":"examples/basic_preprocessing_eeglab/#pipeline-flowchart","title":"Pipeline Flowchart","text":"<pre><code>flowchart TD\n    A[Load Raw Datapop_loadset] --&gt; B[High-pass Filterpop_eegfiltnew1 Hz]\n    B --&gt; C[Low-pass Filterpop_eegfiltnew40 Hz]\n    C --&gt; D[Notch Filterpop_eegfiltnew58-62 Hz]\n    D --&gt; E[Average Referencepop_reref]\n    E --&gt; F[Interpolate Bad Channelspop_interp]\n\n    %% Input file\n    G[\"\ud83d\udcc1 sub-01_task-rest_raw.setRaw EEGLAB dataset\"] --&gt; A\n\n    %% Final output\n    F --&gt; H[\"\ud83d\udcbe sub-01_task-rest_desc-preproc_eeg.setPreprocessed dataset\"]\n\n    %% Quality metrics\n    E --&gt; Q1[\"\ud83d\udcc8 Reference type: averageChannels: 62\"]\n    F --&gt; Q2[\"\ud83d\udcc8 Interpolated: [53, 21]Method: spherical\"]\n\n    %% Styling\n    classDef processStep fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef inputFile fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    classDef outputFile fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px\n    classDef qualityMetric fill:#f9f9f9,stroke:#666,stroke-width:1px\n\n    class A,B,C,D,E,F processStep\n    class G inputFile\n    class H outputFile\n    class Q1,Q2 qualityMetric\n</code></pre>"},{"location":"examples/basic_preprocessing_eeglab/#key-eeglab-features-demonstrated","title":"Key EEGLAB Features Demonstrated","text":""},{"location":"examples/basic_preprocessing_eeglab/#eeglab-function-calls","title":"EEGLAB Function Calls","text":"<ul> <li><code>pop_loadset</code>: Load EEGLAB dataset files (.set/.fdt)</li> <li><code>pop_eegfiltnew</code>: Modern FIR filtering with linear phase</li> <li><code>pop_reref</code>: Re-referencing to average or specific channels</li> <li><code>pop_interp</code>: Spherical spline interpolation for bad channels</li> <li><code>pop_saveset</code>: Save processed datasets</li> </ul>"},{"location":"examples/basic_preprocessing_eeglab/#eeglab-specific-parameters","title":"EEGLAB-Specific Parameters","text":"<ul> <li>Filter specifications: Uses EEGLAB's default FIR filter parameters</li> <li>Channel selection: EEGLAB channel indexing and selection</li> <li>Dataset structure: EEGLAB EEG structure format preservation</li> </ul>"},{"location":"examples/basic_preprocessing_eeglab/#example-json-structure","title":"Example JSON Structure","text":"<p>The signalJourney file documents each processing step with:</p> <pre><code>{\n  \"stepId\": \"2\",\n  \"name\": \"Apply High-pass Filter\",\n  \"description\": \"Apply a FIR high-pass filter at 1 Hz.\",\n  \"software\": {\n    \"name\": \"EEGLAB\",\n    \"version\": \"2023.1\",\n    \"functionCall\": \"pop_eegfiltnew(EEG, 'locutoff', 1, 'plotfreqz', 0)\"\n  },\n  \"parameters\": {\n    \"locutoff\": 1.0,\n    \"hicutoff\": null,\n    \"filtorder\": [],\n    \"revfilt\": 0,\n    \"usefft\": 1,\n    \"plotfreqz\": 0\n  }\n}\n</code></pre>"},{"location":"examples/basic_preprocessing_eeglab/#quality-control-integration","title":"Quality Control Integration","text":"<p>Each step includes quality metrics specific to EEGLAB processing: - Channel counts and indices - Filter specifications and verification - Reference channel information - Interpolation success metrics</p>"},{"location":"examples/basic_preprocessing_eeglab/#eeglab-vs-mne-python-comparison","title":"EEGLAB vs MNE-Python Comparison","text":"Aspect EEGLAB Version MNE-Python Version Data Format .set/.fdt files .fif files Filtering <code>pop_eegfiltnew</code> <code>filter</code>, <code>notch_filter</code> Referencing <code>pop_reref</code> <code>set_eeg_reference</code> Interpolation <code>pop_interp</code> <code>interpolate_bads</code> Function Style Pop-up GUI functions Object methods Filter Design Separate high/low pass Combined band-pass"},{"location":"examples/basic_preprocessing_eeglab/#eeglab-specific-workflow","title":"EEGLAB-Specific Workflow","text":""},{"location":"examples/basic_preprocessing_eeglab/#step-by-step-filtering-approach","title":"Step-by-Step Filtering Approach","text":"<p>Unlike MNE-Python's combined band-pass filter, EEGLAB demonstrates: 1. Separate high-pass filtering for drift removal 2. Separate low-pass filtering for noise reduction 3. Independent notch filtering for line noise</p>"},{"location":"examples/basic_preprocessing_eeglab/#eeg-structure-preservation","title":"EEG Structure Preservation","text":"<p>EEGLAB maintains the EEG structure throughout processing: - Channel information preserved across all steps - Event markers maintained during filtering - Dataset history automatically tracked</p>"},{"location":"examples/basic_preprocessing_eeglab/#usage-notes","title":"Usage Notes","text":"<p>This example demonstrates: - EEGLAB workflow patterns with pop_ functions - Parameter documentation for reproducible processing - File format handling for EEGLAB datasets - Quality metrics relevant to EEGLAB processing - Sequential filtering approach common in EEGLAB pipelines</p> <p>The pipeline is designed to be representative of standard EEGLAB preprocessing workflows while maintaining full parameter transparency for reproducibility. </p>"},{"location":"examples/basic_preprocessing_mne/","title":"Example: Basic EEG Preprocessing Pipeline (MNE-Python)","text":"<p>This page explains the <code>basic_preprocessing_pipeline_mne.signalJourney.json</code> example file, which documents a standard EEG preprocessing workflow using MNE-Python.</p>"},{"location":"examples/basic_preprocessing_mne/#pipeline-overview","title":"Pipeline Overview","text":"<p>The MNE-Python basic preprocessing pipeline demonstrates fundamental EEG preprocessing steps using MNE-Python functions:</p> <ul> <li>Load raw data from FIF format</li> <li>Apply band-pass filtering (1-40 Hz) using <code>raw.filter</code></li> <li>Apply notch filtering (60 Hz) using <code>raw.notch_filter</code></li> <li>Set average reference using <code>raw.set_eeg_reference</code></li> <li>Interpolate bad channels using <code>raw.interpolate_bads</code></li> </ul>"},{"location":"examples/basic_preprocessing_mne/#pipeline-flowchart","title":"Pipeline Flowchart","text":"<pre><code>flowchart TD\n    A[Load Raw Datamne.io.read_raw_fif] --&gt; B[Apply Band-pass Filterraw.filter1-40 Hz]\n    B --&gt; C[Apply Notch Filterraw.notch_filter60 Hz]\n    C --&gt; D[Set Average Referenceraw.set_eeg_reference]\n    D --&gt; E[Interpolate Bad Channelsraw.interpolate_bads]\n\n    %% Input file\n    F[\"\ud83d\udcc1 sub-01_task-rest_raw.fifRaw EEG data\"] --&gt; A\n\n    %% Final output\n    E --&gt; G[\"\ud83d\udcbe sub-01_task-rest_desc-preproc_eeg.fifPreprocessed data\"]\n\n    %% Quality metrics\n    D --&gt; Q1[\"\ud83d\udcc8 Projection added: true\"]\n    E --&gt; Q2[\"\ud83d\udcc8 Channels interpolated: [EEG 053, EEG 021]\"]\n\n    %% Styling\n    classDef processStep fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef inputFile fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    classDef outputFile fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px\n    classDef qualityMetric fill:#f9f9f9,stroke:#666,stroke-width:1px\n\n    class A,B,C,D,E processStep\n    class F inputFile\n    class G outputFile\n    class Q1,Q2 qualityMetric\n</code></pre>"},{"location":"examples/basic_preprocessing_mne/#key-mne-python-features-demonstrated","title":"Key MNE-Python Features Demonstrated","text":""},{"location":"examples/basic_preprocessing_mne/#mne-python-function-calls","title":"MNE-Python Function Calls","text":"<ul> <li><code>mne.io.read_raw_fif</code>: Load FIF format files with preloading</li> <li><code>raw.filter</code>: FIR filtering with linear phase response</li> <li><code>raw.notch_filter</code>: Notch filtering for line noise removal</li> <li><code>raw.set_eeg_reference</code>: EEG referencing with projection</li> <li><code>raw.interpolate_bads</code>: Spherical spline interpolation</li> </ul>"},{"location":"examples/basic_preprocessing_mne/#mne-python-specific-parameters","title":"MNE-Python-Specific Parameters","text":"<ul> <li>Filter specifications: FIR design with 'firwin' method</li> <li>Reference handling: Projection-based average referencing</li> <li>Channel interpolation: Accurate mode with automatic bad channel reset</li> </ul>"},{"location":"examples/basic_preprocessing_mne/#example-json-structure","title":"Example JSON Structure","text":"<p>The signalJourney file documents each processing step with:</p> <pre><code>{\n  \"stepId\": \"2\",\n  \"name\": \"Apply Band-pass Filter\",\n  \"description\": \"Apply a FIR band-pass filter (1-40 Hz).\",\n  \"software\": {\n    \"name\": \"MNE-Python\",\n    \"version\": \"1.6.1\",\n    \"functionCall\": \"raw.filter(l_freq=1.0, h_freq=40.0, fir_design='firwin')\"\n  },\n  \"parameters\": {\n    \"l_freq\": 1.0,\n    \"h_freq\": 40.0,\n    \"method\": \"fir\",\n    \"fir_design\": \"firwin\",\n    \"phase\": \"zero\"\n  }\n}\n</code></pre>"},{"location":"examples/basic_preprocessing_mne/#quality-control-integration","title":"Quality Control Integration","text":"<p>Each step includes quality metrics specific to MNE-Python processing: - Projection status for referencing - Interpolated channel tracking - BIDS-compatible file naming</p>"},{"location":"examples/basic_preprocessing_mne/#mne-python-vs-eeglab-comparison","title":"MNE-Python vs EEGLAB Comparison","text":"Aspect MNE-Python Version EEGLAB Version Data Format .fif files .set/.fdt files Filtering <code>filter</code>, <code>notch_filter</code> <code>pop_eegfiltnew</code> Referencing <code>set_eeg_reference</code> <code>pop_reref</code> Interpolation <code>interpolate_bads</code> <code>pop_interp</code> Function Style Object methods Pop-up GUI functions"},{"location":"examples/basic_preprocessing_mne/#usage-notes","title":"Usage Notes","text":"<p>This example demonstrates: - MNE-Python workflow patterns with object-oriented design - Parameter documentation for reproducible processing - BIDS compatibility with standardized naming conventions - Quality metrics relevant to MNE-Python processing</p> <p>The pipeline serves as a foundation for more complex analysis workflows including ICA decomposition, time-frequency analysis, and source localization.</p>"},{"location":"examples/best_practices/","title":"Best Practices","text":"<p>This section outlines recommended best practices when creating and using signalJourney files.</p>"},{"location":"examples/best_practices/#file-naming","title":"File Naming","text":"<ul> <li>Follow the BIDS (Brain Imaging Data Structure) naming convention where applicable, typically <code>sub-&lt;label&gt;_task-&lt;label&gt;_signalJourney.json</code>.</li> <li>Use descriptive names for files not strictly part of a BIDS dataset.</li> </ul>"},{"location":"examples/best_practices/#metadata-completeness","title":"Metadata Completeness","text":"<ul> <li>Provide comprehensive information in <code>pipelineInfo</code>, including the pipeline's purpose, version, and execution context.</li> <li>Clearly document the software used in each <code>processingStep</code>, including version numbers.</li> </ul>"},{"location":"examples/best_practices/#parameter-documentation","title":"Parameter Documentation","text":"<ul> <li>Be specific about parameter values.</li> <li>Use the <code>description</code> field for parameters to clarify their meaning or units if not standard.</li> <li>Document the source of parameters if they are derived (e.g., calculated from data, user input).</li> </ul>"},{"location":"examples/best_practices/#inputoutput-linking","title":"Input/Output Linking","text":"<ul> <li>Clearly define <code>inputSources</code> and <code>outputTargets</code> for each step.</li> <li>Use <code>sourceType: \"previousStepOutput\"</code> and consistent <code>outputId</code> values to explicitly link consecutive steps.</li> <li>For inputs originating from other pipelines, use <code>pipelineSource</code> to document the provenance.</li> </ul>"},{"location":"examples/best_practices/#quality-metrics","title":"Quality Metrics","text":"<ul> <li>Include relevant <code>qualityMetrics</code> at both the step level and in <code>summaryMetrics</code>.</li> <li>Define metrics clearly (e.g., what does a specific score represent?).</li> <li>Be consistent in the metrics reported for similar processing steps.</li> </ul>"},{"location":"examples/best_practices/#versioning","title":"Versioning","text":"<ul> <li>Keep <code>sj_version</code> updated to reflect the specification version being followed.</li> <li>Increment <code>pipelineInfo.version</code> appropriately when the pipeline logic changes.</li> </ul>"},{"location":"examples/best_practices/#extensibility","title":"Extensibility","text":"<ul> <li>Use the <code>extensions</code> field for non-standard information, potentially organizing by domain (e.g., <code>extensions.eeg</code>).</li> <li>Document custom extension fields clearly.</li> </ul>"},{"location":"examples/best_practices/#validation","title":"Validation","text":"<ul> <li>Regularly validate your signalJourney files against the official schema using the provided validator tools.</li> </ul> <p>(More best practices will be added here) </p>"},{"location":"examples/connectivity_eeglab/","title":"Example: Connectivity Analysis (EEGLAB)","text":"<p>This page explains the <code>connectivity_analysis_pipeline_eeglab.signalJourney.json</code> example file, which documents spectral connectivity analysis using EEGLAB and MATLAB functions.</p>"},{"location":"examples/connectivity_eeglab/#pipeline-overview","title":"Pipeline Overview","text":"<p>This EEGLAB pipeline demonstrates connectivity analysis for examining relationships between EEG channels: - Load cleaned data from ICA decomposition pipeline - Extract epochs for connectivity analysis - Compute power spectral density using EEGLAB's <code>spectopo</code> - Calculate coherence matrix using MATLAB Signal Processing Toolbox - Generate connectivity report with network visualizations</p>"},{"location":"examples/connectivity_eeglab/#pipeline-flowchart","title":"Pipeline Flowchart","text":"<pre><code>flowchart TD\n    A[Load Cleaned Datapop_loadset] --&gt; B[Extract Epochspop_epoch]\n    B --&gt; C[Compute PSDspectopo]\n    C --&gt; D[Calculate Coherencemscohere]\n    D --&gt; E[Generate Network Plottopoplot_connect]\n    E --&gt; F[Save Resultssave]\n\n    %% Input file\n    G[\"\ud83d\udcc1 sub-01_task-rest_desc-cleaned_eeg.setFrom: ICA Decomposition Pipeline\"] --&gt; A\n\n    %% Inline data\n    B --&gt; V1[\"\ud83d\udcca Epoch Windows[-1.0, 2.0] s\"]\n    C --&gt; V2[\"\ud83d\udcca Frequency RangeAlpha: 8-12 Hz\"]\n    D --&gt; V3[\"\ud83d\udcca Coherence MethodMATLAB mscohere\"]\n\n    %% Final outputs\n    F --&gt; H[\"\ud83d\udcbe sub-01_task-rest_desc-connectivity_eeg.matConnectivity results\"]\n    E --&gt; I[\"\ud83d\udcbe sub-01_task-rest_desc-network_plot.figNetwork visualization\"]\n    E --&gt; J[\"\ud83d\udcbe sub-01_task-rest_desc-connectivity_matrix.pngMatrix plot\"]\n\n    %% Quality metrics\n    C --&gt; Q1[\"\ud83d\udcc8 Frequency bins: 256Spectral resolution: 0.25 Hz\"]\n    D --&gt; Q2[\"\ud83d\udcc8 Significant pairs: 312/2016Max coherence: 0.78\"]\n\n    %% Styling\n    classDef processStep fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef inputFile fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    classDef outputFile fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px\n    classDef inlineData fill:#f3e5f5,stroke:#4a148c,stroke-width:1px\n    classDef qualityMetric fill:#f9f9f9,stroke:#666,stroke-width:1px\n\n    class A,B,C,D,E,F processStep\n    class G inputFile\n    class H,I,J outputFile\n    class V1,V2,V3 inlineData\n    class Q1,Q2 qualityMetric\n</code></pre>"},{"location":"examples/connectivity_eeglab/#key-eeglab-features-demonstrated","title":"Key EEGLAB Features Demonstrated","text":""},{"location":"examples/connectivity_eeglab/#eeglab-connectivity-functions","title":"EEGLAB Connectivity Functions","text":"<ul> <li><code>pop_epoch</code>: Extract event-related epochs from continuous data</li> <li><code>spectopo</code>: Power spectral density computation with multitaper method</li> <li><code>mscohere</code>: MATLAB coherence calculation between channel pairs</li> <li><code>topoplot_connect</code>: Network visualization on scalp topography</li> <li>MATLAB integration: Leveraging Signal Processing Toolbox functions</li> </ul>"},{"location":"examples/connectivity_eeglab/#eeglab-specific-parameters","title":"EEGLAB-Specific Parameters","text":"<ul> <li>Epoch extraction: EEGLAB event-based epoching with GUI support</li> <li>Spectral analysis: Integrated with EEGLAB channel structure</li> <li>Visualization: EEGLAB topographic plotting with connectivity overlays</li> <li>File formats: MATLAB .mat files for connectivity matrices</li> </ul>"},{"location":"examples/connectivity_eeglab/#example-json-structure","title":"Example JSON Structure","text":"<p>The EEGLAB coherence computation demonstrates MATLAB integration:</p> <pre><code>{\n  \"stepId\": \"4\",\n  \"name\": \"Calculate Coherence Matrix\",\n  \"description\": \"Compute magnitude-squared coherence between all channel pairs using MATLAB mscohere.\",\n  \"software\": {\n    \"name\": \"MATLAB\",\n    \"version\": \"R2023a\",\n    \"functionCall\": \"for i=1:64; for j=1:64; [coh,f] = mscohere(epochs(i,:), epochs(j,:), window, noverlap, nfft, srate); end; end\"\n  },\n  \"parameters\": {\n    \"window\": \"hann(128)\",\n    \"noverlap\": 64,\n    \"nfft\": 256,\n    \"srate\": 500,\n    \"freq_range\": [8, 12]\n  }\n}\n</code></pre>"},{"location":"examples/connectivity_eeglab/#eeglab-dataset-integration","title":"EEGLAB Dataset Integration","text":"<p>EEGLAB connectivity analysis leverages the EEG structure:</p> <pre><code>{\n  \"stepId\": \"3\",\n  \"name\": \"Compute Power Spectral Density\",\n  \"software\": {\n    \"name\": \"EEGLAB\",\n    \"version\": \"2023.1\",\n    \"functionCall\": \"[spectra,freqs] = spectopo(EEG.data, 0, EEG.srate, 'freqrange', [1 40], 'electrodes', 'on')\"\n  },\n  \"parameters\": {\n    \"freqrange\": [1, 40],\n    \"electrodes\": \"on\",\n    \"overlap\": 50,\n    \"nfft\": 256,\n    \"winsize\": 128\n  }\n}\n</code></pre>"},{"location":"examples/connectivity_eeglab/#eeglab-connectivity-features","title":"EEGLAB Connectivity Features","text":""},{"location":"examples/connectivity_eeglab/#matlab-signal-processing-integration","title":"MATLAB Signal Processing Integration","text":"<ul> <li>mscohere function: Magnitude-squared coherence with Welch's method</li> <li>Cross-spectral density: Full spectral analysis capabilities</li> <li>Windowing options: Hann, Hamming, Bartlett windows</li> <li>Frequency resolution: Configurable FFT parameters</li> </ul>"},{"location":"examples/connectivity_eeglab/#eeglab-visualization-tools","title":"EEGLAB Visualization Tools","text":"<ul> <li>topoplot integration: Connectivity overlaid on channel locations</li> <li>Channel location support: 3D electrode positions for accurate plotting</li> <li>Network graphs: Node-edge representations of connectivity</li> <li>Matrix visualization: Heatmaps and connectivity matrices</li> </ul>"},{"location":"examples/connectivity_eeglab/#quality-control-features","title":"Quality Control Features","text":"<ul> <li>Spectral validation: Power spectral density verification</li> <li>Coherence thresholds: Statistical significance testing</li> <li>Channel quality: Bad channel identification and exclusion</li> <li>Epoch rejection: Artifact-contaminated epoch removal</li> </ul>"},{"location":"examples/connectivity_eeglab/#eeglab-vs-mne-python-comparison","title":"EEGLAB vs MNE-Python Comparison","text":"Aspect EEGLAB Version MNE-Python Version Coherence Function <code>mscohere</code> (MATLAB) <code>spectral_connectivity_epochs</code> PSD Computation <code>spectopo</code> <code>compute_psd()</code> Visualization <code>topoplot_connect</code> matplotlib/mayavi File Format .mat files HDF5, NPZ Integration MATLAB ecosystem Python ecosystem GUI Support Built-in EEGLAB GUI Command-line focused"},{"location":"examples/connectivity_eeglab/#eeglab-specific-workflow","title":"EEGLAB-Specific Workflow","text":""},{"location":"examples/connectivity_eeglab/#matlab-ecosystem-integration","title":"MATLAB Ecosystem Integration","text":"<p>EEGLAB connectivity analysis benefits from: 1. Signal Processing Toolbox: Professional-grade spectral analysis functions 2. Parallel Computing: Multi-core coherence computation  3. Visualization Tools: Advanced plotting and 3D visualization 4. Statistical Toolbox: Comprehensive statistical testing capabilities</p>"},{"location":"examples/connectivity_eeglab/#eeg-structure-preservation","title":"EEG Structure Preservation","text":"<ul> <li>Channel information: Electrode locations automatically used</li> <li>Event markers: Epoch extraction based on EEG.event structure</li> <li>Sampling rate: Automatically extracted from EEG.srate</li> <li>Data history: Processing steps recorded in EEG.history</li> </ul>"},{"location":"examples/connectivity_eeglab/#interactive-analysis","title":"Interactive Analysis","text":"<ul> <li>GUI integration: Pop-up functions for parameter selection</li> <li>Visual feedback: Real-time plotting during analysis</li> <li>Manual adjustment: Interactive parameter tuning</li> <li>Batch processing: Automated analysis across multiple datasets</li> </ul>"},{"location":"examples/connectivity_eeglab/#usage-notes","title":"Usage Notes","text":"<p>This example demonstrates: - EEGLAB connectivity workflows using MATLAB integration - Spectral analysis integration with EEGLAB functions - Multi-format outputs for matrices and visualizations - Quality control with spectral validation - MATLAB ecosystem leveraging Signal Processing Toolbox</p> <p>The pipeline showcases EEGLAB's connectivity analysis capabilities through MATLAB integration while maintaining comprehensive parameter documentation for reproducible network analysis. The combination of EEGLAB's EEG-specific tools with MATLAB's signal processing functions provides a powerful framework for connectivity research. </p>"},{"location":"examples/connectivity_mne/","title":"Example: Connectivity Analysis (MNE-Python)","text":"<p>This page explains the <code>connectivity_analysis_pipeline_mne.signalJourney.json</code> example file, which documents a functional connectivity analysis workflow. This pipeline calculates spectral coherence between EEG sensors using the MNE-Python and SciPy libraries.</p>"},{"location":"examples/connectivity_mne/#pipeline-overview","title":"Pipeline Overview","text":"<p>This MNE-Python pipeline demonstrates connectivity analysis: - Load cleaned data from ICA decomposition pipeline - Extract epochs for connectivity analysis - Compute power spectral density for individual channels - Calculate coherence matrix between channel pairs - Generate connectivity report with visualization</p>"},{"location":"examples/connectivity_mne/#pipeline-flowchart","title":"Pipeline Flowchart","text":"<pre><code>flowchart TD\n    A[Load Cleaned Datamne.io.read_raw_fif] --&gt; B[Extract Epochsmne.Epochs]\n    B --&gt; C[Compute PSDepochs.compute_psd]\n    C --&gt; D[Calculate Coherencemne.connectivity.spectral_connectivity_epochs]\n    D --&gt; E[Generate Reportconnectivity_report]\n\n    %% Input file\n    F[\"\ud83d\udcc1 sub-01_task-rest_desc-cleaned_eeg.fifFrom: ICA Decomposition Pipeline\"] --&gt; A\n\n    %% Inline data\n    B --&gt; V1[\"\ud83d\udcca Event Windows[-0.5, 1.5] s\"]\n    C --&gt; V2[\"\ud83d\udcca Frequency BandsAlpha: 8-12 Hz\"]\n    D --&gt; V3[\"\ud83d\udcca Connectivity MethodCoherence (magnitude)\"]\n\n    %% Final outputs\n    E --&gt; G[\"\ud83d\udcbe sub-01_task-rest_desc-connectivity_eeg.h5Connectivity matrix\"]\n    E --&gt; H[\"\ud83d\udcbe sub-01_task-rest_desc-connectivity_plot.pngConnectivity visualization\"]\n\n    %% Quality metrics\n    C --&gt; Q1[\"\ud83d\udcc8 Frequency resolution: 0.5 HzSpectral range: 1-40 Hz\"]\n    D --&gt; Q2[\"\ud83d\udcc8 Significant connections: 245/2016Mean coherence: 0.32\"]\n\n    %% Styling\n    classDef processStep fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef inputFile fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    classDef outputFile fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px\n    classDef inlineData fill:#f3e5f5,stroke:#4a148c,stroke-width:1px\n    classDef qualityMetric fill:#f9f9f9,stroke:#666,stroke-width:1px\n\n    class A,B,C,D,E processStep\n    class F inputFile\n    class G,H outputFile\n    class V1,V2,V3 inlineData\n    class Q1,Q2 qualityMetric\n</code></pre>"},{"location":"examples/connectivity_mne/#key-mne-python-features-demonstrated","title":"Key MNE-Python Features Demonstrated","text":""},{"location":"examples/connectivity_mne/#connectivity-functions","title":"Connectivity Functions","text":"<ul> <li><code>mne.Epochs</code>: Extract task-related epochs for connectivity analysis</li> <li><code>epochs.compute_psd</code>: Calculate power spectral density using multitaper method</li> <li><code>mne.connectivity.spectral_connectivity_epochs</code>: Compute spectral connectivity measures</li> <li>Connectivity metrics: Coherence, PLI, PLV, and other measures</li> </ul>"},{"location":"examples/connectivity_mne/#advanced-parameters","title":"Advanced Parameters","text":"<ul> <li>Frequency bands: Specific frequency ranges for connectivity analysis</li> <li>Connectivity methods: Multiple algorithms for different connectivity aspects</li> <li>Statistical testing: Permutation tests for connectivity significance</li> <li>Visualization: Network plots and connectivity matrices</li> </ul>"},{"location":"examples/connectivity_mne/#example-json-structure","title":"Example JSON Structure","text":"<p>The connectivity computation demonstrates advanced parameter documentation:</p> <pre><code>{\n  \"stepId\": \"4\",\n  \"name\": \"Calculate Spectral Coherence\",\n  \"description\": \"Compute coherence between all channel pairs in alpha band.\",\n  \"software\": {\n    \"name\": \"MNE-Python\",\n    \"version\": \"1.6.1\",\n    \"functionCall\": \"mne.connectivity.spectral_connectivity_epochs(epochs, method='coh', fmin=8, fmax=12)\"\n  },\n  \"parameters\": {\n    \"method\": \"coh\",\n    \"fmin\": 8,\n    \"fmax\": 12,\n    \"sfreq\": 1000,\n    \"n_jobs\": 1,\n    \"verbose\": false\n  }\n}\n</code></pre>"},{"location":"examples/connectivity_mne/#multi-output-connectivity-analysis","title":"Multi-Output Connectivity Analysis","text":"<p>Connectivity analysis typically produces multiple related outputs:</p> <pre><code>\"outputTargets\": [\n  {\n    \"targetType\": \"file\",\n    \"location\": \"./derivatives/signaljourney/sub-01/eeg/sub-01_task-rest_desc-connectivity_eeg.h5\",\n    \"format\": \"HDF5\",\n    \"description\": \"Connectivity matrix with channel pairs and frequency bins.\"\n  },\n  {\n    \"targetType\": \"inlineData\",\n    \"name\": \"connectivity_matrix\",\n    \"data\": \"{{coherence_matrix_alpha}}\",\n    \"description\": \"Alpha band coherence matrix (64x64).\"\n  }\n]\n</code></pre>"},{"location":"examples/connectivity_mne/#connectivity-analysis-features","title":"Connectivity Analysis Features","text":""},{"location":"examples/connectivity_mne/#frequency-specific-analysis","title":"Frequency-Specific Analysis","text":"<ul> <li>Band-specific connectivity: Focus on particular frequency ranges (alpha, beta, gamma)</li> <li>Broadband analysis: Connectivity across multiple frequency bands</li> <li>Time-frequency connectivity: Dynamic connectivity over time</li> <li>Cross-frequency coupling: Interactions between different frequency bands</li> </ul>"},{"location":"examples/connectivity_mne/#connectivity-metrics","title":"Connectivity Metrics","text":"<ul> <li>Coherence: Linear relationship in frequency domain</li> <li>Phase-Locking Value (PLV): Phase synchronization between signals</li> <li>Phase Lag Index (PLI): Phase relationship corrected for volume conduction</li> <li>Granger Causality: Directed connectivity measures</li> </ul>"},{"location":"examples/connectivity_mne/#statistical-assessment","title":"Statistical Assessment","text":"<ul> <li>Permutation testing: Significance testing for connectivity values</li> <li>Multiple comparison correction: Control for multiple channel pairs</li> <li>Confidence intervals: Bootstrap confidence bounds for connectivity</li> <li>Network measures: Graph theory metrics on connectivity networks</li> </ul>"},{"location":"examples/connectivity_mne/#mne-python-vs-eeglab-comparison","title":"MNE-Python vs EEGLAB Comparison","text":"Aspect MNE-Python Version EEGLAB Version Methods Multiple algorithms <code>mscohere</code>, coherence2 Frequency Analysis <code>compute_psd()</code> <code>spectopo</code> Statistical Testing Built-in permutations External testing Visualization matplotlib/mayavi EEGLAB plots Output Format HDF5, NPZ .mat files"},{"location":"examples/connectivity_mne/#usage-notes","title":"Usage Notes","text":"<p>This example demonstrates: - Connectivity analysis workflows with frequency-specific measures - Statistical testing integration for significance assessment - Multi-output documentation for matrices and visualizations - Quality metrics for connectivity analysis validation - Pipeline integration building on previous processing steps</p> <p>The pipeline showcases MNE-Python's comprehensive connectivity analysis capabilities while maintaining full parameter transparency for reproducible network analysis. </p>"},{"location":"examples/ica_decomposition_eeglab/","title":"Example: ICA Decomposition Pipeline (EEGLAB)","text":"<p>This page explains the <code>ica_decomposition_pipeline_eeglab.signalJourney.json</code> example file, which documents an Independent Component Analysis (ICA) workflow for artifact removal using EEGLAB.</p>"},{"location":"examples/ica_decomposition_eeglab/#pipeline-overview","title":"Pipeline Overview","text":"<p>This EEGLAB pipeline demonstrates how to apply ICA to remove artifacts from preprocessed EEG data using EEGLAB's ICA functions and ICLabel for component classification:</p> <ul> <li>Loading preprocessed data from the basic preprocessing pipeline</li> <li>Computing ICA decomposition using extended Infomax algorithm</li> <li>Classifying components with ICLabel to identify artifacts</li> <li>Removing artifact components from the data</li> <li>Saving cleaned data for further analysis</li> </ul>"},{"location":"examples/ica_decomposition_eeglab/#pipeline-flowchart","title":"Pipeline Flowchart","text":"<pre><code>flowchart TD\n    A[Load Preprocessed Datapop_loadset] --&gt; B[Run ICA Decompositionpop_runica]\n    B --&gt; C[Classify Componentspop_iclabel]\n    C --&gt; D[Select Componentspop_selectcomps]\n    D --&gt; E[Remove Componentspop_subcomp]\n    E --&gt; F[Save Cleaned Datapop_saveset]\n\n    %% Input file\n    G[\"\ud83d\udcc1 sub-01_task-rest_desc-preproc_eeg.setFrom: Basic Preprocessing Pipeline\"] --&gt; A\n\n    %% Inline data\n    C --&gt; V3[\"\ud83d\udcca Classification ScoresBrain: 0.92, Eye: 0.05\"]\n    E --&gt; V4[\"\ud83d\udcca Artifact Components[2, 15, 23] removed\"]\n\n    %% Final outputs\n    F --&gt; H[\"\ud83d\udcbe sub-01_task-rest_desc-cleaned_eeg.setICA-cleaned dataset\"]\n    D --&gt; I[\"\ud83d\udcbe sub-01_task-rest_desc-components_review.figComponent browser\"]\n    C --&gt; J[\"\ud83d\udcbe sub-01_task-rest_desc-iclabel_report.matClassification results\"]\n\n    %% Quality metrics\n    C --&gt; Q1[\"\ud83d\udcc8 Brain components: 58/64Muscle components: 3/64\"]\n    E --&gt; Q2[\"\ud83d\udcc8 Components removed: 3Variance preserved: 92.1%\"]\n\n    %% Styling\n    classDef processStep fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef inputFile fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    classDef outputFile fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px\n    classDef inlineData fill:#f3e5f5,stroke:#4a148c,stroke-width:1px\n    classDef qualityMetric fill:#f9f9f9,stroke:#666,stroke-width:1px\n\n    class A,B,C,D,E,F processStep\n    class G inputFile\n    class H,I,J outputFile\n    class V3,V4 inlineData\n    class Q1,Q2 qualityMetric\n</code></pre>"},{"location":"examples/ica_decomposition_eeglab/#key-eeglab-features-demonstrated","title":"Key EEGLAB Features Demonstrated","text":""},{"location":"examples/ica_decomposition_eeglab/#ica-specific-functions","title":"ICA-Specific Functions","text":"<ul> <li><code>pop_runica</code>: Extended Infomax ICA decomposition</li> <li><code>pop_iclabel</code>: Automated component classification using deep learning</li> <li><code>pop_selectcomps</code>: Interactive component selection interface</li> <li><code>pop_subcomp</code>: Remove components from EEG data</li> <li>ICLabel integration: Brain vs. artifact classification with probability scores</li> </ul>"},{"location":"examples/ica_decomposition_eeglab/#eeglab-specific-parameters","title":"EEGLAB-Specific Parameters","text":"<ul> <li>Extended Infomax: Natural gradient algorithm for ICA</li> <li>ICLabel classification: Automated artifact detection using trained models</li> <li>Interactive selection: GUI-based component review and selection</li> <li>Dataset integration: Seamless EEGLAB structure preservation</li> </ul>"},{"location":"examples/ica_decomposition_eeglab/#example-json-structure","title":"Example JSON Structure","text":"<p>The ICA decomposition step demonstrates complex output documentation:</p> <pre><code>{\n  \"stepId\": \"2\",\n  \"name\": \"Run ICA Decomposition\",\n  \"description\": \"Compute ICA decomposition using extended Infomax algorithm.\",\n  \"software\": {\n    \"name\": \"EEGLAB\",\n    \"version\": \"2023.1\",\n    \"functionCall\": \"pop_runica(EEG, 'icatype', 'runica', 'extended', 1, 'interrupt', 'off')\"\n  },\n  \"parameters\": {\n    \"icatype\": \"runica\",\n    \"extended\": 1,\n    \"interrupt\": \"off\",\n    \"pca\": null,\n    \"lrate\": 0.001,\n    \"maxsteps\": 500,\n    \"stop\": 1e-7\n  }\n}\n</code></pre>"},{"location":"examples/ica_decomposition_eeglab/#iclabel-component-classification","title":"ICLabel Component Classification","text":"<p>The ICLabel step showcases automated artifact classification:</p> <pre><code>{\n  \"stepId\": \"3\", \n  \"name\": \"Classify Components with ICLabel\",\n  \"description\": \"Automatically classify ICA components using ICLabel deep learning model.\",\n  \"software\": {\n    \"name\": \"EEGLAB\",\n    \"version\": \"2023.1\",\n    \"functionCall\": \"EEG = pop_iclabel(EEG, 'default')\"\n  },\n  \"qualityMetrics\": {\n    \"brain_components\": 58,\n    \"muscle_components\": 3,\n    \"eye_components\": 2,\n    \"heart_components\": 0,\n    \"line_noise_components\": 1,\n    \"channel_noise_components\": 0,\n    \"other_components\": 0\n  }\n}\n</code></pre>"},{"location":"examples/ica_decomposition_eeglab/#advanced-signaljourney-features","title":"Advanced signalJourney Features","text":"<ul> <li>Variable storage: ICA weights and sphere matrices</li> <li>Inline data preservation: Component timecourses and topographies</li> <li>Quality metrics: Classification probabilities and removal statistics</li> <li>Multi-output steps: Steps generating both files and variables</li> </ul>"},{"location":"examples/ica_decomposition_eeglab/#eeglab-ica-workflow","title":"EEGLAB ICA Workflow","text":""},{"location":"examples/ica_decomposition_eeglab/#extended-infomax-algorithm","title":"Extended Infomax Algorithm","text":"<p>EEGLAB's <code>pop_runica</code> provides: - Natural gradient optimization: Efficient convergence for EEG data - Extended model: Handles both sub- and super-Gaussian sources - Robust convergence: Adaptive learning rate and stopping criteria - Reproducible results: Fixed random seed options</p>"},{"location":"examples/ica_decomposition_eeglab/#iclabel-classification-system","title":"ICLabel Classification System","text":"<ul> <li>Deep learning model: Trained on thousands of manually labeled components</li> <li>Multiple artifact types: Brain, muscle, eye, heart, line noise, channel noise</li> <li>Probability scores: Confidence levels for each classification</li> <li>Automatic thresholding: Configurable probability thresholds for removal</li> </ul>"},{"location":"examples/ica_decomposition_eeglab/#interactive-component-review","title":"Interactive Component Review","text":"<ul> <li>Visual inspection: Component topographies, time courses, and spectra</li> <li>Classification overlay: ICLabel probabilities displayed for each component</li> <li>Manual override: User can override automatic classifications</li> <li>Batch selection: Tools for selecting multiple components efficiently</li> </ul>"},{"location":"examples/ica_decomposition_eeglab/#eeglab-vs-mne-python-comparison","title":"EEGLAB vs MNE-Python Comparison","text":"Aspect EEGLAB Version MNE-Python Version Algorithm Extended Infomax (runica) FastICA, Infomax, Picard Classification ICLabel (automated) Manual inspection + correlation Component Removal <code>pop_subcomp</code> <code>ica.apply()</code> Visualization Built-in component plots Custom plotting functions Integration Seamless EEGLAB workflow Object-oriented approach Automation Highly automated Semi-automatic workflow"},{"location":"examples/ica_decomposition_eeglab/#advanced-features","title":"Advanced Features","text":""},{"location":"examples/ica_decomposition_eeglab/#component-classification-workflow","title":"Component Classification Workflow","text":"<ol> <li>Automated Classification: ICLabel provides probability scores for each component type</li> <li>Threshold-Based Selection: Components automatically flagged based on classification confidence</li> <li>Quality Assurance: Detailed metrics on component types and removal decisions</li> <li>Manual Review: Interactive interface for validation and adjustment</li> </ol>"},{"location":"examples/ica_decomposition_eeglab/#data-provenance-and-quality-control","title":"Data Provenance and Quality Control","text":"<ul> <li>Pipeline Dependencies: Clear links to preprocessing steps</li> <li>Parameter Tracking: Complete ICA algorithm parameters</li> <li>Component Documentation: Full record of which components were removed and why</li> <li>Classification Confidence: Probability scores for reproducibility</li> </ul>"},{"location":"examples/ica_decomposition_eeglab/#eeglab-dataset-integration","title":"EEGLAB Dataset Integration","text":"<ul> <li>Structure Preservation: EEG structure maintained throughout processing</li> <li>History Tracking: All operations recorded in EEG.history</li> <li>Compatibility: Works seamlessly with other EEGLAB functions</li> <li>STUDY Integration: Results compatible with group-level analysis</li> </ul>"},{"location":"examples/ica_decomposition_eeglab/#usage-notes","title":"Usage Notes","text":"<p>This example demonstrates: - Advanced EEGLAB workflows with ICA and automated artifact classification - ICLabel integration for objective component classification - Complex data documentation with multiple output types - Quality control integration for comprehensive artifact removal validation - Interactive workflow support with manual review capabilities</p> <p>The pipeline showcases signalJourney's ability to document sophisticated processing workflows while maintaining the flexibility needed for various EEGLAB analysis approaches. The combination of automated classification and manual review provides a robust framework for artifact removal that balances efficiency with quality control. </p>"},{"location":"examples/ica_decomposition_mne/","title":"Example: ICA Decomposition Pipeline (MNE-Python)","text":"<p>This page explains the <code>ica_decomposition_pipeline_mne.signalJourney.json</code> example file, which documents an Independent Component Analysis (ICA) workflow for artifact removal using MNE-Python.</p>"},{"location":"examples/ica_decomposition_mne/#pipeline-overview","title":"Pipeline Overview","text":"<p>This pipeline demonstrates how to apply ICA to remove ocular artifacts (eye movements and blinks) from preprocessed EEG data. It builds upon the Basic Preprocessing Pipeline by:</p> <ul> <li>Loading preprocessed data from the previous pipeline</li> <li>Fitting ICA decomposition using FastICA algorithm</li> <li>Identifying artifact components correlated with EOG channels</li> <li>Removing artifact components from the data</li> <li>Saving cleaned data for further analysis</li> </ul>"},{"location":"examples/ica_decomposition_mne/#pipeline-flowchart","title":"Pipeline Flowchart","text":"<pre><code>flowchart TD\n    A[Load Preprocessed Datamne.io.read_raw_fif] --&gt; B[Fit ICAica.fit]\n    B --&gt; C[Find EOG Componentsica.find_bads_eog]\n    C --&gt; D[Review Componentsica.plot_components]\n    D --&gt; E[Apply ICAica.apply]\n    E --&gt; F[Save Cleaned Dataraw.save]\n\n    %% Input file\n    G[\"\ud83d\udcc1 sub-01_task-rest_desc-preproc_eeg.fifFrom: Basic Preprocessing Pipeline\"] --&gt; A\n\n    %% Inline data\n    C --&gt; V3[\"\ud83d\udcca Bad Components[0, 15, 23] identified\"]\n\n    %% Final outputs\n    F --&gt; H[\"\ud83d\udcbe sub-01_task-rest_desc-cleaned_eeg.fifICA-cleaned data\"]\n    D --&gt; I[\"\ud83d\udcbe sub-01_task-rest_desc-components_plot.pngComponent visualization\"]\n\n    %% Quality metrics\n    C --&gt; Q1[\"\ud83d\udcc8 Components removed: 3/64Explained variance: 15.7%\"]\n    E --&gt; Q2[\"\ud83d\udcc8 EOG correlation: 0.82SNR improvement: 4.2 dB\"]\n\n    %% Styling\n    classDef processStep fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef inputFile fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    classDef outputFile fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px\n    classDef inlineData fill:#f3e5f5,stroke:#4a148c,stroke-width:1px\n    classDef qualityMetric fill:#f9f9f9,stroke:#666,stroke-width:1px\n\n    class A,B,C,D,E,F processStep\n    class G inputFile\n    class H,I outputFile\n    class V3 inlineData\n    class Q1,Q2 qualityMetric\n</code></pre>"},{"location":"examples/ica_decomposition_mne/#key-mne-python-features-demonstrated","title":"Key MNE-Python Features Demonstrated","text":""},{"location":"examples/ica_decomposition_mne/#ica-functions-and-methods","title":"ICA Functions and Methods","text":"<ul> <li><code>mne.preprocessing.ICA</code>: Create ICA object with FastICA algorithm</li> <li><code>ica.fit</code>: Fit ICA decomposition to preprocessed data</li> <li><code>ica.find_bads_eog</code>: Automatically identify EOG-correlated components</li> <li><code>ica.plot_components</code>: Visualize component topographies and properties</li> <li><code>ica.apply</code>: Remove identified artifact components from data</li> </ul>"},{"location":"examples/ica_decomposition_mne/#advanced-ica-parameters","title":"Advanced ICA Parameters","text":"<ul> <li>Algorithm selection: FastICA vs. Infomax vs. Picard algorithms</li> <li>Component number: Automatic or manual specification</li> <li>Convergence criteria: Tolerance and maximum iterations</li> <li>Random state: Reproducible decomposition results</li> </ul>"},{"location":"examples/ica_decomposition_mne/#example-json-structure","title":"Example JSON Structure","text":"<p>The ICA fitting step demonstrates complex parameter documentation:</p> <pre><code>{\n  \"stepId\": \"2\",\n  \"name\": \"Fit ICA Decomposition\",\n  \"description\": \"Apply FastICA to decompose data into independent components.\",\n  \"software\": {\n    \"name\": \"MNE-Python\",\n    \"version\": \"1.6.1\",\n    \"functionCall\": \"ica.fit(raw, picks='eeg', decim=2, reject=dict(eeg=100e-6))\"\n  },\n  \"parameters\": {\n    \"n_components\": 64,\n    \"algorithm\": \"fastica\",\n    \"fun\": \"logcosh\",\n    \"max_iter\": 200,\n    \"tol\": 1e-4,\n    \"w_init\": null,\n    \"whiten\": true,\n    \"random_state\": 42\n  }\n}\n</code></pre>"},{"location":"examples/ica_decomposition_mne/#artifact-identification-documentation","title":"Artifact Identification Documentation","text":"<p>The component identification step includes correlation thresholds:</p> <pre><code>{\n  \"stepId\": \"3\",\n  \"name\": \"Find EOG Components\",\n  \"description\": \"Identify components correlated with EOG channels.\",\n  \"qualityMetrics\": {\n    \"eogChannels\": [\"EOG001\", \"EOG002\"],\n    \"correlationThreshold\": 0.7,\n    \"componentsFound\": 3,\n    \"maxCorrelation\": 0.82,\n    \"explainedVariance\": 15.7\n  }\n}\n</code></pre>"},{"location":"examples/ica_decomposition_mne/#ica-analysis-features","title":"ICA Analysis Features","text":""},{"location":"examples/ica_decomposition_mne/#component-identification-methods","title":"Component Identification Methods","text":"<ul> <li>EOG correlation: Correlation with electrooculogram channels</li> <li>Variance explained: Contribution to total signal variance  </li> <li>Topographic patterns: Spatial distributions matching known artifacts</li> <li>Time course properties: Temporal characteristics of components</li> </ul>"},{"location":"examples/ica_decomposition_mne/#quality-assessment-metrics","title":"Quality Assessment Metrics","text":"<ul> <li>Correlation values: Quantify artifact-component relationships</li> <li>Explained variance: Component contribution to signal</li> <li>Signal-to-noise ratio: Improvement after component removal</li> <li>Component stability: Reproducibility across runs</li> </ul>"},{"location":"examples/ica_decomposition_mne/#mne-python-vs-eeglab-comparison","title":"MNE-Python vs EEGLAB Comparison","text":"Aspect MNE-Python Version EEGLAB Version Algorithm FastICA, Infomax, Picard Extended Infomax (runica) Identification <code>find_bads_eog()</code> ICLabel classification Visualization <code>plot_components()</code> <code>pop_selectcomps</code> Application <code>ica.apply()</code> <code>pop_subcomp</code> Automation Semi-automatic Manual + automatic"},{"location":"examples/ica_decomposition_mne/#ica-workflow-patterns","title":"ICA Workflow Patterns","text":""},{"location":"examples/ica_decomposition_mne/#component-selection-strategy","title":"Component Selection Strategy","text":"<ol> <li>Automatic detection: Use correlation thresholds with reference channels</li> <li>Visual inspection: Review topographies and time courses</li> <li>Combined approach: Automatic detection + manual verification</li> <li>Conservative removal: Remove only clearly artifactual components</li> </ol>"},{"location":"examples/ica_decomposition_mne/#quality-control-steps","title":"Quality Control Steps","text":"<ul> <li>Component stability: Ensure reproducible decomposition</li> <li>Artifact effectiveness: Verify artifact reduction in cleaned data</li> <li>Signal preservation: Confirm minimal neural signal loss</li> <li>Validation metrics: Compare before/after signal characteristics</li> </ul>"},{"location":"examples/ica_decomposition_mne/#usage-notes","title":"Usage Notes","text":"<p>This example demonstrates: - ICA workflow patterns for artifact removal - Parameter documentation for reproducible decomposition - Quality metrics for component evaluation - Visualization integration for manual review - Pipeline continuation feeding into further analysis</p> <p>The pipeline showcases MNE-Python's flexible ICA capabilities while emphasizing the importance of quality control and parameter documentation for reproducible artifact removal. </p>"},{"location":"examples/source_localization_eeglab/","title":"Example: Source Localization (EEGLAB)","text":"<p>This page explains the <code>source_localization_pipeline_eeglab.signalJourney.json</code> example file, which documents equivalent dipole modeling using EEGLAB's DIPFIT plugin.</p>"},{"location":"examples/source_localization_eeglab/#pipeline-overview","title":"Pipeline Overview","text":"<p>This EEGLAB pipeline demonstrates source localization using equivalent dipole modeling on ICA components: - Load ICA dataset with decomposed components from artifact removal pipeline - Initialize DIPFIT settings with head model and electrode locations - Coregister electrodes to the head model coordinate system - Perform grid search for initial dipole locations across components - Optimize dipole locations with nonlinear search algorithms - Save dipole results with locations and quality metrics</p>"},{"location":"examples/source_localization_eeglab/#pipeline-flowchart","title":"Pipeline Flowchart","text":"<pre><code>flowchart TD\n    A[Load ICA Datasetpop_loadset] --&gt; B[Initialize DIPFITpop_dipfit_settings]\n    B --&gt; C[Coregister Electrodespop_dipfit_batch]\n    C --&gt; D[Grid Search Dipolespop_dipfit_gridsearch]\n    D --&gt; E[Optimize Dipole Fitspop_dipfit_nonlinear]\n    E --&gt; F[Save Resultspop_saveset]\n\n    %% Input files\n    G[\"\ud83d\udcc1 sub-01_task-rest_desc-ica_eeg.setFrom: ICA decomposition\"] --&gt; A\n    H[\"\ud83d\udcc1 Standard BEMDIPFIT head model\"] --&gt; B\n    I[\"\ud83d\udcc1 Electrode TemplateStandard locations\"] --&gt; B\n\n    %% Inline data\n    B --&gt; V1[\"\ud83d\udcca Coordinate SystemMNI space\"]\n    D --&gt; V2[\"\ud83d\udcca Grid Resolution20mm spacing\"]\n    E --&gt; V3[\"\ud83d\udcca RV Threshold&lt; 15% residual\"]\n\n    %% Final outputs\n    F --&gt; J[\"\ud83d\udcbe sub-01_task-rest_desc-dipoles_eeg.setDataset with dipoles\"]\n    E --&gt; K[\"\ud83d\udcbe sub-01_task-rest_desc-dipole_plot.pngBrain visualization\"]\n\n    %% Quality metrics\n    D --&gt; Q1[\"\ud83d\udcc8 Components fitted: 25/32Initial RV: 12.3%\"]\n    E --&gt; Q2[\"\ud83d\udcc8 Optimized dipoles: 18Final RV: 8.7%\"]\n\n    %% Styling\n    classDef processStep fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef inputFile fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    classDef outputFile fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px\n    classDef inlineData fill:#f3e5f5,stroke:#4a148c,stroke-width:1px\n    classDef qualityMetric fill:#f9f9f9,stroke:#666,stroke-width:1px\n\n    class A,B,C,D,E,F processStep\n    class G,H,I inputFile\n    class J,K outputFile\n    class V1,V2,V3 inlineData\n    class Q1,Q2 qualityMetric\n</code></pre>"},{"location":"examples/source_localization_eeglab/#key-eeglab-dipfit-features-demonstrated","title":"Key EEGLAB DIPFIT Features Demonstrated","text":""},{"location":"examples/source_localization_eeglab/#dipfit-core-functions","title":"DIPFIT Core Functions","text":"<ul> <li><code>pop_dipfit_settings</code>: Initialize head model and coordinate system setup</li> <li><code>pop_dipfit_batch</code>: Electrode coregistration to head model</li> <li><code>pop_dipfit_gridsearch</code>: Grid search for initial dipole locations</li> <li><code>pop_dipfit_nonlinear</code>: Nonlinear optimization of dipole parameters</li> <li>Template integration: Standard BEM head models and electrode templates</li> </ul>"},{"location":"examples/source_localization_eeglab/#equivalent-dipole-modeling","title":"Equivalent Dipole Modeling","text":"<ul> <li>Component-based analysis: Single dipole per ICA component</li> <li>MNI coordinate system: Standardized brain space for group analysis</li> <li>Residual variance: Goodness-of-fit metric for dipole quality</li> <li>Spatial constraints: Dipoles constrained to physiologically plausible locations</li> </ul>"},{"location":"examples/source_localization_eeglab/#example-json-structure","title":"Example JSON Structure","text":"<p>The dipole optimization demonstrates EEGLAB's component-based approach:</p> <pre><code>{\n  \"stepId\": \"5\",\n  \"name\": \"Optimize Dipole Fits\",\n  \"description\": \"Nonlinear optimization of dipole locations for components with low residual variance.\",\n  \"software\": {\n    \"name\": \"EEGLAB DIPFIT\",\n    \"version\": \"4.3\",\n    \"functionCall\": \"pop_dipfit_nonlinear(EEG, 'component', find([EEG.dipfit.model.rv] &lt; 0.15))\"\n  },\n  \"parameters\": {\n    \"component_selection\": \"rv &lt; 0.15\",\n    \"threshold\": 0.15,\n    \"optimization_method\": \"nonlinear\",\n    \"mni_coord\": true\n  },\n  \"qualityMetrics\": {\n    \"dipoles_optimized\": 18,\n    \"mean_residual_variance\": 0.087,\n    \"components_localized\": \"72% (18/25)\"\n  }\n}\n</code></pre>"},{"location":"examples/source_localization_eeglab/#dipfit-settings-configuration","title":"DIPFIT Settings Configuration","text":"<p>The head model initialization shows EEGLAB's template system:</p> <pre><code>{\n  \"stepId\": \"2\", \n  \"name\": \"Initialize DIPFIT\",\n  \"description\": \"Setup DIPFIT with standard BEM head model and coordinate system.\",\n  \"software\": {\n    \"name\": \"EEGLAB DIPFIT\",\n    \"version\": \"4.3\",\n    \"functionCall\": \"pop_dipfit_settings(EEG, 'hdmfile', 'standard_BEM.mat', 'coordformat', 'MNI')\"\n  },\n  \"parameters\": {\n    \"hdmfile\": \"standard_BEM.mat\",\n    \"coordformat\": \"MNI\",\n    \"mrifile\": \"avg152t1.mat\",\n    \"chanfile\": \"standard_1005.elc\"\n  }\n}\n</code></pre>"},{"location":"examples/source_localization_eeglab/#dipfit-source-localization-features","title":"DIPFIT Source Localization Features","text":""},{"location":"examples/source_localization_eeglab/#equivalent-dipole-analysis","title":"Equivalent Dipole Analysis","text":"<ul> <li>Single dipole assumption: One dipole per independent component</li> <li>Grid search initialization: Systematic search across brain volume</li> <li>Nonlinear optimization: Refinement of dipole position and orientation</li> <li>Quality assessment: Residual variance and explained variance metrics</li> </ul>"},{"location":"examples/source_localization_eeglab/#coordinate-system-integration","title":"Coordinate System Integration","text":"<ul> <li>MNI standardization: Results in standard brain coordinate space</li> <li>Template head models: Standard BEM for group-level analysis</li> <li>Electrode coregistration: Proper spatial alignment procedures</li> <li>Brain visualization: Integration with EEGLAB plotting functions</li> </ul>"},{"location":"examples/source_localization_eeglab/#quality-control-features","title":"Quality Control Features","text":"<ul> <li>Residual variance thresholds: Automated dipole acceptance criteria</li> <li>Component selection: Based on ICA decomposition quality</li> <li>Spatial validation: Dipoles constrained to gray matter regions</li> <li>Outlier detection: Identification of poorly fitted dipoles</li> </ul>"},{"location":"examples/source_localization_eeglab/#eeglab-vs-mne-python-comparison","title":"EEGLAB vs MNE-Python Comparison","text":"Aspect EEGLAB Version MNE-Python Version Modeling Approach Equivalent dipole (single) Distributed sources (thousands) Analysis Target ICA components Sensor-level data Head Model Standard BEM templates Custom BEM/FreeSurfer Coordinate System MNI space Individual/fsaverage Software Plugin DIPFIT plugin Core MNE functions Computational Cost Low (few dipoles) High (dense source space)"},{"location":"examples/source_localization_eeglab/#dipfit-specific-workflow","title":"DIPFIT-Specific Workflow","text":""},{"location":"examples/source_localization_eeglab/#ica-component-integration","title":"ICA Component Integration","text":"<p>DIPFIT analysis leverages ICA decomposition results: 1. Component selection: Based on ICLabel classification and quality 2. Spatial patterns: Component topographies used for source fitting 3. Time courses: Independent component activations preserved 4. Quality metrics: Component-specific residual variance</p>"},{"location":"examples/source_localization_eeglab/#template-based-analysis","title":"Template-Based Analysis","text":"<ul> <li>Standard head models: Facilitates group-level comparisons</li> <li>Electrode templates: Standard 10-20 and high-density layouts</li> <li>MNI brain space: Enables meta-analysis and literature comparison</li> <li>Automated workflows: Batch processing for multiple datasets</li> </ul>"},{"location":"examples/source_localization_eeglab/#interactive-analysis-features","title":"Interactive Analysis Features","text":"<ul> <li>GUI integration: Pop-up functions for parameter adjustment</li> <li>Visual feedback: Real-time dipole visualization during fitting</li> <li>Manual refinement: Interactive dipole position adjustment</li> <li>Quality inspection: Visual assessment of dipole fits</li> </ul>"},{"location":"examples/source_localization_eeglab/#usage-notes","title":"Usage Notes","text":"<p>This example demonstrates: - Component-based source localization using equivalent dipole modeling - DIPFIT workflow documentation with complete parameter preservation - Quality control integration for automated dipole validation - Template-based analysis for standardized group studies - ICA integration leveraging independent component analysis</p> <p>The DIPFIT approach provides an interpretable source localization method particularly well-suited for ICA components, offering complementary insights to distributed source modeling approaches. The equivalent dipole assumption enables straightforward interpretation while maintaining computational efficiency for routine analysis workflows. </p>"},{"location":"examples/source_localization_mne/","title":"Example: Source Localization (MNE-Python)","text":"<p>This page explains the <code>source_localization_pipeline_mne.signalJourney.json</code> example file, documenting a source localization workflow using distributed source modeling with MNE-Python.</p>"},{"location":"examples/source_localization_mne/#pipeline-overview","title":"Pipeline Overview","text":"<p>This MNE-Python pipeline demonstrates brain source localization using forward/inverse modeling: - Load preprocessed evoked data from averaged epochs - Setup source space using FreeSurfer fsaverage template - Compute BEM solution for head modeling - Create forward solution (leadfield matrix) - Compute noise covariance from baseline periods - Create inverse operator for source estimation - Apply dSPM inverse solution to estimate source time courses</p>"},{"location":"examples/source_localization_mne/#pipeline-flowchart","title":"Pipeline Flowchart","text":"<pre><code>flowchart TD\n    A[Load Evoked Datamne.read_evokeds] --&gt; B[Setup Source Spacemne.setup_source_space]\n    B --&gt; C[Compute BEM Solutionmne.make_bem_solution]\n    C --&gt; D[Create Forward Solutionmne.make_forward_solution]\n    D --&gt; E[Compute Covariancemne.compute_covariance]\n    E --&gt; F[Create Inverse Operatormne.minimum_norm.make_inverse_operator]\n    F --&gt; G[Apply dSPM Solutionmne.minimum_norm.apply_inverse]\n\n    %% Input files\n    H[\"\ud83d\udcc1 sub-01_task-rest_ave.fifAveraged evoked data\"] --&gt; A\n    I[\"\ud83d\udcc1 sub-01_task-rest_epo.fifEpochs for covariance\"] --&gt; E\n    J[\"\ud83d\udcc1 fsaverageTemplate brain anatomy\"] --&gt; B\n    K[\"\ud83d\udcc1 BEM surfacesHead model\"] --&gt; C\n\n    %% Inline data\n    B --&gt; V1[\"\ud83d\udcca Source Spacingoct6 (4098 vertices)\"]\n    C --&gt; V2[\"\ud83d\udcca Conductivity[0.3, 0.006, 0.3] S/m\"]\n    F --&gt; V3[\"\ud83d\udcca SNR Parameters\u03bb\u00b2 = 1/SNR\u00b2\"]\n\n    %% Final outputs\n    G --&gt; L[\"\ud83d\udcbe sub-01_task-rest_desc-dSPM_stc.h5Source time courses\"]\n    G --&gt; M[\"\ud83d\udcbe sub-01_task-rest_desc-sources_plot.pngBrain activation plot\"]\n\n    %% Quality metrics\n    D --&gt; Q1[\"\ud83d\udcc8 Forward channels: 64Source points: 4098\"]\n    G --&gt; Q2[\"\ud83d\udcc8 Peak activation: 15.2 dSPMLocation: Left STG\"]\n\n    %% Styling\n    classDef processStep fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef inputFile fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    classDef outputFile fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px\n    classDef inlineData fill:#f3e5f5,stroke:#4a148c,stroke-width:1px\n    classDef qualityMetric fill:#f9f9f9,stroke:#666,stroke-width:1px\n\n    class A,B,C,D,E,F,G processStep\n    class H,I,J,K inputFile\n    class L,M outputFile\n    class V1,V2,V3 inlineData\n    class Q1,Q2 qualityMetric\n</code></pre>"},{"location":"examples/source_localization_mne/#key-mne-python-features-demonstrated","title":"Key MNE-Python Features Demonstrated","text":""},{"location":"examples/source_localization_mne/#source-localization-functions","title":"Source Localization Functions","text":"<ul> <li><code>mne.read_evokeds</code>: Load averaged evoked responses from FIF files</li> <li><code>mne.setup_source_space</code>: Create source space from FreeSurfer anatomy</li> <li><code>mne.make_bem_solution</code>: Compute boundary element model head solution</li> <li><code>mne.make_forward_solution</code>: Calculate leadfield matrix</li> <li><code>mne.minimum_norm.make_inverse_operator</code>: Create inverse solution operator</li> <li><code>mne.minimum_norm.apply_inverse</code>: Apply dSPM source estimation</li> </ul>"},{"location":"examples/source_localization_mne/#advanced-source-modeling","title":"Advanced Source Modeling","text":"<ul> <li>Source space: Cortical surface-based source model with configurable resolution</li> <li>Forward modeling: Realistic head geometry using boundary element method</li> <li>Inverse methods: dSPM, sLORETA, eLORETA distributed source solutions</li> <li>Regularization: SNR-based regularization parameter selection</li> </ul>"},{"location":"examples/source_localization_mne/#example-json-structure","title":"Example JSON Structure","text":"<p>The forward solution computation demonstrates complex dependency management:</p> <pre><code>{\n  \"stepId\": \"4\",\n  \"name\": \"Create Forward Solution\",\n  \"description\": \"Compute leadfield matrix relating sources to sensors.\",\n  \"software\": {\n    \"name\": \"MNE-Python\",\n    \"version\": \"1.6.1\",\n    \"functionCall\": \"mne.make_forward_solution(evoked.info, trans, src, bem_sol, eeg=True, mindist=5.0)\"\n  },\n  \"parameters\": {\n    \"trans\": \"fsaverage\",\n    \"eeg\": true,\n    \"meg\": false,\n    \"mindist\": 5.0,\n    \"n_jobs\": 1,\n    \"verbose\": false\n  },\n  \"dependsOn\": [\"1\", \"2\", \"3\"]\n}\n</code></pre>"},{"location":"examples/source_localization_mne/#inverse-solution-application","title":"Inverse Solution Application","text":"<p>The dSPM application step shows advanced parameter control:</p> <pre><code>{\n  \"stepId\": \"7\",\n  \"name\": \"Apply dSPM Solution\",\n  \"description\": \"Estimate source time courses using dynamic Statistical Parametric Mapping.\",\n  \"software\": {\n    \"name\": \"MNE-Python\", \n    \"version\": \"1.6.1\",\n    \"functionCall\": \"mne.minimum_norm.apply_inverse(evoked, inverse_operator, lambda2, method='dSPM')\"\n  },\n  \"parameters\": {\n    \"method\": \"dSPM\",\n    \"lambda2\": 0.111111,\n    \"pick_ori\": \"normal\",\n    \"verbose\": false\n  },\n  \"qualityMetrics\": {\n    \"peakActivation\": 15.2,\n    \"peakLocation\": \"Left STG\",\n    \"snrEstimate\": 3.0\n  }\n}\n</code></pre>"},{"location":"examples/source_localization_mne/#source-localization-features","title":"Source Localization Features","text":""},{"location":"examples/source_localization_mne/#anatomical-integration","title":"Anatomical Integration","text":"<ul> <li>FreeSurfer compatibility: Seamless integration with FreeSurfer anatomy</li> <li>Template brains: Support for fsaverage and individual anatomies</li> <li>Source space options: Surface-based, volumetric, or mixed source models</li> <li>Coordinate systems: MNI, Talairach, and individual head coordinates</li> </ul>"},{"location":"examples/source_localization_mne/#forward-modeling-accuracy","title":"Forward Modeling Accuracy","text":"<ul> <li>BEM head model: Multi-layer realistic head geometry</li> <li>Conductor specification: Brain, skull, and scalp conductivity values</li> <li>Sensor modeling: Accurate EEG and MEG sensor positions</li> <li>Quality assessment: Forward solution validation metrics</li> </ul>"},{"location":"examples/source_localization_mne/#inverse-solution-methods","title":"Inverse Solution Methods","text":"<ul> <li>Minimum norm estimation: L2-regularized linear inverse solutions</li> <li>dSPM normalization: Dynamic statistical parametric mapping</li> <li>sLORETA: Standardized low resolution electromagnetic tomography</li> <li>eLORETA: Exact low resolution electromagnetic tomography</li> </ul>"},{"location":"examples/source_localization_mne/#mne-python-vs-eeglab-comparison","title":"MNE-Python vs EEGLAB Comparison","text":"Aspect MNE-Python Version EEGLAB Version Anatomy FreeSurfer integration DIPFIT equivalent dipoles Forward Model BEM head model 3-sphere or BEM Inverse Method dSPM, sLORETA, eLORETA LORETA, sLORETA Source Space Cortical surface-based Volumetric grid Visualization 3D brain rendering 2D slice display File Format HDF5, STC files .mat files"},{"location":"examples/source_localization_mne/#usage-notes","title":"Usage Notes","text":"<p>This example demonstrates: - Comprehensive source localization with realistic head modeling - Multi-step dependency management for complex workflows - External resource integration (FreeSurfer, BEM surfaces) - Quality metrics for source localization validation - Advanced parameter documentation for reproducible inverse solutions</p> <p>The pipeline showcases MNE-Python's sophisticated source localization capabilities while maintaining complete parameter transparency for reproducible brain source estimation. The integration of anatomical templates and realistic head modeling provides state-of-the-art source localization accuracy. </p>"},{"location":"examples/time_frequency_eeglab/","title":"Example: Time-Frequency Analysis (EEGLAB)","text":"<p>This page explains the <code>time_frequency_analysis_pipeline_eeglab.signalJourney.json</code> example file, which documents time-frequency decomposition using EEGLAB's <code>timef</code> function.</p>"},{"location":"examples/time_frequency_eeglab/#pipeline-overview","title":"Pipeline Overview","text":"<p>This EEGLAB pipeline demonstrates time-frequency analysis using EEGLAB's wavelet decomposition capabilities: - Load cleaned data from ICA decomposition pipeline - Extract event-related epochs for time-frequency analysis - Compute time-frequency decomposition using <code>timef</code> function - Apply baseline correction and save results - Generate visualization plots for ERSP and ITC</p>"},{"location":"examples/time_frequency_eeglab/#pipeline-flowchart","title":"Pipeline Flowchart","text":"<pre><code>flowchart TD\n    A[Load Cleaned Datapop_loadset] --&gt; B[Extract Epochspop_epoch]\n    B --&gt; C[Compute ERSPtimef]\n    C --&gt; D[Compute ITCnewtimef]\n    D --&gt; E[Save Resultspop_saveset]\n\n    %% Input file\n    F[\"\ud83d\udcc1 sub-01_task-rest_desc-cleaned_eeg.setFrom: ICA Decomposition Pipeline\"] --&gt; A\n\n    %% Inline data\n    C --&gt; V1[\"\ud83d\udcca Frequencies[3:0.5:30] Hz\"]\n    C --&gt; V2[\"\ud83d\udcca Cycles[3 0.5] wavelet\"]\n    C --&gt; V3[\"\ud83d\udcca Baseline[-200 0] ms\"]\n\n    %% Final outputs\n    E --&gt; G[\"\ud83d\udcbe sub-01_task-rest_desc-ersp_eeg.setERSP results\"]\n    E --&gt; H[\"\ud83d\udcbe sub-01_task-rest_desc-itc_eeg.setITC results\"]\n    D --&gt; I[\"\ud83d\udcbe sub-01_task-rest_desc-tfr_plot.figEEGLAB figure\"]\n\n    %% Quality metrics\n    C --&gt; Q1[\"\ud83d\udcc8 Frequency bins: 55Time points: 200\"]\n    D --&gt; Q2[\"\ud83d\udcc8 Baseline mode: RelativeSignificance: p&lt;0.01\"]\n\n    %% Styling\n    classDef processStep fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef inputFile fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    classDef outputFile fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px\n    classDef inlineData fill:#f3e5f5,stroke:#4a148c,stroke-width:1px\n    classDef qualityMetric fill:#f9f9f9,stroke:#666,stroke-width:1px\n\n    class A,B,C,D,E processStep\n    class F inputFile\n    class G,H,I outputFile\n    class V1,V2,V3 inlineData\n    class Q1,Q2 qualityMetric\n</code></pre>"},{"location":"examples/time_frequency_eeglab/#key-eeglab-features-demonstrated","title":"Key EEGLAB Features Demonstrated","text":""},{"location":"examples/time_frequency_eeglab/#eeglab-time-frequency-functions","title":"EEGLAB Time-Frequency Functions","text":"<ul> <li><code>pop_epoch</code>: Extract event-related epochs from continuous data</li> <li><code>timef</code>: Time-frequency decomposition with Morlet wavelets</li> <li><code>newtimef</code>: Enhanced time-frequency analysis with bootstrap statistics</li> <li><code>pop_saveset</code>: Save results in EEGLAB dataset format</li> </ul>"},{"location":"examples/time_frequency_eeglab/#eeglab-specific-parameters","title":"EEGLAB-Specific Parameters","text":"<ul> <li>Wavelet specification: EEGLAB's [cycles freqs] format</li> <li>Baseline correction: Built into <code>timef</code> function</li> <li>Statistical testing: Bootstrap significance testing</li> <li>Visualization: Automatic ERSP and ITC plotting</li> </ul>"},{"location":"examples/time_frequency_eeglab/#example-json-structure","title":"Example JSON Structure","text":"<p>The EEGLAB time-frequency computation demonstrates integrated analysis:</p> <pre><code>{\n  \"stepId\": \"3\",\n  \"name\": \"Compute ERSP\",\n  \"description\": \"Calculate event-related spectral perturbation using EEGLAB timef.\",\n  \"software\": {\n    \"name\": \"EEGLAB\", \n    \"version\": \"2023.1\",\n    \"functionCall\": \"timef(EEG.data, frames, tlimits, srate, cycles, 'baseline', [-200 0], 'plotitc', 'off')\"\n  },\n  \"parameters\": {\n    \"frames\": 1000,\n    \"tlimits\": [-500, 1000],\n    \"srate\": 500,\n    \"cycles\": [3, 0.5],\n    \"freqs\": [3, 30],\n    \"nfreqs\": 55,\n    \"baseline\": [-200, 0],\n    \"baselinetype\": \"relative\",\n    \"plotitc\": \"off\",\n    \"plotersp\": \"on\"\n  }\n}\n</code></pre>"},{"location":"examples/time_frequency_eeglab/#integrated-output-documentation","title":"Integrated Output Documentation","text":"<p>EEGLAB's <code>timef</code> produces multiple outputs simultaneously:</p> <pre><code>\"outputTargets\": [\n  {\n    \"targetType\": \"in-memory\",\n    \"format\": \"double\",\n    \"description\": \"ERSP matrix (freqs x times x trials).\",\n    \"variableName\": \"ersp\"\n  },\n  {\n    \"targetType\": \"in-memory\", \n    \"format\": \"double\",\n    \"description\": \"ITC matrix (freqs x times).\",\n    \"variableName\": \"itc\"\n  },\n  {\n    \"targetType\": \"in-memory\",\n    \"format\": \"double\",\n    \"description\": \"Power base matrix for baseline correction.\",\n    \"variableName\": \"powbase\"\n  }\n]\n</code></pre>"},{"location":"examples/time_frequency_eeglab/#eeglab-time-frequency-features","title":"EEGLAB Time-Frequency Features","text":""},{"location":"examples/time_frequency_eeglab/#wavelet-parameter-system","title":"Wavelet Parameter System","text":"<ul> <li>Cycles specification: <code>[min_cycles max_cycles]</code> format</li> <li>Automatic scaling: Cycles increase linearly with frequency</li> <li>Frequency resolution: Number of frequency bins automatically calculated</li> <li>Time resolution: Determined by cycle parameters and sampling rate</li> </ul>"},{"location":"examples/time_frequency_eeglab/#baseline-correction-options","title":"Baseline Correction Options","text":"<ul> <li>Relative: <code>(power - baseline) / baseline</code></li> <li>Absolute: <code>power - baseline</code></li> <li>Relchange: <code>(power - baseline) / baseline * 100</code></li> <li>Log: <code>10 * log10(power / baseline)</code></li> </ul>"},{"location":"examples/time_frequency_eeglab/#bootstrap-statistics","title":"Bootstrap Statistics","text":"<ul> <li>Significance testing: Built-in permutation testing</li> <li>Multiple comparison correction: False discovery rate (FDR)</li> <li>Confidence intervals: Bootstrap confidence bounds</li> <li>Alpha level: Customizable significance threshold</li> </ul>"},{"location":"examples/time_frequency_eeglab/#eeglab-vs-mne-python-comparison","title":"EEGLAB vs MNE-Python Comparison","text":"Aspect EEGLAB Version MNE-Python Version Function <code>timef</code>, <code>newtimef</code> <code>tfr_morlet</code> Output ERSP + ITC combined Power only (separate ITC) Baseline Built into function Separate <code>apply_baseline()</code> Statistics Bootstrap testing External stats required Visualization Automatic EEGLAB plots matplotlib customization File Format .set/.mat files HDF5/NPZ formats"},{"location":"examples/time_frequency_eeglab/#eeglab-specific-workflow","title":"EEGLAB-Specific Workflow","text":""},{"location":"examples/time_frequency_eeglab/#integrated-analysis-approach","title":"Integrated Analysis Approach","text":"<p>EEGLAB's <code>timef</code> provides: 1. Combined ERSP/ITC computation in single function call 2. Automatic baseline correction with multiple methods 3. Built-in statistical testing via bootstrap procedures 4. Immediate visualization with publication-ready plots</p>"},{"location":"examples/time_frequency_eeglab/#eeg-dataset-integration","title":"EEG Dataset Integration","text":"<ul> <li>Event information automatically extracted from EEG.event</li> <li>Channel locations used for topographic plotting</li> <li>Dataset history updated with analysis parameters</li> <li>STUDY compatibility for group-level analysis</li> </ul>"},{"location":"examples/time_frequency_eeglab/#usage-notes","title":"Usage Notes","text":"<p>This example demonstrates: - EEGLAB's integrated approach to time-frequency analysis - Automatic parameter optimization for wavelet decomposition - Built-in statistical testing for significance assessment - Multi-output handling with ERSP, ITC, and baseline data - EEGLAB visualization with publication-ready plots</p> <p>The pipeline showcases EEGLAB's comprehensive time-frequency analysis capabilities with emphasis on ease-of-use and integrated statistical testing while maintaining full parameter documentation for reproducibility. </p>"},{"location":"examples/time_frequency_mne/","title":"Example: Time-Frequency Analysis (MNE-Python)","text":"<p>This page explains the <code>time_frequency_analysis_pipeline_mne.signalJourney.json</code> example file, which documents a typical time-frequency analysis using MNE-Python.</p>"},{"location":"examples/time_frequency_mne/#pipeline-overview","title":"Pipeline Overview","text":"<p>This MNE-Python pipeline demonstrates time-frequency analysis using Morlet wavelets: - Load cleaned data from ICA decomposition pipeline - Extract event-related epochs - Compute time-frequency decomposition using <code>mne.time_frequency.tfr_morlet</code> - Apply baseline correction and save results - Generate visualization plots</p>"},{"location":"examples/time_frequency_mne/#pipeline-flowchart","title":"Pipeline Flowchart","text":"<pre><code>flowchart TD\n    A[Load Cleaned Datamne.io.read_raw_fif] --&gt; B[Extract Epochsmne.Epochs]\n    B --&gt; C[Compute TFRmne.time_frequency.tfr_morlet]\n    C --&gt; D[Apply Baseline Correctiontfr.apply_baseline]\n    D --&gt; E[Save TFR Resultstfr.save]\n\n    %% Input file\n    F[\"\ud83d\udcc1 sub-01_task-rest_desc-cleaned_eeg.fifFrom: ICA Decomposition Pipeline\"] --&gt; A\n\n    %% Inline data\n    C --&gt; V2[\"\ud83d\udcca Frequencies[4, 8, 13, 30] Hz\"]\n\n    %% Final outputs\n    E --&gt; G[\"\ud83d\udcbe sub-01_task-rest_desc-tfr_eeg.h5Time-frequency results\"]\n    D --&gt; H[\"\ud83d\udcbe sub-01_task-rest_desc-tfr_plot.pngVisualization\"]\n\n    %% Quality metrics\n    C --&gt; Q1[\"\ud83d\udcc8 Frequency range: 4-30 HzCycles: 2-15\"]\n    D --&gt; Q2[\"\ud83d\udcc8 Baseline: [-0.2, 0] sMode: percent\"]\n\n    %% Styling\n    classDef processStep fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef inputFile fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    classDef outputFile fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px\n    classDef inlineData fill:#f3e5f5,stroke:#4a148c,stroke-width:1px\n    classDef qualityMetric fill:#f9f9f9,stroke:#666,stroke-width:1px\n\n    class A,B,C,D,E processStep\n    class F inputFile\n    class G,H outputFile\n    class V2 inlineData\n    class Q1,Q2 qualityMetric\n</code></pre>"},{"location":"examples/time_frequency_mne/#key-mne-python-features-demonstrated","title":"Key MNE-Python Features Demonstrated","text":""},{"location":"examples/time_frequency_mne/#time-frequency-functions","title":"Time-Frequency Functions","text":"<ul> <li><code>mne.Epochs</code>: Event-related epoch extraction</li> <li><code>mne.time_frequency.tfr_morlet</code>: Morlet wavelet decomposition</li> <li><code>tfr.apply_baseline</code>: Baseline correction for power changes</li> <li><code>tfr.save</code>: Save time-frequency results in HDF5 format</li> </ul>"},{"location":"examples/time_frequency_mne/#advanced-parameters","title":"Advanced Parameters","text":"<ul> <li>Wavelet parameters: Adaptive cycles for different frequencies</li> <li>Baseline correction: Percent change from pre-stimulus period</li> <li>Frequency selection: Logarithmically spaced frequencies</li> <li>Output formats: Both HDF5 data and PNG visualization</li> </ul>"},{"location":"examples/time_frequency_mne/#example-json-structure","title":"Example JSON Structure","text":"<p>The time-frequency computation step demonstrates complex parameter documentation:</p> <pre><code>{\n  \"stepId\": \"3\",\n  \"name\": \"Compute Time-Frequency Decomposition\",\n  \"description\": \"Calculate time-frequency representation using Morlet wavelets.\",\n  \"software\": {\n    \"name\": \"MNE-Python\",\n    \"version\": \"1.6.1\",\n    \"functionCall\": \"mne.time_frequency.tfr_morlet(epochs, freqs=freqs, n_cycles=n_cycles, return_itc=False)\"\n  },\n  \"parameters\": {\n    \"freqs\": [4, 6, 8, 10, 13, 17, 22, 30],\n    \"n_cycles\": [2, 3, 4, 5, 6.5, 8.5, 11, 15],\n    \"use_fft\": true,\n    \"return_itc\": false,\n    \"decim\": 1,\n    \"n_jobs\": 1\n  }\n}\n</code></pre>"},{"location":"examples/time_frequency_mne/#multi-output-documentation","title":"Multi-Output Documentation","text":"<p>Steps can produce multiple related outputs:</p> <pre><code>\"outputTargets\": [\n  {\n    \"targetType\": \"file\",\n    \"location\": \"./derivatives/signaljourney/sub-01/eeg/sub-01_task-rest_desc-tfr_eeg.h5\",\n    \"format\": \"HDF5\",\n    \"description\": \"Time-frequency power data.\"\n  },\n  {\n    \"targetType\": \"file\", \n    \"location\": \"./derivatives/signaljourney/sub-01/eeg/sub-01_task-rest_desc-tfr_plot.png\",\n    \"format\": \"PNG\",\n    \"description\": \"Time-frequency plot visualization.\"\n  }\n]\n</code></pre>"},{"location":"examples/time_frequency_mne/#time-frequency-analysis-features","title":"Time-Frequency Analysis Features","text":""},{"location":"examples/time_frequency_mne/#wavelet-parameter-selection","title":"Wavelet Parameter Selection","text":"<ul> <li>Frequency-dependent cycles: Lower frequencies use fewer cycles for better temporal resolution</li> <li>Higher frequencies: More cycles for better frequency resolution</li> <li>Adaptive approach: Balances time-frequency trade-off across spectrum</li> </ul>"},{"location":"examples/time_frequency_mne/#baseline-correction-methods","title":"Baseline Correction Methods","text":"<ul> <li>Percent change: <code>(power - baseline) / baseline * 100</code></li> <li>Ratio: <code>power / baseline</code></li> <li>Decibel: <code>10 * log10(power / baseline)</code></li> <li>Z-score: <code>(power - baseline_mean) / baseline_std</code></li> </ul>"},{"location":"examples/time_frequency_mne/#mne-python-vs-eeglab-comparison","title":"MNE-Python vs EEGLAB Comparison","text":"Aspect MNE-Python Version EEGLAB Version Method Morlet wavelets Morlet wavelets (timef) Baseline <code>apply_baseline()</code> Built into <code>timef</code> Output HDF5, NPZ formats MATLAB .mat files Visualization matplotlib plots EEGLAB plots Cycles Manual specification Automatic or manual"},{"location":"examples/time_frequency_mne/#usage-notes","title":"Usage Notes","text":"<p>This example demonstrates: - Time-frequency decomposition with optimal parameters - Baseline correction for interpretable results - Multi-format outputs for analysis and visualization - Pipeline integration building on previous processing steps - Quality documentation for reproducible analysis</p> <p>The pipeline showcases MNE-Python's comprehensive time-frequency analysis capabilities while maintaining full parameter transparency for reproducibility. </p>"},{"location":"examples/real_world/nemar_pipeline/","title":"Real-World Example: NEMAR EEG Processing Pipeline","text":"<p>This example demonstrates the complete NEMAR (EEGLAB-based) EEG processing pipeline documented in <code>nemar_pipeline.signalJourney.json</code>. This production pipeline processes OpenNeuro EEG datasets and showcases advanced signalJourney features including inline data preservation, multi-level quality metrics, and extension schema integration.</p>"},{"location":"examples/real_world/nemar_pipeline/#pipeline-architecture","title":"Pipeline Architecture","text":"<p>The NEMAR pipeline implements a 12-step EEG preprocessing workflow organized into four main processing stages:</p> <ol> <li>Data Import &amp; Validation (Steps 1-3): BIDS import, status verification, channel location checks</li> <li>Channel Selection &amp; Preprocessing (Steps 4-6): Non-EEG removal, DC offset correction, high-pass filtering  </li> <li>Automated Artifact Rejection (Step 7): Clean Raw Data algorithm, ICA decomposition (Step 8), ICLabel classification (Step 9)</li> <li>Quality Assessment &amp; Export (Steps 10-12): Data quality metrics, power spectral analysis, dataset export</li> </ol>"},{"location":"examples/real_world/nemar_pipeline/#processing-flow","title":"Processing Flow","text":"<p>The diagram below shows the complete 12-step NEMAR pipeline workflow with color-coded elements:</p> <pre><code>flowchart TD\n    A[\"1.Import BIDS Dataset'pop_importbids'\"] --&gt; B[\"2.Check Import Status\"]\n    B --&gt; C[\"3.Check Channel Locations\"]\n    C --&gt; D[\"4.Remove Non-EEG Channels'pop_select'\"]\n    D --&gt; E[\"5.Remove DC Offset'pop_rmbase'\"]\n    E --&gt; F[\"6.High-pass Filter'pop_eegfiltnew'\"]\n    F --&gt; G[\"7.Clean Raw Data'clean_rawdata'\"]\n    G --&gt; H[\"8.Run ICA Decomposition'runica'\"]\n    H --&gt; I[\"9.ICLabel Classification'ICLabel'\"]\n    I --&gt; J[\"10.Data Quality AssessmentQuality metrics\"]\n    J --&gt; K[\"11.Power Spectral AnalysisLine noise assessment\"]\n    K --&gt; L[\"12.Save Processed DatasetFinal output\"]\n\n    %% Inline Data Outputs\n    D --&gt; D1[\"\ud83d\udcca removed_channelsChannel list\"]\n    G --&gt; G1[\"\ud83d\udcca clean_sample_maskTime samples\"]\n    G --&gt; G2[\"\ud83d\udcca clean_channel_maskChannel mask\"]\n    G --&gt; G3[\"\ud83d\udcca rejected_channelsRejected list\"]\n    H --&gt; H1[\"\ud83d\udcca ica_weightsWeight matrix\"]\n    H --&gt; H2[\"\ud83d\udcca ica_sphereSphere matrix\"]\n    H --&gt; H3[\"\ud83d\udcca ica_componentsActivations\"]\n    I --&gt; I1[\"\ud83d\udcca ic_classificationProbabilities\"]\n    I --&gt; I2[\"\ud83d\udcca flagged_componentsArtifact indices\"]\n    J --&gt; J1[\"\ud83d\udcca data_quality_metricsQC measures\"]\n    K --&gt; K1[\"\ud83d\udcca power_spectrumPSD data\"]\n    K --&gt; K2[\"\ud83d\udcca line_noise_assessmentNoise levels\"]\n\n    %% Saved File Outputs\n    J --&gt; J2[\"\ud83d\udcbe dataqual.jsonQuality report\"]\n    L --&gt; L1[\"\ud83d\udcbe processed_eeg.setFinal dataset\"]\n    L --&gt; L2[\"\ud83d\udcbe pipeline_status.csvStep status\"]\n\n    %% Styling\n    classDef processStep fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef inlineData fill:#f3e5f5,stroke:#4a148c,stroke-width:1px\n    classDef savedFile fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px\n\n    class A,B,C,D,E,F,G,H,I,J,K,L processStep\n    class D1,G1,G2,G3,H1,H2,H3,I1,I2,J1,K1,K2 inlineData\n    class J2,L1,L2 savedFile\n</code></pre> <p>Legend: - \ud83d\udd35 Light Blue: Processing steps (EEGLAB/MATLAB functions) - \ud83d\udfe3 Purple: Inline data (small parameters/values preserved in JSON) - \ud83d\udfe2 Green: Saved files (datasets and reports)</p> <p>Note: Inmemory processing objects (Raw EEG structures, filtered data) are not shown as they represent the default temporary data flow between steps.</p>"},{"location":"examples/real_world/nemar_pipeline/#advanced-features","title":"Advanced Features","text":""},{"location":"examples/real_world/nemar_pipeline/#inline-data-preservation","title":"Inline Data Preservation","text":"<p>Critical intermediate results are preserved using <code>inlineData</code> targets, enabling post-hoc analysis and reproducibility. Key examples include ICA decomposition matrices, component classifications, and quality control masks:</p> <pre><code>{\n  \"targetType\": \"inlineData\",\n  \"name\": \"ica_weights\",\n  \"data\": \"{{ica_weights_matrix}}\",\n  \"formatDescription\": \"Matrix of ICA unmixing weights [n_components x n_channels]\",\n  \"description\": \"ICA unmixing weight matrix\"\n}\n</code></pre>"},{"location":"examples/real_world/nemar_pipeline/#multi-level-quality-assessment","title":"Multi-Level Quality Assessment","text":"<p>Quality metrics are computed at both step-level and pipeline-level, providing comprehensive quality control:</p> <p>Step-level metrics (e.g., Step 7 clean_rawdata): <pre><code>\"qualityMetrics\": {\n  \"percentDataRetained\": \"{{percent_clean_data}}\",\n  \"percentChannelsRetained\": \"{{percent_clean_channels}}\",\n  \"channelsRejected\": \"{{num_rejected_channels}}\"\n}\n</code></pre></p> <p>Pipeline-level summary: <pre><code>\"summaryMetrics\": {\n  \"pipelineCompleted\": true,\n  \"totalProcessingSteps\": 12,\n  \"overallDataQuality\": {\n    \"goodDataPercent\": \"{{overall_good_data_percent}}\",\n    \"goodChannelsPercent\": \"{{overall_good_channels_percent}}\",\n    \"goodICAPercent\": \"{{overall_good_ica_percent}}\"\n  }\n}\n</code></pre></p>"},{"location":"examples/real_world/nemar_pipeline/#extension-schema-integration","title":"Extension Schema Integration","text":"<p>Domain-specific metadata is captured using the NEMAR extension schema:</p> <pre><code>\"extensions\": {\n  \"nemar\": {\n    \"dataset_id\": \"{{openneuro_dataset_id}}\",\n    \"processing_cluster\": \"SDSC Expanse\",\n    \"eeglab_plugins\": [\"clean_rawdata\", \"ICLabel\", \"AMICA\", \"firfilt\"],\n    \"custom_code_applied\": \"{{custom_dataset_code}}\",\n    \"batch_processing\": true\n  }\n}\n</code></pre>"},{"location":"examples/real_world/nemar_pipeline/#conditional-algorithm-selection","title":"Conditional Algorithm Selection","text":"<p>Step 8 demonstrates algorithm selection logic with complete parameter documentation for both AMICA and runica ICA methods:</p> <pre><code>{\n  \"stepId\": \"8\",\n  \"name\": \"Run ICA Decomposition\",\n  \"description\": \"Perform Independent Component Analysis using either AMICA (if &gt;=5 channels) or extended Infomax ICA\",\n  \"software\": {\n    \"name\": \"AMICA/EEGLAB\",\n    \"version\": \"1.7/2023.1\", \n    \"functionCall\": \"runamica17_nsg(EEG, 'batch', 1) OR pop_runica(EEG, 'icatype', 'runica', 'extended', 1)\"\n  },\n  \"parameters\": {\n    \"method\": \"{{ica_method}}\",\n    \"amica_options\": {\"batch\": 1},\n    \"runica_options\": {\n      \"icatype\": \"runica\",\n      \"concatcond\": \"on\", \n      \"extended\": 1,\n      \"lrate\": 1e-5,\n      \"maxsteps\": 2000\n    }\n  }\n}\n</code></pre>"},{"location":"examples/real_world/nemar_pipeline/#template-variables-and-batch-processing","title":"Template Variables and Batch Processing","text":"<p>Template variables (e.g., <code>{{subject}}</code>, <code>{{session}}</code>, <code>{{openneuro_dataset_id}}</code>) enable automated batch processing while maintaining complete parameter documentation. This approach supports systematic processing of multiple datasets with consistent methodology.</p>"},{"location":"examples/real_world/nemar_pipeline/#research-applications","title":"Research Applications","text":"<p>This documentation format enables:</p> <ul> <li>Exact reproduction through complete parameter and dependency documentation</li> <li>Quality assessment via comprehensive multi-level metrics</li> <li>Method comparison with complete parameter sets and summary metrics</li> <li>Regulatory compliance through full audit trails of the processing steps and parameters</li> <li>Storage efficiency through inline data preservation of key intermediate results, eliminating the need to store both raw and processed datasets</li> </ul>"},{"location":"examples/real_world/nemar_pipeline/#references","title":"References","text":"<ul> <li>NEMAR Pipeline Repository</li> <li>EEGLAB</li> <li>Clean Raw Data Plugin</li> <li>ICLabel Plugin</li> </ul> <p>This NEMAR example demonstrates the full potential of signalJourney for documenting complex, production-grade signal processing workflows with complete transparency and reproducibility. </p>"},{"location":"guides/","title":"User Guides Index","text":"<p>Detailed guides for the signalJourney tools. Choose one of the topics on the left to get started.</p>"},{"location":"guides/tools_matlab/","title":"User Guide: MATLAB Tools","text":"<p>This guide provides comprehensive details on using the signalJourney MATLAB tools for reading, writing, and performing basic validation on signalJourney JSON files.</p>"},{"location":"guides/tools_matlab/#setup","title":"Setup","text":"<ol> <li>Ensure you have MATLAB R2019b or newer installed.</li> <li>Add the directory containing the MATLAB tools (<code>scripts/matlab</code> in the repository) to your MATLAB path:     <pre><code>addpath('path/to/signalJourney/scripts/matlab'); \nsavepath; % Optional: Save the path for future sessions\n</code></pre></li> </ol>"},{"location":"guides/tools_matlab/#core-functions","title":"Core Functions","text":""},{"location":"guides/tools_matlab/#readsignaljourneyfilename","title":"<code>readSignalJourney(filename)</code>","text":"<p>Reads a signalJourney JSON file and converts it into a MATLAB structure.</p> <p>Syntax: <pre><code>journeyData = readSignalJourney(filename)\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>filename</code> (char vector or string): Path to the <code>*.signalJourney.json</code> file.</li> </ul> <p>Returns:</p> <ul> <li><code>journeyData</code> (struct): A MATLAB structure representing the content of the JSON file. Field names in the structure correspond to the keys in the JSON object.</li> </ul> <p>Error Handling:</p> <ul> <li>Throws an error if the file does not exist (<code>readSignalJourney:FileDoesNotExist</code>).</li> <li>Throws an error if the file cannot be read (<code>readSignalJourney:fileReadError</code>).</li> <li>Throws an error if the file contains invalid JSON (<code>readSignalJourney:jsonDecodeError</code>).</li> </ul> <p>Example: <pre><code>fname = '../../schema/examples/basic_preprocessing_pipeline_mne.signalJourney.json';\ntry\n    data = readSignalJourney(fname);\n    disp(data.pipelineInfo);\ncatch ME\n    disp(ME.getReport());\nend\n</code></pre></p>"},{"location":"guides/tools_matlab/#writesignaljourneydata-filename","title":"<code>writeSignalJourney(data, filename)</code>","text":"<p>Writes a MATLAB structure (representing signalJourney data) to a JSON file.</p> <p>Syntax: <pre><code>writeSignalJourney(data, filename)\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>data</code> (struct): A MATLAB structure containing the signalJourney data.</li> <li><code>filename</code> (char vector or string): Path to the output JSON file. The file will be overwritten if it exists.</li> </ul> <p>Behavior:</p> <ul> <li>Performs a basic check for essential top-level fields (<code>sj_version</code>, <code>schema_version</code>, etc.) and issues a warning (<code>writeSignalJourney:FormatWarning</code>) if they are missing.</li> <li>Encodes the MATLAB structure into a JSON string using <code>jsonencode</code> with the <code>PrettyPrint</code> option enabled for human readability.</li> <li>Writes the JSON string to the specified file using UTF-8 encoding.</li> </ul> <p>Error Handling:</p> <ul> <li>Throws an error if the input <code>data</code> is not a struct.</li> <li>Throws an error if the structure cannot be encoded to JSON (<code>writeSignalJourney:jsonEncodeError</code>).</li> <li>Throws an error if the output file cannot be opened (<code>writeSignalJourney:fileOpenError</code>) or written to (<code>writeSignalJourney:fileWriteError</code>).</li> </ul> <p>Example: <pre><code>% Assume 'modifiedData' is a struct loaded and modified\noutput_fname = 'my_output.signalJourney.json';\ntry\n    writeSignalJourney(modifiedData, output_fname);\n    disp(['File written to: ', output_fname]);\ncatch ME\n    disp(ME.getReport());\nend\n</code></pre></p>"},{"location":"guides/tools_matlab/#validatesignaljourneydata","title":"<code>validateSignalJourney(data)</code>","text":"<p>Performs basic validation checks on a MATLAB structure to see if it loosely conforms to the signalJourney format. Note: This is not a full JSON schema validation.</p> <p>Syntax: <pre><code>[isValid, messages] = validateSignalJourney(data)\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>data</code> (struct): The MATLAB structure to validate.</li> </ul> <p>Returns:</p> <ul> <li><code>isValid</code> (logical): <code>true</code> if basic checks pass, <code>false</code> otherwise.</li> <li><code>messages</code> (cell array of strings): Contains error or warning messages if validation issues are found. Empty if <code>isValid</code> is <code>true</code>.</li> </ul> <p>Checks Performed:</p> <ul> <li>Verifies the input is a single struct.</li> <li>Checks for the presence of required top-level fields (<code>sj_version</code>, <code>schema_version</code>, <code>description</code>, <code>pipelineInfo</code>, <code>processingSteps</code>).</li> <li>Checks the basic data types of required fields (e.g., <code>sj_version</code> is char, <code>pipelineInfo</code> is struct, <code>processingSteps</code> is cell or struct array).</li> <li>Checks semantic version format for version fields.</li> <li>Checks types of optional top-level fields if they exist (<code>summaryMetrics</code>, <code>extensions</code>, <code>versionHistory</code>) and issues warnings if incorrect.</li> <li>Performs basic structural checks on <code>processingSteps</code> array elements (e.g., checks for required fields like <code>stepId</code>, <code>name</code>).</li> </ul> <p>Example: <pre><code>% Assume 'journeyData' is loaded\n[isValid, validationMsgs] = validateSignalJourney(journeyData);\nif ~isValid\n    disp('Basic validation found issues:');\n    disp(strjoin(validationMsgs, '\\n'));\nelse\n    disp('Basic structure appears valid.');\nend\n</code></pre></p>"},{"location":"guides/tools_matlab/#convertsignaljourneyversiondata-targetversion","title":"<code>convertSignalJourneyVersion(data, targetVersion)</code>","text":"<p>(Placeholder) This function is intended for future implementation to convert a signalJourney MATLAB structure from its current <code>sj_version</code> to a specified <code>targetVersion</code>.</p> <p>Syntax (Intended): <pre><code>convertedData = convertSignalJourneyVersion(data, targetVersion)\n</code></pre></p> <p>Current Behavior:</p> <ul> <li>Checks if <code>data.sj_version</code> exists.</li> <li>If <code>currentVersion</code> matches <code>targetVersion</code>, issues a warning and returns the original data.</li> <li>Otherwise, throws a <code>convertSignalJourneyVersion:NotImplemented</code> error.</li> </ul>"},{"location":"guides/tools_matlab/#limitations","title":"Limitations","text":"<ul> <li>Validation: <code>validateSignalJourney</code> performs only basic structural and type checks. It does not fully validate against the official JSON schema. For rigorous validation, use the Python <code>signaljourney-validator</code> library or CLI tool.</li> <li>Version Conversion: Not yet implemented.</li> </ul> <p>(Content to be added) </p>"},{"location":"guides/validator_cli/","title":"User Guide: CLI Validator Tool (<code>signaljourney-validate</code>)","text":"<p>This guide provides comprehensive details on using the <code>signaljourney-validate</code> command-line interface (CLI) to validate signalJourney JSON files.</p>"},{"location":"guides/validator_cli/#installation","title":"Installation","text":"<p>The CLI tool is installed as part of the <code>signaljourney-validator</code> Python package:</p> <pre><code>pip install signaljourney-validator\n</code></pre> <p>This makes the <code>signaljourney-validate</code> command available in your terminal.</p>"},{"location":"guides/validator_cli/#basic-usage","title":"Basic Usage","text":"<p>The primary command is <code>validate</code>. It takes the path to a signalJourney JSON file or a directory containing such files as its main argument.</p> <pre><code>signaljourney-validate [OPTIONS] PATH\n</code></pre> <ul> <li><code>PATH</code>: Path to a single <code>*_signalJourney.json</code> file or a directory.</li> </ul> <p>Examples:</p> <ul> <li> <p>Validate a single file: <pre><code>signaljourney-validate path/to/my_pipeline.signalJourney.json\n</code></pre>     Output on success:     <pre><code>Validating: path/to/my_pipeline.signalJourney.json ... OK\n</code></pre>     Output on failure:     <pre><code>Validating: path/to/my_pipeline.signalJourney.json ... FAILED\n  - Error at 'pipelineInfo/version': '1.0' is not a 'string' -- Suggestion: Change value type from 'float' to 'string'.\n  - Error at 'processingSteps/0/stepId': Required property 'stepId' is missing. -- Suggestion: Ensure required property or properties ('stepId') are present.\n</code></pre></p> </li> <li> <p>Validate all <code>*_signalJourney.json</code> files in a directory (non-recursive): <pre><code>signaljourney-validate path/to/my_pipelines/\n</code></pre>     Output:     <pre><code>Scanning directory: path/to/my_pipelines/\nValidating: path/to/my_pipelines/pipeline_a.signalJourney.json ... OK\nValidating: path/to/my_pipelines/pipeline_b.signalJourney.json ... FAILED\n  - Error at ...\n</code></pre></p> </li> <li> <p>Validate recursively through subdirectories: <pre><code>signaljourney-validate -r path/to/project/\n# OR\nsignaljourney-validate --recursive path/to/project/\n</code></pre>     Output:     <pre><code>Scanning directory: path/to/project/ recursively\nValidating: path/to/project/pipeline1.signalJourney.json ... OK\nValidating: path/to/project/derivatives/pipeline2.signalJourney.json ... OK\n</code></pre></p> </li> <li> <p>Validate with specific schema version (overrides auto-detection): <pre><code>signaljourney-validate --schema-version 0.1.0 my_file.signalJourney.json\n</code></pre>     Output:     <pre><code>Validating: my_file.signalJourney.json ... OK\n</code></pre>     This forces validation against the 0.1.0 schema regardless of the <code>schema_version</code> field in the file.</p> </li> </ul>"},{"location":"guides/validator_cli/#options","title":"Options","text":"<ul> <li><code>-s, --schema PATH</code>: Validate against a custom JSON schema file instead of using version-based schema selection.     <pre><code>signaljourney-validate -s path/to/custom_schema.json my_file.signalJourney.json\n</code></pre></li> <li><code>--schema-version VERSION</code>: Validate using a specific schema version (e.g., '0.1.0') instead of auto-detecting from the file's <code>schema_version</code> field.     <pre><code>signaljourney-validate --schema-version 0.1.0 my_file.signalJourney.json\n</code></pre> Note: Cannot be used together with <code>--schema</code>.</li> <li><code>-r, --recursive</code>: Recursively search for <code>*_signalJourney.json</code> files in subdirectories when <code>PATH</code> is a directory.</li> <li><code>-o, --output-format [text|json]</code>: Specify the output format. Defaults to <code>text</code> (human-readable). Use <code>json</code> for machine-readable output.     <pre><code>signaljourney-validate -o json path/to/directory/\n</code></pre></li> <li><code>-v, --verbose</code>: Enable verbose output in <code>text</code> format. Shows more error details, including the specific validator and schema path involved, in addition to the basic message and suggestion.     <pre><code>signaljourney-validate -v path/to/invalid_file.signalJourney.json\n</code></pre></li> <li><code>--bids</code>: Enable experimental BIDS context validation checks (see below).</li> <li><code>--bids-root PATH</code>: Specify the path to the BIDS dataset root directory. Required if <code>--bids</code> is used.     <pre><code>signaljourney-validate --bids --bids-root /path/to/bids_dataset path/to/bids_dataset/derivatives/...\n</code></pre></li> <li><code>-h, --help</code>: Show the help message and exit.</li> <li><code>--version</code>: Show the version of the <code>signaljourney-validator</code> package and exit.</li> </ul>"},{"location":"guides/validator_cli/#output-formats","title":"Output Formats","text":"<ul> <li><code>text</code> (Default):<ul> <li>Prints the validation status (<code>OK</code> or <code>FAILED</code>) for each file.</li> <li>On failure, lists errors with the path within the JSON, the error message, and any suggestions.</li> <li>If <code>--verbose</code> is used, adds more detail to error messages.</li> </ul> </li> <li><code>json</code>:<ul> <li>Outputs a single JSON object.</li> <li>Contains an <code>overall_status</code> (\"passed\" or \"failed\").</li> <li>Contains a <code>files</code> array, where each element is an object representing a validated file.</li> <li>Each file object includes <code>filepath</code>, <code>status</code> (\"passed\", \"failed\", or \"error\" if processing failed), and an <code>errors</code> array if applicable.</li> <li>The <code>errors</code> array contains objects with detailed error information: <code>message</code>, <code>path</code>, <code>schema_path</code>, <code>validator</code>, <code>validator_value</code>, <code>instance_value</code>, <code>suggestion</code>.</li> <li>Example JSON Output Fragment:     <pre><code>{\n  \"files\": [\n    {\n      \"filepath\": \"path/to/valid_file.signalJourney.json\",\n      \"status\": \"passed\",\n      \"errors\": []\n    },\n    {\n      \"filepath\": \"path/to/invalid_file.signalJourney.json\",\n      \"status\": \"failed\",\n      \"errors\": [\n        {\n          \"message\": \"'1.0' is not of type 'string'\",\n          \"path\": [\"pipelineInfo\", \"version\"],\n          \"schema_path\": [\"properties\", \"pipelineInfo\", \"properties\", \"version\", \"type\"],\n          \"validator\": \"type\",\n          \"validator_value\": \"string\",\n          \"instance_value\": \"1.0\",\n          \"suggestion\": \"Change value type from 'float' to 'string'.\"\n        }\n      ]\n    }\n  ],\n  \"bids_mode_enabled\": false,\n  \"overall_status\": \"failed\"\n}\n</code></pre></li> </ul> </li> </ul>"},{"location":"guides/validator_cli/#bids-context-validation-bids","title":"BIDS Context Validation (<code>--bids</code>)","text":"<p>This is an experimental feature.</p> <p>When the <code>--bids</code> flag is used (along with <code>--bids-root</code>), the validator attempts to perform checks relevant to the BIDS standard:</p> <ul> <li>(Planned) Verify the signalJourney file is located appropriately within the BIDS <code>derivatives/</code> structure.</li> <li>(Planned) Check if file paths referenced within the signalJourney file (e.g., in <code>inputSources</code>) exist relative to the BIDS root.</li> </ul> <p>Currently, enabling this flag primarily prints an informational message in the underlying library. Full BIDS validation logic will be added in future versions.</p>"},{"location":"guides/validator_cli/#exit-codes","title":"Exit Codes","text":"<p>The CLI uses the following exit codes, useful for scripting:</p> <ul> <li><code>0</code>: Validation successful (all files passed, or no <code>*_signalJourney.json</code> files found when using <code>json</code> output).</li> <li><code>1</code>: Validation failed (one or more files failed validation or encountered a processing error like file not found/invalid JSON). Also used if no <code>*_signalJourney.json</code> files are found when using <code>text</code> output.</li> <li><code>&gt;1</code>: Typically indicates an error with the CLI arguments themselves (handled by <code>click</code>). </li> </ul>"},{"location":"guides/validator_python/","title":"User Guide: Python Validator Library","text":"<p>This guide provides comprehensive details on using the <code>signaljourney-validator</code> Python library for validating signalJourney JSON files against the official schema.</p>"},{"location":"guides/validator_python/#installation","title":"Installation","text":"<pre><code>pip install signaljourney-validator\n</code></pre> <p>To include optional suggestion features based on fuzzy string matching (which requires <code>python-Levenshtein</code> for optimal performance), install the <code>suggestions</code> extra:</p> <pre><code>pip install signaljourney-validator[suggestions]\n</code></pre>"},{"location":"guides/validator_python/#core-components","title":"Core Components","text":"<p>The library primarily revolves around two classes:</p> <ul> <li><code>signaljourney_validator.Validator</code>: The main class used to load schemas and perform validation.</li> <li><code>signaljourney_validator.errors.ValidationErrorDetail</code>: A dataclass representing a single validation error, including its message, location, and potential suggestions.</li> <li><code>signaljourney_validator.validator.SignalJourneyValidationError</code>: A custom exception raised by the <code>Validator</code> when validation fails (if <code>raise_exceptions=True</code>). It contains a list of <code>ValidationErrorDetail</code> objects.</li> </ul>"},{"location":"guides/validator_python/#usage","title":"Usage","text":""},{"location":"guides/validator_python/#initialization","title":"Initialization","text":"<p>Create an instance of the <code>Validator</code> class. By default, it uses version-based schema selection, automatically detecting the appropriate schema version from each file being validated.</p> <pre><code>from signaljourney_validator import Validator\n\n# Use version-based validation (auto-detects schema version from files)\nvalidator = Validator()\n</code></pre> <p>You can also specify a particular schema version to use for all validations:</p> <pre><code># Use a specific schema version\nvalidator_v010 = Validator(schema_version=\"0.1.0\")\n</code></pre> <p>For advanced use cases, you can provide a path to a custom schema file or a dictionary containing the schema:</p> <pre><code>from pathlib import Path\n\n# Use a custom schema file (bypasses version-based validation)\ncustom_schema_file = Path('./path/to/your_schema.json')\nvalidator_custom_file = Validator(schema=custom_schema_file)\n\n# Use a schema loaded into a dictionary\nimport json\nwith open('./path/to/your_schema.json', 'r') as f:\n    custom_schema_dict = json.load(f)\nvalidator_custom_dict = Validator(schema=custom_schema_dict)\n</code></pre>"},{"location":"guides/validator_python/#performing-validation","title":"Performing Validation","text":"<p>The <code>validate()</code> method is used to check data against the loaded schema. It accepts:</p> <ul> <li>A <code>pathlib.Path</code> object pointing to the JSON file.</li> <li>A string containing the path to the JSON file.</li> <li>A string containing the JSON data itself (this is discouraged for large files).</li> <li>A Python dictionary representing the loaded JSON data.</li> </ul> <p>Validation Modes:</p> <ol> <li> <p>Raise Exception on Failure (Default):     If <code>raise_exceptions=True</code> (the default), the method returns <code>True</code> if the data is valid, and raises a <code>SignalJourneyValidationError</code> if it's invalid. The exception object has an <code>errors</code> attribute containing a list of <code>ValidationErrorDetail</code> objects.</p> <pre><code>from signaljourney_validator import Validator, SignalJourneyValidationError\n\nvalidator = Validator()\ndata_to_validate = 'path/to/your_file.signalJourney.json' # or a dict\n\ntry:\n    is_valid = validator.validate(data_to_validate)\n    print(\"Data is valid!\")\nexcept SignalJourneyValidationError as e:\n    print(f\"Validation Failed: {e}\")\n    for error in e.errors:\n        print(f\"  - Path: {error.path}, Message: {error.message}\")\n        if error.suggestion:\n            print(f\"    Suggestion: {error.suggestion}\")\nexcept FileNotFoundError:\n    print(\"Input file not found.\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n</code></pre> </li> <li> <p>Return Errors on Failure:     If <code>raise_exceptions=False</code>, the method returns an empty list (<code>[]</code>) if the data is valid, or a list of <code>ValidationErrorDetail</code> objects if it's invalid.</p> <pre><code>validator = Validator()\ndata_to_validate = {\"sj_version\": \"invalid\"} # Example invalid data\n\nerrors = validator.validate(data_to_validate, raise_exceptions=False)\n\nif not errors: # Empty list means valid\n    print(\"Data is valid!\")\nelse:\n    print(f\"Validation Failed. Found {len(errors)} errors:\")\n    for error in errors:\n        print(f\"  - Path: {error.path}, Message: {error.message}\")\n        if error.suggestion:\n            print(f\"    Suggestion: {error.suggestion}\")\n</code></pre> </li> </ol>"},{"location":"guides/validator_python/#version-based-validation","title":"Version-Based Validation","text":"<p>The validator automatically detects the <code>schema_version</code> field in signalJourney files and uses the appropriate schema version for validation. This enables backward compatibility and multi-version support.</p> <pre><code>from signaljourney_validator import Validator\n\n# Create validator that auto-detects schema versions\nvalidator = Validator()\n\n# Validate files with different schema versions\nfile_v010 = {\"schema_version\": \"0.1.0\", \"description\": \"Old format\", ...}\nfile_v020 = {\"schema_version\": \"0.2.0\", \"description\": \"New format\", ...}\n\n# Both files will be validated against their respective schema versions\nerrors_v010 = validator.validate(file_v010, raise_exceptions=False)\nerrors_v020 = validator.validate(file_v020, raise_exceptions=False)\n</code></pre> <p>You can also query version information:</p> <pre><code># Check what versions are supported\nprint(\"Supported versions:\", validator.get_supported_versions())\n\n# Get the current default version\nprint(\"Latest version:\", validator.get_latest_version())\n</code></pre> <p>To disable auto-detection and use the validator's configured schema:</p> <pre><code># This will use the validator's schema regardless of the file's schema_version\nerrors = validator.validate(data, auto_detect_version=False)\n</code></pre>"},{"location":"guides/validator_python/#bids-context-validation-experimental","title":"BIDS Context Validation (Experimental)","text":"<p>To enable experimental checks related to BIDS structure (like file placement and referencing), provide the path to the BIDS dataset root using the <code>bids_context</code> argument.</p> <pre><code>from pathlib import Path\n\nvalidator = Validator()\nbids_root = Path('/data/my_bids_dataset')\njourney_file = bids_root / 'derivatives' / 'pipeline' / 'sub-01_signalJourney.json'\n\nerrors = validator.validate(journey_file, bids_context=bids_root, raise_exceptions=False)\n\nif errors:\n    print(\"Validation failed (possibly BIDS context issues):\")\n    # ... process errors ...\n</code></pre> <p>Currently, BIDS context validation is primarily a placeholder and will print an informational message. Future versions will implement more specific checks.</p>"},{"location":"guides/validator_python/#error-details-validationerrordetail","title":"Error Details (<code>ValidationErrorDetail</code>)","text":"<p>Each object in the list returned by <code>validate(..., raise_exceptions=False)</code> or contained in <code>SignalJourneyValidationError.errors</code> provides details about a specific validation failure:</p> <ul> <li><code>message</code> (str): The error message from the underlying <code>jsonschema</code> validator.</li> <li><code>path</code> (List[Union[str, int]]): A list representing the path to the failing element within the JSON data (e.g., <code>['processingSteps', 0, 'parameters', 'cutoff']</code>).</li> <li><code>schema_path</code> (List[Union[str, int]]): The path within the JSON schema that defines the rule that failed.</li> <li><code>validator</code> (str): The name of the JSON schema keyword that failed (e.g., 'required', 'type', 'pattern').</li> <li><code>validator_value</code> (Any): The value of the schema keyword that failed (e.g., the required property name, the expected type, the regex pattern).</li> <li><code>instance_value</code> (Any): The actual value in the data that caused the failure.</li> <li><code>context</code> (List['ValidationErrorDetail']): For complex validation failures (like <code>anyOf</code>), this may contain sub-errors providing more context.</li> <li><code>suggestion</code> (Optional[str]): An optional suggestion generated by the library to help fix the error. Requires the <code>[suggestions]</code> extra to be installed for some suggestion types (like enum fuzzy matching).</li> </ul> <p>The <code>ValidationErrorDetail</code> class also has a <code>__str__</code> method that provides a basic formatted error message including the path and suggestion if available.</p>"},{"location":"guides/validator_python/#suggestions","title":"Suggestions","text":"<p>The library attempts to generate helpful suggestions for common errors:</p> <ul> <li><code>required</code>: Suggests adding the missing property/properties.</li> <li><code>type</code>: Suggests changing the value to the expected type(s).</li> <li><code>pattern</code>: Reminds the user to match the specified regex pattern.</li> <li><code>enum</code>: Lists the allowed values. If <code>[suggestions]</code> extra is installed, it may suggest the closest match using fuzzy string matching for string values.</li> <li><code>format</code>: Reminds the user to conform to the format (e.g., 'date-time', 'uri') and may provide examples.</li> <li>Length/Numeric Constraints: Suggests ensuring the value meets the minimum/maximum length, item count, or numeric value.</li> </ul> <p>Suggestions are best-effort and may not cover all cases.</p>"},{"location":"guides/validator_python/#error-handling","title":"Error Handling","text":"<p>Besides <code>SignalJourneyValidationError</code>, the validator might raise:</p> <ul> <li><code>FileNotFoundError</code>: If a specified file path (for data or schema) does not exist.</li> <li><code>json.JSONDecodeError</code> (via <code>SignalJourneyValidationError</code>): If a file contains invalid JSON.</li> <li><code>TypeError</code>: If invalid argument types are passed (e.g., providing a number as the schema).</li> <li><code>jsonschema.SchemaError</code> (via <code>SignalJourneyValidationError</code>): If the provided schema itself is invalid.</li> </ul> <p>It's recommended to wrap validation calls in appropriate try-except blocks. </p>"},{"location":"specification/","title":"Specification Index","text":"<p>This section details the signalJourney JSON schema. Choose one of the topics on the left to get started.</p>"},{"location":"specification/fields/","title":"Specification Fields","text":"<p>This section details the primary fields defined in the <code>signalJourney.schema.json</code>.</p>"},{"location":"specification/fields/#top-level-fields","title":"Top-Level Fields","text":"<p>These fields are required at the root level of every signalJourney JSON file.</p> <ul> <li><code>sj_version</code> (string, required)<ul> <li>Description: The version of the signalJourney specification that this file conforms to. Must follow semantic versioning (e.g., \"0.1.0\").</li> <li>Example: <code>\"sj_version\": \"0.1.0\"</code></li> </ul> </li> <li><code>schema_version</code> (string, required)<ul> <li>Description: The version of the specific <code>signalJourney.schema.json</code> file used for validation. Must follow semantic versioning.</li> <li>Example: <code>\"schema_version\": \"0.1.0\"</code></li> </ul> </li> <li><code>description</code> (string, required)<ul> <li>Description: A brief, human-readable summary of the processing pipeline described in this file.</li> <li>Example: <code>\"description\": \"EEG preprocessing pipeline including filtering and ICA.\"</code></li> </ul> </li> <li><code>pipelineInfo</code> (object, required)<ul> <li>Description: Contains metadata about the overall processing pipeline.</li> <li>See: Pipeline Info Object</li> </ul> </li> <li><code>processingSteps</code> (array, required)<ul> <li>Description: An ordered array detailing each individual step performed in the pipeline.</li> <li>See: Processing Step Object</li> </ul> </li> </ul>"},{"location":"specification/fields/#optional-top-level-fields","title":"Optional Top-Level Fields","text":"<ul> <li><code>summaryMetrics</code> (object, optional)<ul> <li>Description: Contains summary quality metrics that summarizes multiple <code>qualtiyMetrics</code> fileds across pipline or dataset. For example, sumamrizing quality metrics such as average powerline noise across subjects in a dataset.</li> <li>Structure: Key-value pairs where keys are metric names and values are the metric results. Can be nested objects.</li> <li>See: Quality Metrics Object</li> <li>Example: <code>\"summaryMetrics\": { \"finalSNR\": 35.2, \"percentDataRejected\": 5.1 }</code></li> </ul> </li> <li><code>extensions</code> (object, optional)<ul> <li>Description: Container for domain-specific or custom extensions to the core specification, organized by namespace.</li> <li>See: Namespaces for details on structure and governance.</li> <li>Example: <code>\"extensions\": { \"eeg\": { \"channelCount\": 64, \"referenceType\": \"average\" } }</code></li> </ul> </li> <li><code>versionHistory</code> (array, optional)<ul> <li>Description: Records changes made to this specific signalJourney file over time. Each item is an object.</li> <li>See: Version History Object</li> </ul> </li> </ul>"},{"location":"specification/fields/#pipeline-info-object","title":"Pipeline Info Object","text":"<p>Describes the overall pipeline.</p> <ul> <li><code>name</code> (string, required)<ul> <li>Description: A descriptive name for the pipeline.</li> <li>Example: <code>\"name\": \"Standard EEG Preprocessing\"</code></li> </ul> </li> <li><code>description</code> (string, required)<ul> <li>Description: A more detailed description of the pipeline's purpose and methods.</li> </ul> </li> <li><code>version</code> (string, required)<ul> <li>Description: The version of this specific pipeline script/implementation (distinct from <code>sj_version</code>).</li> <li>Example: <code>\"version\": \"1.2.0\"</code></li> </ul> </li> <li><code>pipelineType</code> (string, optional)<ul> <li>Description: A category describing the pipeline's main function (e.g., \"preprocessing\", \"ica\", \"source-localization\", \"connectivity\", \"statistics\").</li> <li>Example: <code>\"pipelineType\": \"preprocessing\"</code></li> </ul> </li> <li><code>executionDate</code> (string, optional, format: date-time)<ul> <li>Description: The date and time when the pipeline was executed (ISO 8601 format).</li> <li>Example: <code>\"executionDate\": \"2024-05-02T10:00:00Z\"</code></li> </ul> </li> <li><code>institution</code> (string, optional)<ul> <li>Description: The institution where the pipeline was run.</li> </ul> </li> <li><code>references</code> (array, optional)<ul> <li>Description: List of relevant publications or resources related to the pipeline methods.</li> <li>Items: Object with <code>doi</code> (string, required) and <code>citation</code> (string, optional) fields.</li> <li>Example: <code>\"references\": [ { \"doi\": \"10.1016/j.neuroimage.2019.116046\" } ]</code></li> </ul> </li> </ul>"},{"location":"specification/fields/#processing-step-object","title":"Processing Step Object","text":"<p>A single step within the processing pipeline.</p> <p>Required Fields: *   <code>stepId</code> (string): A unique identifier for this step within the pipeline (e.g., \"01_filter\", \"ica_run\"). *   <code>name</code> (string): A human-readable name for the step (e.g., \"High-pass Filter\", \"ICA Decomposition\"). *   <code>description</code> (string): A brief description of what the step does. *   <code>software</code> (array): An array of softwareDetails objects detailing the software used. *   <code>inputSources</code> (array): An array of inputSource objects defining the inputs. Must contain at least one item.</p> <p>Optional Fields: *   <code>parameters</code> (object): Key-value pairs representing parameters specific to this step. *   <code>outputTargets</code> (array): An array of outputTarget objects defining the outputs. *   <code>qualityMetrics</code> (object): A qualityMetricsObject containing metrics relevant to this step's output. *   <code>extensions</code> (object): Container for extensions.</p>"},{"location":"specification/fields/#inputsource-object","title":"inputSource Object","text":"<p>Defines a source of input data for a processing step.</p> <ul> <li><code>description</code> (string, required): Describes the input data (e.g., \"Raw EEG data\", \"ICA component activations\").</li> <li><code>location</code> (string, required): Path to the input file, relative to the dataset root or the <code>signalJourney.json</code> file itself (convention TBD, likely relative to dataset root for BIDS).</li> <li><code>format</code> (string, optional): The file format (e.g., \"SET\", \"FIF\", \"EDF\").</li> </ul>"},{"location":"specification/fields/#outputtarget-object","title":"outputTarget Object","text":"<p>Defines the target location or method for storing the output of a processing step.</p> <ul> <li><code>description</code> (string, required): Describes the output data (e.g., \"Filtered EEG data\", \"Cleaned IC weights\", \"Rejected channels list\").</li> <li><code>targetType</code> (string, required): Specifies the storage method. Must be one of:<ul> <li><code>file</code>: The output is stored in an external file (maps to <code>location</code>).</li> <li><code>in-memory</code>: Output exists in memory (e.g., a specific data structure in a script) but isn't explicitly saved to a file or variable name in this record. Use <code>format</code> for data type description.</li> <li><code>variable</code>: Output is stored as a named variable within the execution environment (maps to <code>name</code>). Use <code>format</code> for data type description.</li> <li><code>report</code>: Output is a report (e.g., HTML, PDF). May optionally use <code>location</code>. Use <code>format</code> for report type.</li> <li><code>userDefined</code>: A custom output type not covered by others (maps to <code>details</code>).</li> <li><code>inlineData</code>: The output data is embedded directly within the JSON (maps to <code>data</code>, <code>encoding</code>, <code>formatDescription</code>).</li> </ul> </li> <li><code>location</code> (string, required if <code>targetType</code> is <code>file</code>, optional for <code>report</code>): Path to the output file, relative path convention same as <code>inputSource.location</code>.</li> <li><code>format</code> (string, optional, recommended for <code>file</code>, <code>in-memory</code>, <code>variable</code>, <code>report</code>): The file format, data type, or report type (e.g., \"SET\", \"FIF\", \"TSV\", \"NumPy array\", \"HTML\").</li> <li><code>entityLabels</code> (object, optional, applicable if <code>targetType</code> is <code>file</code>): Key-value pairs representing BIDS-like entities for the output file. Example: <code>{\"desc\": \"filtered\"}</code>.</li> <li><code>name</code> (string, required if <code>targetType</code> is <code>variable</code>): Name of the variable stored.</li> <li><code>details</code> (string, required if <code>targetType</code> is <code>userDefined</code>): Description of the user-defined output.</li> <li><code>data</code> (any, required if <code>targetType</code> is <code>inlineData</code>): The embedded data itself (e.g., a list of bad channel names, an ICA weight matrix as a nested array, QC metrics as an object).</li> <li><code>encoding</code> (string, optional if <code>targetType</code> is <code>inlineData</code>, default: \"utf-8\"): Specifies how the <code>data</code> is encoded if it's binary or needs special handling (e.g., \"base64\", \"gzip+base64\"). Defaults to standard JSON types if omitted.</li> <li><code>formatDescription</code> (string, required if <code>targetType</code> is <code>inlineData</code>): A human-readable description of the <code>data</code> field's format (e.g., \"List of bad channel labels\", \"NumPy array serialized to list\", \"JSON object with QC scores\").</li> </ul>"},{"location":"specification/fields/#example-inlinedata-usage","title":"Example <code>inlineData</code> Usage:","text":"<pre><code>{\n  \"description\": \"List of channels rejected during artifact removal\",\n  \"targetType\": \"inlineData\",\n  \"formatDescription\": \"List of bad channel labels\",\n  \"data\": [\"Fp1\", \"Oz\", \"T7\"]\n}\n</code></pre> <pre><code>{\n  \"description\": \"ICA weight matrix\",\n  \"targetType\": \"inlineData\",\n  \"formatDescription\": \"ICA weights matrix (channels x components) as nested list\",\n  \"encoding\": null, // Or omit if standard JSON types\n  \"data\": [\n    [0.1, 0.5, -0.2],\n    [-0.3, 0.8, 0.1],\n    // ... more rows ...\n  ]\n}\n</code></pre>"},{"location":"specification/fields/#software-details-object","title":"Software Details Object","text":"<p>Describes the software used in a processing step. Corresponds to items in the <code>processingSteps[].software</code> array.</p> <p>Required Fields: *   <code>name</code> (string): The name of the software package or library (e.g., \"MNE-Python\", \"EEGLAB\", \"FieldTrip\"). *   <code>version</code> (string): The specific version of the software used.</p> <p>Optional Fields: *   <code>functionCall</code> (string): The specific function or command executed (potentially abbreviated or representative). Example: <code>\"functionCall\": \"raw.filter(l_freq=1.0, h_freq=40.0)\"</code> *   <code>operatingSystem</code> (string): The operating system the software was run on. *   <code>repository</code> (string, format: uri): URL to the software's source code repository (e.g., GitHub link). *   <code>commitHash</code> (string): Specific Git commit hash of the software version used, if applicable.</p>"},{"location":"specification/fields/#quality-metrics-object","title":"Quality Metrics Object","text":"<p>Container for quality metrics, used in <code>summaryMetrics</code> and <code>processingSteps[].qualityMetrics</code>.</p> <ul> <li>Structure: Free-form key-value pairs. Keys should be descriptive metric names. Values can be numbers, strings, booleans, arrays, or nested objects.</li> <li>Recommendation: Use consistent naming conventions (e.g., camelCase, snake_case) and include units where applicable (e.g., <code>snrDb</code>, <code>latencyMs</code>). Use namespaces for custom or tool-specific metrics within the <code>extensions</code> object if necessary.</li> <li>Examples: <pre><code>\"qualityMetrics\": {\n  \"channelsInterpolated\": [\"EEG 053\"],\n  \"numComponentsRemoved\": 3,\n  \"percentVarianceExplainedICA\": 85.2,\n  \"eogCorrelationThreshold\": 0.8\n}\n</code></pre></li> </ul>"},{"location":"specification/fields/#extensions-object","title":"Extensions Object","text":"<p>Container for domain-specific or tool-specific information not covered by the core schema.</p> <ul> <li>Structure: Key-value pairs where keys are namespace prefixes (e.g., \"eeg\", \"mne\", \"iclabel\") and values are objects containing the extension data.</li> <li>See: Namespaces for details.</li> <li>Location: Can appear at the top level (<code>extensions</code>) or within a <code>processingStep</code> (<code>extensions</code>).</li> <li>Example (within a step): <pre><code>\"extensions\": {\n  \"iclabel\": {\n    \"componentClassifications\": [\n      {\"icIndex\": 0, \"label\": \"Brain\", \"probability\": 0.95},\n      {\"icIndex\": 1, \"label\": \"Eye\", \"probability\": 0.88}\n    ]\n  }\n}\n</code></pre></li> </ul>"},{"location":"specification/fields/#version-history-object","title":"Version History Object","text":"<p>Describes a single entry in the top-level <code>versionHistory</code> array.</p> <p>Required Fields: *   <code>version</code> (string): The version identifier for this entry (e.g., \"1.1.0\"). Should correspond to <code>schema_version</code> at the time of change. *   <code>date</code> (string, format: date): The date this version was created (ISO 8601 format YYYY-MM-DD). *   <code>changes</code> (string): A description of the changes made in this version of the signalJourney file.</p> <p>Optional Fields: *   <code>author</code> (string): The person or entity who made the changes. </p>"},{"location":"specification/namespaces/","title":"Signal Journey Namespaces","text":""},{"location":"specification/namespaces/#purpose","title":"Purpose","text":"<p>The Signal Journey specification utilizes namespaces within the top-level <code>extensions</code> object to accommodate domain-specific or tool-specific metadata without cluttering the core schema definition. This allows for flexibility while maintaining a standardized base structure.</p>"},{"location":"specification/namespaces/#reserved-namespaces","title":"Reserved Namespaces","text":"<p>The following namespaces are currently defined and reserved:</p> <ul> <li><code>core</code> (Implied): Properties defined directly within the main schema body (not under <code>extensions</code>) belong to the core namespace. These represent fundamental concepts applicable across domains.</li> <li><code>eeg</code>: Reserved for properties specifically related to Electroencephalography (EEG) data, processing, and metadata (e.g., channel locations, reference schemes, specific artifact types). This namespace is primarily managed in coordination with EEG-focused communities and tools.</li> <li><code>nemar</code>: Reserved for properties related to the Neuroelectromagnetic Data Archive and Tools Resource (NEMAR) project, such as specific pipeline identifiers or project metadata required by NEMAR.</li> </ul> <p>These reserved namespaces are managed by the core Signal Journey maintainers and associated projects (like NEMAR, EEGLAB). Changes or additions within these namespaces follow the project's internal governance.</p>"},{"location":"specification/namespaces/#using-extensions","title":"Using Extensions","text":"<p>To add information specific to a domain like EEG, you would place it within the corresponding namespace object under <code>extensions</code>:</p> <pre><code>{\n  // ... core properties ...\n  \"extensions\": {\n    \"eeg\": {\n      \"channelLocationsFile\": \"/path/to/standard_1020.elc\",\n      \"referenceChannels\": [\"TP9\", \"TP10\"],\n      \"percentBadEpochs\": 5.2\n    },\n    \"nemar\": {\n      \"nemarPipelineId\": \"uuid-1234-abcd-5678\"\n    }\n    // Potentially other approved namespaces here\n  }\n}\n</code></pre>"},{"location":"specification/namespaces/#proposing-a-new-namespace","title":"Proposing a New Namespace","text":"<p>Adding new top-level namespaces (e.g., <code>meg</code>, <code>fnirs</code>, <code>myToolX</code>) requires a formal proposal and review process to ensure consistency, avoid redundancy, and maintain the integrity of the specification.</p> <p>Proposal Guidelines:</p> <ol> <li>Justification: Clearly explain the need for the new namespace. Why can't existing core fields or reserved namespaces accommodate the required information? What specific domain, tool, or data type does it represent?</li> <li>Scope: Define the intended scope and types of properties the namespace will contain.</li> <li>Schema (Optional but Recommended): Provide a draft JSON schema definition for the proposed namespace object. This helps clarify the structure and data types.</li> <li>Maintainer: Identify a point person or group responsible for maintaining the namespace definition.</li> <li>Community Support: Demonstrate potential utility or adoption within a relevant community or for a specific tool.</li> </ol> <p>Process:</p> <ol> <li>Discuss: Open an issue on the Signal Journey GitHub repository to discuss the proposed namespace.</li> <li>Formal Proposal: If the initial discussion is positive, submit a formal proposal as outlined in the main CONTRIBUTING.md guide (usually via a Pull Request modifying documentation or a dedicated proposal document).</li> <li>Review: The proposal will be reviewed by the Signal Journey maintainers based on the guidelines above.</li> <li>Approval &amp; Integration: If approved, the namespace will be officially recognized, documented, and potentially added to the main schema's <code>extensions.properties</code> list in a future release.</li> </ol> <p>For guidelines on proposing changes to namespaces, please see the main CONTRIBUTING.md. </p>"},{"location":"specification/overview/","title":"Specification Overview","text":"<p>The signalJourney specification provides a standardized JSON format for describing biosignal processing pipelines and their outputs. Its primary goal is to enhance reproducibility and data sharing by capturing detailed provenance information.</p>"},{"location":"specification/overview/#core-concepts","title":"Core Concepts","text":"<ul> <li>Pipeline Description: Defines the overall goal, software environment, and execution context of the processing workflow.</li> <li>Processing Steps: Details each individual operation performed on the data, including the specific software, function calls, parameters used, inputs, and outputs.</li> <li>Data Provenance: Explicitly links processing steps, defining dependencies and tracking data flow.</li> <li>Quality Metrics: Allows for embedding quantitative or qualitative metrics about the data quality or processing outcomes at various stages.</li> <li>Extensibility: Uses a namespace system to allow for domain-specific extensions (e.g., for EEG, MEG) while maintaining a core standard.</li> </ul>"},{"location":"specification/overview/#file-structure","title":"File Structure","text":"<p>A signalJourney file is a JSON object with several key top-level fields:</p> <ul> <li><code>sj_version</code>: The version of the signalJourney specification the file adheres to.</li> <li><code>schema_version</code>: The version of the JSON schema file itself.</li> <li><code>description</code>: A brief, human-readable description of the pipeline documented in the file.</li> <li><code>pipelineInfo</code>: An object containing metadata about the overall pipeline (name, version, type, execution date, etc.).</li> <li><code>processingSteps</code>: An array of objects, each detailing a single step in the pipeline.</li> <li><code>summaryMetrics</code> (optional): An object containing summary quality metrics for the entire pipeline output.</li> <li><code>extensions</code> (optional): An object containing namespaced, domain-specific extensions.</li> <li><code>versionHistory</code> (optional): An array documenting changes to the signalJourney file itself.</li> </ul>"},{"location":"specification/overview/#purpose","title":"Purpose","text":"<p>By standardizing how processing pipelines are documented, signalJourney aims to:</p> <ul> <li>Improve reproducibility by capturing exact parameters and software versions.</li> <li>Facilitate data sharing and meta-analysis by providing rich, machine-readable provenance.</li> <li>Enable automated analysis of processing pipelines across different studies and labs.</li> <li>Provide a clear audit trail for complex data transformations.</li> </ul> <p>See the Fields section for a detailed description of each component. </p>"},{"location":"specification/validation/","title":"Validation","text":"<p>The signalJourney specification is formally defined by a JSON Schema (<code>signalJourney.schema.json</code>). This schema allows for automated validation of signalJourney files.</p>"},{"location":"specification/validation/#schema-validation","title":"Schema Validation","text":"<p>Any valid signalJourney file MUST conform to the structure and constraints defined in the official JSON Schema corresponding to the <code>schema_version</code> specified within the file.</p>"},{"location":"specification/validation/#version-based-validation","title":"Version-Based Validation","text":"<p>The signalJourney validator supports multiple schema versions simultaneously. When validating a file:</p> <ol> <li>Automatic Version Detection: The validator reads the <code>schema_version</code> field from the signalJourney file and automatically loads the corresponding schema for validation.</li> <li>Backward Compatibility: Files using older schema versions continue to validate correctly against their corresponding schemas.</li> <li>Version Support: The validator maintains schemas for all released versions in a version registry, allowing seamless validation across different specification versions.</li> </ol> <p>If no <code>schema_version</code> is specified in the file, or if an unsupported version is detected, the validator provides clear error messages indicating the available supported versions.</p> <p>Tools like the <code>signaljourney-validator</code> library leverage this schema to check for:</p> <ul> <li>Presence of required fields: Ensures all mandatory fields (e.g., <code>sj_version</code>, <code>pipelineInfo</code>, <code>processingSteps</code>) are included.</li> <li>Correct data types: Verifies that field values match their expected types (string, number, boolean, object, array).</li> <li>Format constraints: Checks adherence to specific formats (e.g., <code>date-time</code> for timestamps, <code>uri</code> for URLs, semantic versioning patterns for <code>sj_version</code> and <code>schema_version</code>).</li> <li>Enum constraints: Ensures values for fields with restricted options are within the allowed set (e.g., <code>sourceType</code>, <code>targetType</code>).</li> <li>Structural integrity: Validates the correct nesting and structure of objects and arrays (e.g., <code>processingSteps</code> must be an array of objects conforming to the processing step definition).</li> </ul>"},{"location":"specification/validation/#additional-validation-tool-specific","title":"Additional Validation (Tool-Specific)","text":"<p>While the JSON Schema enforces structural validity, dedicated validation tools may perform additional checks beyond the schema's scope:</p> <ul> <li>Dependency Checking: Verifying that <code>stepId</code> values referenced in <code>dependsOn</code> fields actually exist within the <code>processingSteps</code> array and that there are no circular dependencies.</li> <li>Input/Output Linking: Ensuring that <code>outputId</code> references in <code>previousStepOutput</code> inputs correctly match <code>description</code> fields in the <code>outputTargets</code> of the specified preceding step.</li> <li>BIDS Context Validation: (Experimental/Optional) If run within a BIDS dataset context, validators might check:<ul> <li>Correct placement of the signalJourney file within the BIDS structure (e.g., in <code>derivatives/&lt;pipeline_name&gt;/</code>).</li> <li>Existence of data files referenced in <code>inputSources</code> relative to the BIDS root.</li> <li>Consistency with BIDS naming conventions where applicable.</li> </ul> </li> <li>Semantic Checks: Potentially checking for logical consistency, such as ensuring parameter values are reasonable for the specified software function (though this is often complex).</li> </ul> <p>Refer to the documentation for specific validation tools (like <code>signaljourney-validator</code>) for details on the checks they perform. </p>"},{"location":"specification/versioning/","title":"Versioning","text":"<p>signalJourney uses semantic versioning (SemVer - MAJOR.MINOR.PATCH) for both the specification itself and the schema file.</p>"},{"location":"specification/versioning/#specification-version-sj_version","title":"Specification Version (<code>sj_version</code>)","text":"<p>This field, located at the root of the signalJourney JSON file, indicates the version of the overall specification standard that the file adheres to.</p> <ul> <li>MAJOR: Incremented for incompatible changes to the conceptual model or fundamental structure.</li> <li>MINOR: Incremented for additions or changes that are backward-compatible with previous minor versions within the same major version.</li> <li>PATCH: Incremented for backward-compatible bug fixes or clarifications to the specification text.</li> </ul> <p>Files declaring a specific <code>sj_version</code> (e.g., \"0.1.0\") should be valid according to the rules and structures defined in that version of the specification document.</p>"},{"location":"specification/versioning/#schema-version-schema_version","title":"Schema Version (<code>schema_version</code>)","text":"<p>This field, also at the root level, indicates the version of the JSON Schema file (<code>signalJourney.schema.json</code>) used to define and validate the file's structure.</p> <p>While often aligned with the <code>sj_version</code>, the <code>schema_version</code> might increment more frequently for schema-specific fixes or improvements that don't necessarily change the conceptual specification.</p> <ul> <li>MAJOR: Incremented for backward-incompatible changes to the schema structure that would break validation for files valid under previous versions.</li> <li>MINOR: Incremented for backward-compatible additions to the schema (e.g., adding new optional fields, adding new enum values).</li> <li>PATCH: Incremented for backward-compatible fixes or refinements to the schema definitions (e.g., improving descriptions, tightening patterns) that do not change validation outcomes for previously valid files.</li> </ul> <p>The signalJourney validator automatically uses the <code>schema_version</code> declared in the file to select the appropriate schema definition for validation. This enables backward compatibility with older versions while supporting new features in newer versions.</p>"},{"location":"specification/versioning/#pipeline-version-pipelineinfoversion","title":"Pipeline Version (<code>pipelineInfo.version</code>)","text":"<p>This field within the <code>pipelineInfo</code> object represents the version of the specific pipeline script or implementation described in the file. It is independent of the signalJourney specification and schema versions and should be managed by the pipeline developers.</p>"},{"location":"specification/versioning/#version-history-versionhistory","title":"Version History (<code>versionHistory</code>)","text":"<p>The optional top-level <code>versionHistory</code> array allows tracking changes made to the content of a specific signalJourney file over time. Each entry should include:</p> <ul> <li><code>version</code> (string): A user-defined version string for this state of the file.</li> <li><code>date</code> (string, format: date): The date the change was made.</li> <li><code>changes</code> (string): A description of the changes.</li> <li><code>author</code> (string, optional): Who made the changes.</li> </ul> <p>This provides an audit trail for modifications to the provenance record itself. </p>"},{"location":"specification/versioning/#version-history","title":"Version History","text":"Version Date Changes Summary 0.2.0 2024-05-06 Refactored <code>InputSource</code>/<code>OutputTarget</code> into definitions. Added <code>inlineData</code> targetType. 0.1.0 2024-05-03 Initial schema structure definition. Basic fields for pipeline, steps, software, parameters."},{"location":"tutorials/","title":"Tutorials Index","text":"<p>Step-by-step guides for using signalJourney. Choose one of the topics on the left to get started.</p>"},{"location":"tutorials/creating_manual/","title":"Tutorial: Creating a signalJourney File Manually","text":"<p>This tutorial guides you through creating a basic signalJourney JSON file by hand using a text editor. This is useful for understanding the core structure or for simple pipelines where automated generation isn't necessary.</p> <p>Prerequisites:</p> <ul> <li>A text editor (like VS Code, Sublime Text, Notepad++).</li> <li>Basic understanding of JSON syntax (key-value pairs, objects {}, arrays []).</li> <li>Familiarity with the pipeline you want to document.</li> </ul>"},{"location":"tutorials/creating_manual/#1-basic-file-structure","title":"1. Basic File Structure","text":"<p>Start with the essential top-level fields:</p> <pre><code>{\n  \"sj_version\": \"0.1.0\", \n  \"schema_version\": \"0.1.0\", \n  \"description\": \"\", \n  \"pipelineInfo\": {},\n  \"processingSteps\": [] \n}\n</code></pre> <ul> <li>Set <code>sj_version</code> and <code>schema_version</code> to the current version you are targeting (e.g., \"0.1.0\").</li> <li>Add a brief <code>description</code> of the pipeline.</li> </ul>"},{"location":"tutorials/creating_manual/#2-describe-the-pipeline-pipelineinfo","title":"2. Describe the Pipeline (<code>pipelineInfo</code>)","text":"<p>Fill in the <code>pipelineInfo</code> object with details about the overall workflow:</p> <pre><code>{\n  // ... sj_version, schema_version, description ...\n  \"pipelineInfo\": {\n    \"name\": \"My Simple EEG Filter Pipeline\",\n    \"description\": \"Applies high-pass and low-pass filters to raw EEG data using EEGLAB.\",\n    \"version\": \"1.0\", \n    \"pipelineType\": \"preprocessing\",\n    \"executionDate\": \"2024-05-02T15:30:00Z\" // Optional: Use ISO 8601 format\n  },\n  \"processingSteps\": []\n}\n</code></pre> <ul> <li>Provide a clear <code>name</code> and more detailed <code>description</code>.</li> <li>Assign a <code>version</code> to your specific pipeline script/implementation.</li> <li>Optionally add <code>pipelineType</code> and <code>executionDate</code>.</li> </ul>"},{"location":"tutorials/creating_manual/#3-add-processing-steps-processingsteps","title":"3. Add Processing Steps (<code>processingSteps</code>)","text":"<p>This is an array where each element is an object describing one step. Let's add a filtering step:</p> <pre><code>{\n  // ... sj_version, schema_version, description, pipelineInfo ...\n  \"processingSteps\": [\n    {\n      \"stepId\": \"step1_filter\", \n      \"name\": \"Apply Band-pass Filter\",\n      \"description\": \"Applied a high-pass filter at 1 Hz and low-pass at 40 Hz.\",\n      \"software\": {\n        \"name\": \"EEGLAB\",\n        \"version\": \"2023.1\"\n      },\n      \"parameters\": {\n        \"locutoff\": 1.0,\n        \"hicutoff\": 40.0,\n        \"filter_type\": \"fir\"\n      },\n      \"inputSources\": [\n        {\n          \"sourceType\": \"file\",\n          \"location\": \"./sub-01_task-rest_eeg.set\"\n        }\n      ],\n      \"outputTargets\": [\n        {\n          \"targetType\": \"file\",\n          \"description\": \"Filtered EEG data file\",\n          \"location\": \"./derivatives/pipeline/sub-01_task-rest_desc-filtered_eeg.set\"\n        }\n      ]\n    }\n    // Add more steps here...\n  ]\n}\n</code></pre> <p>Key fields for each step:</p> <ul> <li><code>stepId</code>: A unique identifier for this step (e.g., \"step1\", \"filter_hp\").</li> <li><code>name</code>: Human-readable name.</li> <li><code>description</code>: What the step did.</li> <li><code>software</code>: Name and version of the tool used.</li> <li><code>parameters</code>: Key-value pairs of the parameters used.</li> <li><code>inputSources</code>: An array describing where the step got its data.<ul> <li><code>sourceType</code>: Typically <code>\"file\"</code> for the first step, or <code>\"previousStepOutput\"</code> for subsequent steps.</li> <li><code>location</code> (for <code>file</code> type): Path to the input file.</li> </ul> </li> <li><code>outputTargets</code>: An array describing the step's output.<ul> <li><code>targetType</code>: Can be <code>\"file\"</code>, <code>\"in-memory\"</code>, etc.</li> <li><code>description</code>: Important: A unique description used to link inputs of later steps.</li> <li><code>location</code> (for <code>file</code> type): Path where the output was saved.</li> </ul> </li> </ul>"},{"location":"tutorials/creating_manual/#4-linking-steps-dependson-and-previousstepoutput","title":"4. Linking Steps (<code>dependsOn</code> and <code>previousStepOutput</code>)","text":"<p>To show the flow, use <code>dependsOn</code> in the step definition and <code>previousStepOutput</code> in <code>inputSources</code>.</p> <pre><code>{\n  // ... sj_version, schema_version, description, pipelineInfo ...\n  \"processingSteps\": [\n    {\n      \"stepId\": \"step1_filter\", \n      \"name\": \"Apply Band-pass Filter\",\n      // ... other fields ...\n      \"outputTargets\": [\n        {\n          \"targetType\": \"in-memory\", // Output is kept in memory for next step\n          \"description\": \"Filtered EEG data in memory\" // Unique description\n        }\n      ]\n    },\n    {\n      \"stepId\": \"step2_reref\",\n      \"name\": \"Apply Average Reference\",\n      \"description\": \"Re-referenced the data to the average.\",\n      \"dependsOn\": [\"step1_filter\"], // Depends on the previous step\n      \"software\": {\n        \"name\": \"EEGLAB\",\n        \"version\": \"2023.1\"\n      },\n      \"parameters\": {\n        \"reference_type\": \"average\"\n      },\n      \"inputSources\": [\n        {\n          \"sourceType\": \"previousStepOutput\",\n          \"stepId\": \"step1_filter\", // ID of the step that produced the input\n          \"outputId\": \"Filtered EEG data in memory\" // Matches description in step1 outputTarget\n        }\n      ],\n      \"outputTargets\": [\n        {\n          \"targetType\": \"file\",\n          \"description\": \"Final preprocessed file\",\n          \"location\": \"./derivatives/pipeline/sub-01_task-rest_desc-preproc_eeg.set\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"tutorials/creating_manual/#5-adding-optional-information","title":"5. Adding Optional Information","text":"<p>You can add more detail:</p> <ul> <li>Quality Metrics: Add a <code>qualityMetrics</code> object to steps or a top-level <code>summaryMetrics</code> object.     <pre><code>// In a processing step:\n\"qualityMetrics\": { \"numBadChannelsInterpolated\": 2 }\n\n// At the top level:\n\"summaryMetrics\": { \"percentGoodICAComponents\": 90 }\n</code></pre></li> <li>Software Details: Add <code>functionCall</code>, <code>repository</code>, etc., to the <code>software</code> object.</li> <li>Extensions: Add domain-specific info under the <code>extensions</code> object (see Namespaces).</li> </ul>"},{"location":"tutorials/creating_manual/#6-validation","title":"6. Validation","text":"<p>Once you have created your file, it's highly recommended to validate it against the schema using a tool like the <code>signaljourney-validator</code> to catch errors in structure or types.</p> <p>This covers the basics of creating a signalJourney file manually. Remember to consult the Specification Fields documentation for details on all available fields. </p>"},{"location":"tutorials/matlab_tools/","title":"Tutorial: Using the MATLAB Tools","text":"<p>This tutorial demonstrates how to use the provided MATLAB functions to interact with signalJourney JSON files.</p> <p>Prerequisites:</p> <ul> <li>MATLAB R2019b or newer.</li> <li>The <code>scripts/matlab</code> directory added to your MATLAB path.</li> <li>signalJourney JSON files to work with.</li> </ul>"},{"location":"tutorials/matlab_tools/#1-reading-a-signaljourney-file","title":"1. Reading a signalJourney File","text":"<p>Use the <code>readSignalJourney</code> function to load a JSON file into a MATLAB structure.</p> <pre><code>% Define the path to your file\nfilename = '../../schema/examples/basic_preprocessing_pipeline_mne.signalJourney.json';\n\ntry\n    journeyData = readSignalJourney(filename);\n    disp('File read successfully!');\n\n    % Access data within the structure\n    disp(['Pipeline Name: ', journeyData.pipelineInfo.name]);\n    fprintf('Number of processing steps: %d\\n', length(journeyData.processingSteps));\n\n    % Display parameters of the first step (if it exists)\n    if ~isempty(journeyData.processingSteps) &amp;&amp; isfield(journeyData.processingSteps(1), 'parameters')\n        disp('Parameters of first step:');\n        disp(journeyData.processingSteps(1).parameters);\n    end\n\ncatch ME\n    fprintf('Error reading file: %s\\n', ME.message);\nend\n</code></pre>"},{"location":"tutorials/matlab_tools/#2-basic-validation-of-matlab-structure","title":"2. Basic Validation of MATLAB Structure","text":"<p>Use <code>validateSignalJourney</code> to perform basic checks on a loaded structure (this is not full schema validation).</p> <pre><code>% Assuming journeyData is loaded from the previous step\n\n[isValid, messages] = validateSignalJourney(journeyData);\n\nif isValid\n    disp('MATLAB structure passed basic validation checks.');\nelse\n    disp('Basic validation failed:');\n    for i = 1:length(messages)\n        disp(['- ', messages{i}]);\n    end\nend\n\n% Example of an invalid structure\ninvalidData = struct('sj_version', '0.1.0'); % Missing required fields\n[isValid, messages] = validateSignalJourney(invalidData);\nif ~isValid\n    disp('Invalid structure failed validation as expected:');\n    disp(messages);\nend\n</code></pre>"},{"location":"tutorials/matlab_tools/#3-modifying-data","title":"3. Modifying Data","text":"<p>You can modify the MATLAB structure directly.</p> <pre><code>% Assuming journeyData is loaded\n\n% Change the pipeline description\njourneyData.pipelineInfo.description = 'Updated pipeline description for tutorial.';\n\n% Add a quality metric to the first step\nif ~isempty(journeyData.processingSteps)\n   journeyData.processingSteps(1).qualityMetrics = struct('filterRippleDb', -60);\n   disp('Added quality metric to step 1.');\nend\n\n% Add a new top-level summary metric\njourneyData.summaryMetrics = struct('totalExecutionTimeSec', 125.5);\ndisp('Added summary metric.');\n</code></pre>"},{"location":"tutorials/matlab_tools/#4-writing-to-a-signaljourney-file","title":"4. Writing to a signalJourney File","text":"<p>Use <code>writeSignalJourney</code> to save a MATLAB structure back to a JSON file. The output will be pretty-printed.</p> <pre><code>% Assuming journeyData has been loaded and possibly modified\noutputFilename = './my_modified_pipeline.signalJourney.json';\n\ntry\n    writeSignalJourney(journeyData, outputFilename);\n    disp(['Successfully wrote modified data to: ', outputFilename]);\ncatch ME\n    fprintf('Error writing file: %s\\n', ME.message);\nend\n</code></pre>"},{"location":"tutorials/matlab_tools/#5-version-conversion-placeholder","title":"5. Version Conversion (Placeholder)","text":"<p>The <code>convertSignalJourneyVersion</code> function is a placeholder for future development. It is intended to convert structures between different versions of the specification.</p> <pre><code>% Placeholder usage - this will currently error\n% targetVersion = '0.2.0'; \n% try\n%    convertedData = convertSignalJourneyVersion(journeyData, targetVersion);\n% catch ME\n%    disp(ME.message); % Expect 'Not Implemented' error\n% end\n</code></pre> <p>These examples cover the basic usage of the MATLAB tools for interacting with signalJourney files. </p>"},{"location":"tutorials/python_lib/","title":"Tutorial: Using the Python Library (<code>signaljourney-validator</code>)","text":"<p>The <code>signaljourney-validator</code> Python library provides programmatic ways to validate your signalJourney JSON files.</p> <p>Prerequisites:</p> <ul> <li>Python 3.8+ installed.</li> <li>The <code>signaljourney-validator</code> library installed (<code>pip install signaljourney-validator</code>).</li> <li>A signalJourney JSON file to validate (e.g., one created manually or an example file).</li> </ul>"},{"location":"tutorials/python_lib/#1-basic-validation","title":"1. Basic Validation","text":"<p>The core functionality involves creating a <code>Validator</code> instance and calling its <code>validate</code> method.</p> <pre><code>from pathlib import Path\nfrom signaljourney_validator import Validator, SignalJourneyValidationError\n\n# Path to your signalJourney file\njourney_file = Path('path/to/your_pipeline.signalJourney.json') \n# Or use one of the examples:\n# journey_file = Path('../../schema/examples/basic_preprocessing_pipeline_mne.signalJourney.json')\n\ntry:\n    # Initialize validator (uses the default schema bundled with the package)\n    validator = Validator()\n\n    # Validate the file\n    # By default, validate() returns True on success \n    # and raises SignalJourneyValidationError on failure.\n    is_valid = validator.validate(journey_file)\n\n    if is_valid:\n        print(f\"Validation successful for: {journey_file.name}\")\n\nexcept SignalJourneyValidationError as e:\n    print(f\"Validation FAILED for {journey_file.name}: {e}\")\n    # Access detailed errors\n    if e.errors:\n        print(\"Details:\")\n        for error_detail in e.errors:\n            path_str = '/'.join(map(str, error_detail.path)) if error_detail.path else 'root'\n            print(f\"  - Error at '{path_str}': {error_detail.message}\")\n            if error_detail.suggestion:\n                 print(f\"    Suggestion: {error_detail.suggestion}\")\n\nexcept FileNotFoundError:\n    print(f\"Error: File not found at {journey_file}\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n</code></pre>"},{"location":"tutorials/python_lib/#2-getting-errors-without-exceptions","title":"2. Getting Errors Without Exceptions","text":"<p>If you prefer to handle errors without try-except blocks, set <code>raise_exceptions=False</code>. The <code>validate</code> method will then return an empty list <code>[]</code> on success or a list of <code>ValidationErrorDetail</code> objects on failure.</p> <pre><code># ... (import Validator, SignalJourneyValidationError, Path) ...\n\njourney_file = Path('path/to/invalid_pipeline.signalJourney.json')\nvalidator = Validator()\n\nvalidation_result = validator.validate(journey_file, raise_exceptions=False)\n\nif not validation_result: # Empty list means success\n    print(f\"Validation successful for: {journey_file.name}\")\nelse:\n    print(f\"Validation FAILED for {journey_file.name}. Found {len(validation_result)} errors:\")\n    for error_detail in validation_result:\n        path_str = '/'.join(map(str, error_detail.path)) if error_detail.path else 'root'\n        print(f\"  - Error at '{path_str}': {error_detail.message}\")\n        if error_detail.suggestion:\n            print(f\"    Suggestion: {error_detail.suggestion}\")\n</code></pre>"},{"location":"tutorials/python_lib/#3-using-a-custom-schema","title":"3. Using a Custom Schema","text":"<p>You can initialize the <code>Validator</code> with a path to your own schema file or a schema dictionary.</p> <pre><code># ... (import Validator, Path) ...\n\ncustom_schema_path = Path('path/to/my_custom_schema.json')\njourney_file = Path('path/to/your_pipeline.signalJourney.json')\n\ntry:\n    # Initialize with a custom schema file\n    validator = Validator(schema=custom_schema_path)\n\n    # Or initialize with a schema dictionary\n    # my_schema_dict = { ... } \n    # validator = Validator(schema=my_schema_dict)\n\n    validator.validate(journey_file) # Raises exception on failure by default\n    print(\"Validation successful with custom schema!\")\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"tutorials/python_lib/#4-validating-data-in-memory","title":"4. Validating Data In-Memory","text":"<p>You can also validate Python dictionaries directly.</p> <pre><code># ... (import Validator) ...\n\nmy_journey_data = {\n  \"sj_version\": \"0.1.0\",\n  \"schema_version\": \"0.1.0\",\n  \"description\": \"In-memory example\",\n  \"pipelineInfo\": { \"name\": \"Test\", \"version\": \"1.0\", \"description\": \"...\" },\n  \"processingSteps\": [\n      # ... steps ... \n      # Missing required fields might cause validation failure\n  ]\n}\n\nvalidator = Validator()\n\nvalidation_errors = validator.validate(my_journey_data, raise_exceptions=False)\n\nif not validation_errors:\n    print(\"In-memory data is valid.\")\nelse:\n    print(f\"In-memory data validation FAILED. Found {len(validation_errors)} errors:\")\n    # ... (print errors as before) ...\n</code></pre>"},{"location":"tutorials/python_lib/#5-bids-context-validation-experimental","title":"5. BIDS Context Validation (Experimental)","text":"<p>The library includes experimental support for validating within a BIDS context. This requires providing the path to the BIDS dataset root.</p> <pre><code># ... (import Validator, Path, SignalJourneyValidationError) ...\n\nbids_root_path = Path('/path/to/your/bids_dataset')\n# Make sure the journey file path is within the BIDS root for context checks\njourney_file_in_bids = bids_root_path / 'derivatives' / 'my_pipeline' / 'sub-01' / 'eeg' / 'sub-01_task-rest_signalJourney.json'\n\nvalidator = Validator()\n\ntry:\n    # Pass the bids_context argument\n    validator.validate(journey_file_in_bids, bids_context=bids_root_path)\n    print(f\"Validation successful within BIDS context: {journey_file_in_bids.name}\")\n\nexcept SignalJourneyValidationError as e:\n    print(f\"Validation FAILED for {journey_file_in_bids.name}: {e}\")\n    if e.errors:\n        print(\"Details (including potential BIDS context errors):\")\n        # ... (print errors as before) ...\n\nexcept Exception as e:\n     print(f\"An unexpected error occurred: {e}\")\n</code></pre> <p>This covers the main ways to use the <code>signaljourney-validator</code> library for validating your files programmatically. For detailed information on the available functions and classes, refer to the Python API documentation. </p>"},{"location":"tutorials/validating/","title":"Tutorial: Validating signalJourney Files","text":"<p>Validating your signalJourney files ensures they conform to the official specification, promoting interoperability and preventing errors when tools consume the files.</p> <p>There are two main ways to validate:</p> <ol> <li>Using the CLI: Quick checks from the command line.</li> <li>Using the Python Library: Programmatic validation within scripts.</li> </ol> <p>Prerequisites:</p> <ul> <li>The <code>signaljourney-validator</code> package installed (<code>pip install signaljourney-validator</code>).</li> <li>signalJourney JSON files to validate.</li> </ul>"},{"location":"tutorials/validating/#1-using-the-command-line-interface-cli","title":"1. Using the Command-Line Interface (CLI)","text":"<p>The <code>signaljourney-validate</code> command provides a simple way to validate files.</p> <p>Validate a Single File:</p> <pre><code>signaljourney-validate path/to/your_pipeline.signalJourney.json\n</code></pre> <ul> <li>If valid, it prints: <code>\u2713 path/to/your_pipeline.signalJourney.json is valid</code> and exits with code 0.</li> <li>If invalid, it prints: <code>\u2717 path/to/your_pipeline.signalJourney.json has validation errors:</code> followed by details, and exits with code 1.</li> </ul> <p>Validate All Files in a Directory:</p> <pre><code># Validate files directly in the directory\nsignaljourney-validate path/to/directory/\n\n# Validate recursively through subdirectories\nsignaljourney-validate -r path/to/directory/\n</code></pre> <p>Getting More Detail (Verbose):</p> <p>Use the <code>-v</code> or <code>--verbose</code> flag to see more details about errors, including suggestions.</p> <pre><code>signaljourney-validate -v path/to/invalid_file.signalJourney.json\n</code></pre> <p>JSON Output:</p> <p>For machine-readable output, use the <code>-o json</code> flag.</p> <pre><code>signaljourney-validate -o json path/to/directory/\n</code></pre> <p>This will output a JSON object summarizing the validation status and errors for each file found.</p> <p>Using a Custom Schema:</p> <p>Specify a different schema file using <code>--schema</code>.</p> <pre><code>signaljourney-validate --schema path/to/my_schema.json path/to/your_pipeline.signalJourney.json\n</code></pre> <p>BIDS Context Validation (Experimental):</p> <p>To enable BIDS-specific checks (like file placement), use the <code>--bids</code> flag and provide the dataset root with <code>--bids-root</code>.</p> <pre><code>signaljourney-validate --bids --bids-root /path/to/bids_dataset /path/to/bids_dataset/derivatives/...\n</code></pre> <p>Refer to the CLI user guide (<code>validator_cli.md</code>) or run <code>signaljourney-validate --help</code> for all options.</p>"},{"location":"tutorials/validating/#2-using-the-python-library","title":"2. Using the Python Library","text":"<p>For validation within Python scripts, use the <code>Validator</code> class.</p> <pre><code>from pathlib import Path\nfrom signaljourney_validator import Validator, SignalJourneyValidationError\n\nvalidator = Validator() # Uses default schema\nfile_to_check = Path('path/to/your_pipeline.signalJourney.json')\n\ntry:\n    # Option 1: Raise exception on failure\n    validator.validate(file_to_check)\n    print(f\"{file_to_check.name} is valid.\")\n\nexcept SignalJourneyValidationError as e:\n    print(f\"{file_to_check.name} validation failed: {e}\")\n    if e.errors:\n        print(\"Errors:\")\n        for err in e.errors:\n            print(f\"  - {err}\") # ValidationErrorDetail has __str__ method\n            # Access err.path, err.message, err.suggestion etc.\n\n# Option 2: Get errors as a list\nvalidation_errors = validator.validate(file_to_check, raise_exceptions=False)\nif validation_errors: # List is not empty\n    print(f\"{file_to_check.name} validation failed:\")\n    for err in validation_errors:\n         print(f\"  - {err}\")\nelse:\n    print(f\"{file_to_check.name} is valid.\")\n</code></pre> <p>See the Python Library Tutorial and the Python Validator Guide for more details on programmatic validation.</p> <p>Regular validation is crucial for maintaining high-quality, interoperable signalJourney provenance records. </p>"}]}